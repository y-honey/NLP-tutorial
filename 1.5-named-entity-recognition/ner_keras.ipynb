{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joydeb/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/joydeb/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/joydeb/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/joydeb/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/joydeb/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/joydeb/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/joydeb/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/joydeb/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/joydeb/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/joydeb/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/joydeb/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/joydeb/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    def __init__():\n",
    "        self.file = file\n",
    "    \n",
    "    def sentence_from_file(self):\n",
    "    f = open(self.filename)\n",
    "    single_file_sentences = []\n",
    "    sentence_list = []\n",
    "    for line in f:\n",
    "        if len(line)==0 or line[0]==\"\\n\":\n",
    "            if len(sentence_list) > 0:\n",
    "                single_file_sentences.append(sentence_list)\n",
    "                sentence_list = []\n",
    "            continue\n",
    "        splits = line.split(' ')\n",
    "        sentence_list.append([splits[0],splits[1],splits[-1]])\n",
    "\n",
    "    if len(sentence_list) >0:\n",
    "        single_file_sentences.append(sentence_list)\n",
    "        sentence_list = []\n",
    "    return single_file_sentences\n",
    "\n",
    "    def get_case_value(self,word, case_dict):   \n",
    "        case_value = 'other'\n",
    "\n",
    "        count_digits = 0\n",
    "        for char in word:\n",
    "            if char.isdigit():\n",
    "                count_digits += 1\n",
    "\n",
    "        if word.isdigit():\n",
    "            case_value = 'number'\n",
    "        elif count_digits / float(len(word)) > 0.5:\n",
    "            case_value = 'fraction'\n",
    "        elif word.islower():\n",
    "            case_value = 'lower'\n",
    "        elif word.isupper():\n",
    "            case_value = 'upper'\n",
    "        elif word[0].isupper():\n",
    "            case_value = 'title'\n",
    "        elif count_digits > 0:\n",
    "            case_value = 'leters_digit'\n",
    "\n",
    "        return case_dict[case_value]\n",
    "\n",
    "\n",
    "    def createBatches(self,data):\n",
    "        l = []\n",
    "        for i in data:\n",
    "            l.append(len(i[0]))\n",
    "        l = set(l)\n",
    "        batches = []\n",
    "        batch_len = []\n",
    "        z = 0\n",
    "        for i in l:\n",
    "            for batch in data:\n",
    "                if len(batch[0]) == i:\n",
    "                    batches.append(batch)\n",
    "                    z += 1\n",
    "            batch_len.append(z)\n",
    "        return batches,batch_len\n",
    "\n",
    "    def createMatrices(self,sentences, wd_to_id, lb_to_id, cs_to_id, ch_to_id, pos_to_id):\n",
    "        #paddingIdx = word2Idx['PAD_TKN']\n",
    "        unknownIdx = wd_to_id['UNK_TKN']\n",
    "\n",
    "        dataset = []\n",
    "\n",
    "        word_count = 0\n",
    "        unknownword_count = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            word_indices = []    \n",
    "            char_indices = []\n",
    "            case_indices = []\n",
    "            label_indices = []\n",
    "            pos_indices = []\n",
    "\n",
    "            for word,char,pos,label in sentence:  \n",
    "\n",
    "                word_count += 1\n",
    "                if word in wd_to_id:\n",
    "                    word_index = wd_to_id[word]\n",
    "                elif word.lower() in wd_to_id:\n",
    "                    word_index = wd_to_id[word.lower()]                 \n",
    "                else:\n",
    "                    word_index = unknownIdx\n",
    "                    unknownword_count += 1\n",
    "                char_index = []\n",
    "                for x in char:\n",
    "                    char_index.append(ch_to_id[x])\n",
    "                word_indices.append(word_index)\n",
    "                case_indices.append(get_case_value(word, cs_to_id))\n",
    "                pos_indices.append(pos_to_id[pos.replace('\\n','')])\n",
    "                char_indices.append(char_index)\n",
    "                label_indices.append(lb_to_id[label])\n",
    "\n",
    "            dataset.append([word_indices, case_indices, char_indices, pos_indices, label_indices]) \n",
    "        return dataset\n",
    "\n",
    "    def iterate_minibatches(self,dataset,batch_len): \n",
    "        start = 0\n",
    "        for i in batch_len:\n",
    "            tokens = []\n",
    "            char = []\n",
    "            labels = []\n",
    "            casing = []\n",
    "            pos_tags = []\n",
    "            data = dataset[start:i]\n",
    "            start = i\n",
    "            for dt in data:\n",
    "                t,c,ch,pos,l = dt\n",
    "                l = np.expand_dims(l,-1)\n",
    "                tokens.append(t)\n",
    "                char.append(ch)\n",
    "                labels.append(l)\n",
    "                casing.append(c)\n",
    "                pos_tags.append(pos)\n",
    "            yield np.asarray(labels),np.asarray(tokens),np.asarray(casing), np.asarray(char), np.asarray(pos_tags)\n",
    "\n",
    "    def addCharInformatioin(self,Sentences):\n",
    "        for i,sentence in enumerate(Sentences):\n",
    "            for j,data in enumerate(sentence):\n",
    "                chars = [c for c in data[0]]\n",
    "                Sentences[i][j] = [data[0],chars,data[1],data[2]]\n",
    "        return Sentences\n",
    "\n",
    "    def padding(self,Sentences):\n",
    "        maxlen = 52\n",
    "        for sentence in Sentences:\n",
    "            char = sentence[2]\n",
    "            for x in char:\n",
    "                maxlen = max(maxlen,len(x))\n",
    "        for i,sentence in enumerate(Sentences):\n",
    "            Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n",
    "        return Sentences\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
