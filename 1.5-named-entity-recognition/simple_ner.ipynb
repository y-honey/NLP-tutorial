{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.initializers import RandomUniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData():\n",
    "    def __init__(self):\n",
    "        self.train_files = None\n",
    "        self.validation_files = None\n",
    "        \n",
    "    def get_data(self):\n",
    "        self.train_files = glob.glob(\"benchmarking_data/Train//*.txt\")\n",
    "        self.validation_files = glob.glob(\"benchmarking_data/Validate//*.txt\")\n",
    "        \n",
    "    def sentence_from_file(self,filename):\n",
    "        single_data_list = list()\n",
    "        with open(filename) as fp:\n",
    "            sentence_list = []\n",
    "            lines = fp.readlines()\n",
    "            for line in lines:\n",
    "                splits = line.split(' ')\n",
    "                if splits[0]=='\\n':\n",
    "                    #sent = \" \".join([word[0] for word in sentence_list])\n",
    "                    #single_data_list.append((sentence_list,sent))\n",
    "                    single_data_list.append(sentence_list)\n",
    "                    sentence_list = list()\n",
    "                else:\n",
    "                    sentence_list.append((splits[0],splits[1],splits[-1].replace('\\n','')))\n",
    "                \n",
    "        return single_data_list\n",
    "    \n",
    "    def addCharInformatioin(self,Sentences):\n",
    "        for i,sentence in enumerate(Sentences):\n",
    "            for j,data in enumerate(sentence):\n",
    "                chars = [c for c in data[0]]\n",
    "                Sentences[i][j] = [data[0],chars,data[1],data[2]]\n",
    "        return Sentences\n",
    "    \n",
    "    def prepared_data(self,files):\n",
    "        list_sentences = list()\n",
    "        for each_file in files:\n",
    "            sentences = self.sentence_from_file(each_file)\n",
    "            #sentences = self.addCharInformatioin(sentences)\n",
    "            list_sentences+= sentences\n",
    "        return list_sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('book', 'NN', 'O'), ('The', 'DT', 'B-restaurant_name'), ('Middle', 'NNP', 'I-restaurant_name'), ('East', 'NNP', 'I-restaurant_name'), ('restaurant', 'NN', 'B-restaurant_type'), ('in', 'IN', 'O'), ('IN', 'NNP', 'B-state'), ('for', 'IN', 'O'), ('noon', 'NN', 'B-timeRange')], [('Book', 'VB', 'O'), ('a', 'DT', 'O'), ('table', 'NN', 'O'), ('at', 'IN', 'O'), ('T-Rex', 'NNP', 'B-restaurant_name'), ('distant', 'NN', 'B-spatial_relation'), ('from', 'IN', 'O'), ('Halsey', 'NNP', 'B-poi'), ('St', 'NNP', 'I-poi'), ('.', '.', 'O')], [(\"I'd\", 'NNP', 'O'), ('like', 'IN', 'O'), ('to', 'TO', 'O'), ('eat', 'VB', 'O'), ('at', 'IN', 'O'), ('a', 'DT', 'O'), ('taverna', 'NN', 'B-restaurant_type'), ('that', 'WDT', 'O'), ('serves', 'VBZ', 'O'), ('chili', 'JJ', 'B-served_dish'), ('con', 'NN', 'I-served_dish'), ('carne', 'NN', 'I-served_dish'), ('for', 'IN', 'O'), ('a', 'DT', 'O'), ('party', 'NN', 'O'), ('of', 'IN', 'O'), ('10', 'CD', 'B-party_size_number')], [('I', 'PRP', 'O'), ('have', 'VBP', 'O'), ('a', 'DT', 'O'), ('party', 'NN', 'O'), ('of', 'IN', 'O'), ('four', 'CD', 'B-party_size_number'), ('in', 'IN', 'O'), ('Japan', 'NNP', 'B-country'), ('and', 'CC', 'O'), ('need', 'VBP', 'O'), ('a', 'DT', 'O'), ('reservation', 'NN', 'O'), ('at', 'IN', 'O'), ('Rimsky-Korsakoffee', 'NNP', 'B-restaurant_name'), ('House', 'NNP', 'I-restaurant_name'), ('on', 'IN', 'O'), ('Aug.', 'NNP', 'B-timeRange'), ('the', 'DT', 'I-timeRange'), ('3rd', 'CD', 'I-timeRange'), ('.', '.', 'O')], [('Please', 'NNP', 'O'), ('make', 'VB', 'O'), ('a', 'DT', 'O'), ('restaurant', 'NN', 'B-restaurant_type'), ('reservation', 'NN', 'O'), ('for', 'IN', 'O'), ('somewhere', 'RB', 'O'), ('in', 'IN', 'O'), ('Mondovi', 'NNP', 'B-city'), (',', ',', 'O'), ('Connecticut', 'NNP', 'B-state'), ('.', '.', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "load_data_obj = LoadData()\n",
    "load_data_obj.get_data()\n",
    "trained_sen_list = load_data_obj.prepared_data(load_data_obj.train_files)\n",
    "validation_sen_list = load_data_obj.prepared_data(load_data_obj.validation_files)\n",
    "print(trained_sen_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    def __init__(self):\n",
    "        self.max_len = len(max(trained_sen_list))\n",
    "        \n",
    "    def make_data(self,data_list):\n",
    "        \n",
    "        \n",
    "        words = list()\n",
    "        for each_sent in data_list:\n",
    "            for each_item in each_sent:\n",
    "                words.append(each_item[0])\n",
    "        words = list(set(words))\n",
    "\n",
    "        \n",
    "        pos_tags = list()\n",
    "        for each_sent in data_list:\n",
    "            for each_item in each_sent:\n",
    "                pos_tags.append(each_item[1])\n",
    "        pos_tags = list(set(pos_tags))\n",
    "        \n",
    "        labels = list()\n",
    "        for each_sent in data_list:\n",
    "            for each_item in each_sent:\n",
    "                labels.append(each_item[2])\n",
    "        labels = list(set(labels))\n",
    "        \n",
    "        self.word2idx = {\"PAD\": 0, \"UNK\": 1}\n",
    "        self.word2idx.update({w: i for i, w in enumerate(words)})\n",
    "        self.num_words = len(self.word2idx)\n",
    "        \n",
    "        self.pos_tag2idx = {t: i for i, t in enumerate(pos_tags)}\n",
    "        self.num_pos_tags = len(self.pos_tag2idx)\n",
    "        \n",
    "        self.label2idx = {t: i for i, t in enumerate(labels)}\n",
    "        self.num_lables = len(self.label2idx)\n",
    "        \n",
    "    def word2features(self,data, word_dict):\n",
    "        word = data[0]\n",
    "        postag = data[1]\n",
    "        binary_map = {True:0,False:1,None:2}\n",
    "        features = [word_dict[word],binary_map[word.islower()], \n",
    "                    binary_map[word.isupper()], binary_map[word.istitle()], \n",
    "                    binary_map[word.isdigit()], self.pos_tag2idx[postag] ]\n",
    "        return features\n",
    "\n",
    "\n",
    "    def sent2features(self,sent,word_dict):\n",
    "        sentence_features = list()\n",
    "        for index in range(len(sent)):\n",
    "            sentence_features.append(self.word2features(sent[index],word_dict))\n",
    "                       \n",
    "        return sentence_features\n",
    "\n",
    "    def sent2labels(self,sent):\n",
    "        return [label for token, postag, label in sent]\n",
    "\n",
    "    def sent2tokens(self,sent):\n",
    "        return [token for token, postag, label in sent]\n",
    "    \n",
    "    def create_data(self,data_list):\n",
    "        self.sentences = data_list\n",
    "        maxlen = max([len(item) for item in data_list])\n",
    "        self.max_len = maxlen\n",
    "        x = [[self.word2idx[w[0]] for w in s] for s in self.sentences]\n",
    "        #x = pad_sequences(maxlen=maxlen, sequences=x, padding=\"post\",value=self.num_words - 1)\n",
    "        x = pad_sequences(maxlen=maxlen, sequences=x, padding=\"post\",value=self.word2idx[\"PAD\"])\n",
    "        #x = [self.sent2features(s,self.word2idx) for s in self.sentences]\n",
    "        #x = pad_sequences(maxlen=maxlen, sequences=x, padding=\"post\",value=[0,2,2,2,2,len(self.pos_tag2idx)])\n",
    "        print(x[2])\n",
    "        y = [[self.label2idx[w[2]] for w in s] for s in self.sentences]\n",
    "        y = pad_sequences(maxlen=maxlen, sequences=y, padding=\"post\", value=self.label2idx[\"O\"])\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8826  8239 10268  3302   870  2937  2239   918  5894  4851 11467  9474\n",
      "  1046  2937  6722  1753 10673     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "preprocess_obj = Preprocessing()\n",
    "preprocess_obj.make_data(trained_sen_list+validation_sen_list)\n",
    "x_train,y_train = preprocess_obj.create_data(trained_sen_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, monitor='acc', baseline=0.95):\n",
    "        self.monitor = monitor\n",
    "        self.baseline = baseline\n",
    "        self.training_stop = False\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.history={'loss': [],'acc': [],'val_loss': [],'val_acc': []}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs and logs.get(self.monitor) >= self.baseline:\n",
    "            print(\"\\nReached %2.2f%% accuracy, so stopping training!!\" %(self.baseline*100))\n",
    "            self.training_stop = True\n",
    "        \n",
    "        if self.training_stop:    \n",
    "            self.model.stop_training = True\n",
    "\n",
    "\n",
    "class CreateModel():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.max_len = preprocess_obj.max_len\n",
    "        self.num_words = preprocess_obj.num_words\n",
    "        self.num_labels = preprocess_obj.num_lables\n",
    "        self.posEmbeddings = np.identity(len(preprocess_obj.pos_tag2idx), dtype='float32') \n",
    "        \n",
    "    def train(self):\n",
    "        word_input = Input(shape=(self.max_len,))\n",
    "        model = Embedding(input_dim=self.num_words, output_dim=50, input_length=self.max_len)(word_input)\n",
    "        model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "        out = TimeDistributed(Dense(self.num_labels, activation=\"softmax\"))(model)\n",
    "        \n",
    "        self.model = Model(word_input,out)\n",
    "        self.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='nadam',metrics=[\"acc\"])\n",
    "        \n",
    "    def run(self,batch_size=32,epoch=5):\n",
    "        logdir = \"logs_tensorboard/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "        \n",
    "        val_acc = 0.99\n",
    "        monitor_param = 'val_acc'\n",
    "        \n",
    "        checkpoint = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min')\n",
    "        \n",
    "        #checkpoint = MyCallback(monitor=monitor_param,baseline=val_acc) \n",
    "        self.history = self.model.fit(self.x_train, self.y_train,\n",
    "                                     batch_size=batch_size, epochs=epoch,\n",
    "                                     validation_split=0.1,callbacks=[checkpoint,tensorboard_callback],\n",
    "                                     verbose=1)\n",
    "    def save_model(self,model_file):\n",
    "        self.model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12405 samples, validate on 1379 samples\n",
      "Epoch 1/100\n",
      "12405/12405 [==============================] - 25s 2ms/sample - loss: 0.7017 - acc: 0.8715 - val_loss: 0.7153 - val_acc: 0.8437\n",
      "Epoch 2/100\n",
      "12405/12405 [==============================] - 25s 2ms/sample - loss: 0.3138 - acc: 0.9211 - val_loss: 0.3802 - val_acc: 0.8973\n",
      "Epoch 3/100\n",
      "12405/12405 [==============================] - 25s 2ms/sample - loss: 0.1584 - acc: 0.9581 - val_loss: 0.2493 - val_acc: 0.9347\n",
      "Epoch 4/100\n",
      "12405/12405 [==============================] - 25s 2ms/sample - loss: 0.0906 - acc: 0.9768 - val_loss: 0.1886 - val_acc: 0.9480\n",
      "Epoch 5/100\n",
      "12405/12405 [==============================] - 25s 2ms/sample - loss: 0.0598 - acc: 0.9851 - val_loss: 0.1742 - val_acc: 0.9520\n",
      "Epoch 6/100\n",
      "12405/12405 [==============================] - 26s 2ms/sample - loss: 0.0427 - acc: 0.9896 - val_loss: 0.1386 - val_acc: 0.9606\n",
      "Epoch 7/100\n",
      "12405/12405 [==============================] - 25s 2ms/sample - loss: 0.0320 - acc: 0.9924 - val_loss: 0.1297 - val_acc: 0.9629\n",
      "Epoch 8/100\n",
      "12405/12405 [==============================] - 27s 2ms/sample - loss: 0.0252 - acc: 0.9942 - val_loss: 0.1297 - val_acc: 0.9650\n"
     ]
    }
   ],
   "source": [
    "model_obj = CreateModel()\n",
    "model_obj.train()\n",
    "model_obj.run(batch_size=32,epoch=100)\n",
    "model_obj.save_model(\"models/simple_ner_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction():\n",
    "    def __init__(self):\n",
    "        self.word2idx = preprocess_obj.word2idx\n",
    "        self.idx2label = {v: k for k,v in preprocess_obj.label2idx.items()}\n",
    "        self.model = model_obj.model\n",
    "        self.max_len = preprocess_obj.max_len\n",
    "    def predict(self,texts):\n",
    "        label_lists = list()\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            x = [[self.word2idx.get(word, self.word2idx[\"UNK\"]) for word in words]]\n",
    "            x = pad_sequences(maxlen=self.max_len, sequences=x,\n",
    "                          padding=\"post\", value=self.word2idx[\"PAD\"])\n",
    "            y_pred = self.model.predict(x)\n",
    "            print(\"Predicted Probabilities on Test Set:\\n\",y_pred.shape)\n",
    "            # taking tag class with maximum probability\n",
    "            pred_index = np.argmax(y_pred, axis=-1)\n",
    "            print(\"Predicted tag indices: \\n\",pred_index.shape)\n",
    "            preds = pred_index.flatten().tolist()\n",
    "            labels = [self.idx2label[ind] for ind in preds]\n",
    "            label_lists.append(labels)\n",
    "            \n",
    "            print([(words[idx],labels[idx]) for idx in range(len(words))])\n",
    "            #print(labels)\n",
    "        return label_lists\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_17 to have shape (35,) but got array with shape (22,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-5a7ca69fafcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m '''\n\u001b[1;32m      8\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Play the last track from Beyoncé off Spotify\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-0c5440984bd5>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     12\u001b[0m             x = pad_sequences(maxlen=self.max_len, sequences=x,\n\u001b[1;32m     13\u001b[0m                           padding=\"post\", value=self.word2idx[\"PAD\"])\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted Probabilities on Test Set:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# taking tag class with maximum probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    580\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    583\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_17 to have shape (35,) but got array with shape (22,)"
     ]
    }
   ],
   "source": [
    "#print(preprocess_obj.word2idx)\n",
    "pred_obj = Prediction()\n",
    "'''\n",
    "for item in validation_sen_list:\n",
    "    sent = \" \".join([self.word2idx[w[0]] for w in s] for item in self.sentences])\n",
    "    \n",
    "'''\n",
    "text = \"Play the last track from Beyoncé off Spotify\"\n",
    "y_pred = pred_obj.predict([text,text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
