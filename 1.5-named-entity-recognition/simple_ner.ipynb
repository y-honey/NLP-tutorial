{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.initializers import RandomUniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData():\n",
    "    def __init__(self):\n",
    "        self.train_files = None\n",
    "        self.validation_files = None\n",
    "        \n",
    "    def get_data(self):\n",
    "        self.train_files = glob.glob(\"benchmarking_data/Train//*.txt\")\n",
    "        self.validation_files = glob.glob(\"benchmarking_data/Validate//*.txt\")\n",
    "        \n",
    "    def sentence_from_file(self,filename):\n",
    "        single_data_list = list()\n",
    "        with open(filename) as fp:\n",
    "            sentence_list = []\n",
    "            lines = fp.readlines()\n",
    "            for line in lines:\n",
    "                splits = line.split(' ')\n",
    "                if splits[0]=='\\n':\n",
    "                    #sent = \" \".join([word[0] for word in sentence_list])\n",
    "                    #single_data_list.append((sentence_list,sent))\n",
    "                    single_data_list.append(sentence_list)\n",
    "                    sentence_list = list()\n",
    "                else:\n",
    "                    sentence_list.append((splits[0],splits[1],splits[-1].replace('\\n','')))\n",
    "                \n",
    "        return single_data_list\n",
    "    \n",
    "    def addCharInformatioin(self,Sentences):\n",
    "        for i,sentence in enumerate(Sentences):\n",
    "            for j,data in enumerate(sentence):\n",
    "                chars = [c for c in data[0]]\n",
    "                Sentences[i][j] = [data[0],chars,data[1],data[2]]\n",
    "        return Sentences\n",
    "    \n",
    "    def prepared_data(self,files):\n",
    "        list_sentences = list()\n",
    "        for each_file in files:\n",
    "            sentences = self.sentence_from_file(each_file)\n",
    "            #sentences = self.addCharInformatioin(sentences)\n",
    "            list_sentences+= sentences\n",
    "        return list_sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_obj = LoadData()\n",
    "load_data_obj.get_data()\n",
    "trained_sen_list = load_data_obj.prepared_data(load_data_obj.train_files)\n",
    "validation_sen_list = load_data_obj.prepared_data(load_data_obj.validation_files)\n",
    "print(trained_sen_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    def __init__(self):\n",
    "        self.max_len = len(max(trained_sen_list))\n",
    "        \n",
    "    def make_data(self,data_list):\n",
    "        \n",
    "        \n",
    "        words = list()\n",
    "        for each_sent in data_list:\n",
    "            for each_item in each_sent:\n",
    "                words.append(each_item[0])\n",
    "        words = list(set(words))\n",
    "\n",
    "        \n",
    "        pos_tags = list()\n",
    "        for each_sent in data_list:\n",
    "            for each_item in each_sent:\n",
    "                pos_tags.append(each_item[1])\n",
    "        pos_tags = list(set(pos_tags))\n",
    "        \n",
    "        labels = list()\n",
    "        for each_sent in data_list:\n",
    "            for each_item in each_sent:\n",
    "                labels.append(each_item[2])\n",
    "        labels = list(set(labels))\n",
    "        \n",
    "        self.word2idx = {\"PAD\": 0, \"UNK\": 1}\n",
    "        self.word2idx.update({w: i for i, w in enumerate(words)})\n",
    "        self.num_words = len(self.word2idx)\n",
    "        \n",
    "        self.pos_tag2idx = {t: i for i, t in enumerate(pos_tags)}\n",
    "        self.num_pos_tags = len(self.pos_tag2idx)\n",
    "        \n",
    "        self.label2idx = {t: i for i, t in enumerate(labels)}\n",
    "        self.num_lables = len(self.label2idx)\n",
    "        \n",
    "    def word2features(self,data, word_dict):\n",
    "        word = data[0]\n",
    "        postag = data[1]\n",
    "        binary_map = {True:0,False:1,None:2}\n",
    "        features = [word_dict[word],binary_map[word.islower()], \n",
    "                    binary_map[word.isupper()], binary_map[word.istitle()], \n",
    "                    binary_map[word.isdigit()], self.pos_tag2idx[postag] ]\n",
    "        return features\n",
    "\n",
    "\n",
    "    def sent2features(self,sent,word_dict):\n",
    "        sentence_features = list()\n",
    "        for index in range(len(sent)):\n",
    "            sentence_features.append(self.word2features(sent[index],word_dict))\n",
    "                       \n",
    "        return sentence_features\n",
    "\n",
    "    def sent2labels(self,sent):\n",
    "        return [label for token, postag, label in sent]\n",
    "\n",
    "    def sent2tokens(self,sent):\n",
    "        return [token for token, postag, label in sent]\n",
    "    \n",
    "    def create_data(self,data_list):\n",
    "        self.sentences = data_list\n",
    "        maxlen = max([len(item) for item in data_list])\n",
    "        self.max_len = maxlen\n",
    "        x = [[self.word2idx[w[0]] for w in s] for s in self.sentences]\n",
    "        #x = pad_sequences(maxlen=maxlen, sequences=x, padding=\"post\",value=self.num_words - 1)\n",
    "        x = pad_sequences(maxlen=maxlen, sequences=x, padding=\"post\",value=self.word2idx[\"PAD\"])\n",
    "        #x = [self.sent2features(s,self.word2idx) for s in self.sentences]\n",
    "        #x = pad_sequences(maxlen=maxlen, sequences=x, padding=\"post\",value=[0,2,2,2,2,len(self.pos_tag2idx)])\n",
    "        print(x[2])\n",
    "        y = [[self.label2idx[w[2]] for w in s] for s in self.sentences]\n",
    "        y = pad_sequences(maxlen=maxlen, sequences=y, padding=\"post\", value=self.label2idx[\"O\"])\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_obj = Preprocessing()\n",
    "preprocess_obj.make_data(trained_sen_list+validation_sen_list)\n",
    "x_train,y_train = preprocess_obj.create_data(trained_sen_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, monitor='acc', baseline=0.95):\n",
    "        self.monitor = monitor\n",
    "        self.baseline = baseline\n",
    "        self.training_stop = False\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.history={'loss': [],'acc': [],'val_loss': [],'val_acc': []}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs and logs.get(self.monitor) >= self.baseline:\n",
    "            print(\"\\nReached %2.2f%% accuracy, so stopping training!!\" %(self.baseline*100))\n",
    "            self.training_stop = True\n",
    "        \n",
    "        if self.training_stop:    \n",
    "            self.model.stop_training = True\n",
    "\n",
    "\n",
    "class CreateModel():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.max_len = preprocess_obj.max_len\n",
    "        self.num_words = preprocess_obj.num_words\n",
    "        self.num_labels = preprocess_obj.num_lables\n",
    "        self.posEmbeddings = np.identity(len(preprocess_obj.pos_tag2idx), dtype='float32') \n",
    "        \n",
    "    def train(self):\n",
    "        word_input = Input(shape=(self.max_len,))\n",
    "        model = Embedding(input_dim=self.num_words, output_dim=50, input_length=self.max_len)(word_input)\n",
    "        model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "        out = TimeDistributed(Dense(self.num_labels, activation=\"softmax\"))(model)\n",
    "        \n",
    "        self.model = Model(word_input,out)\n",
    "        self.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='nadam',metrics=[\"acc\"])\n",
    "        \n",
    "    def run(self,batch_size=32,epoch=5):\n",
    "        logdir = \"logs_tensorboard/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "        \n",
    "        val_acc = 0.99\n",
    "        monitor_param = 'val_acc'\n",
    "        \n",
    "        checkpoint = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min')\n",
    "        \n",
    "        #checkpoint = MyCallback(monitor=monitor_param,baseline=val_acc) \n",
    "        self.history = self.model.fit(self.x_train, self.y_train,\n",
    "                                     batch_size=batch_size, epochs=epoch,\n",
    "                                     validation_split=0.1,callbacks=[checkpoint,tensorboard_callback],\n",
    "                                     verbose=1)\n",
    "    def save_model(self,model_file):\n",
    "        self.model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj = CreateModel()\n",
    "model_obj.train()\n",
    "model_obj.run(batch_size=32,epoch=100)\n",
    "model_obj.save_model(\"models/simple_ner_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction():\n",
    "    def __init__(self):\n",
    "        self.word2idx = preprocess_obj.word2idx\n",
    "        self.idx2label = {v: k for k,v in preprocess_obj.label2idx.items()}\n",
    "        self.model = model_obj.model\n",
    "        self.max_len = preprocess_obj.max_len\n",
    "    def predict(self,texts):\n",
    "        label_lists = list()\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            x = [[self.word2idx.get(word, self.word2idx[\"UNK\"]) for word in words]]\n",
    "            x = pad_sequences(maxlen=self.max_len, sequences=x,\n",
    "                          padding=\"post\", value=self.word2idx[\"PAD\"])\n",
    "            y_pred = self.model.predict(x)\n",
    "            print(\"Predicted Probabilities on Test Set:\\n\",y_pred.shape)\n",
    "            # taking tag class with maximum probability\n",
    "            pred_index = np.argmax(y_pred, axis=-1)\n",
    "            print(\"Predicted tag indices: \\n\",pred_index.shape)\n",
    "            preds = pred_index.flatten().tolist()\n",
    "            labels = [self.idx2label[ind] for ind in preds]\n",
    "            label_lists.append(labels)\n",
    "            \n",
    "            print([(words[idx],labels[idx]) for idx in range(len(words))])\n",
    "            #print(labels)\n",
    "        return label_lists\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(preprocess_obj.word2idx)\n",
    "pred_obj = Prediction()\n",
    "'''\n",
    "for item in validation_sen_list:\n",
    "    sent = \" \".join([self.word2idx[w[0]] for w in s] for item in self.sentences])\n",
    "    \n",
    "'''\n",
    "text = \"Play the last track from Beyoncé off Spotify\"\n",
    "y_pred = pred_obj.predict([text,text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
