{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import nltk \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.initializers import RandomUniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData():\n",
    "    def __init__(self):\n",
    "        self.train_files = None\n",
    "        self.validation_files = None\n",
    "        \n",
    "    def get_data(self):\n",
    "        self.train_files = glob.glob(\"benchmarking_data/Train//*.txt\")\n",
    "        self.validation_files = glob.glob(\"benchmarking_data/Validate//*.txt\")\n",
    "        \n",
    "    def sentence_from_file(self,filename):\n",
    "        single_data_list = list()\n",
    "        with open(filename) as fp:\n",
    "            sentence_list = []\n",
    "            lines = fp.readlines()\n",
    "            for line in lines:\n",
    "                splits = line.split(' ')\n",
    "                if splits[0]=='\\n':\n",
    "                    #sent = \" \".join([word[0] for word in sentence_list])\n",
    "                    #single_data_list.append((sentence_list,sent))\n",
    "                    single_data_list.append(sentence_list)\n",
    "                    sentence_list = list()\n",
    "                else:\n",
    "                    sentence_list.append((splits[0],splits[1],splits[-1].replace('\\n','')))\n",
    "                \n",
    "        return single_data_list\n",
    "    \n",
    "    def addCharInformatioin(self,Sentences):\n",
    "        for i,sentence in enumerate(Sentences):\n",
    "            for j,data in enumerate(sentence):\n",
    "                chars = [c for c in data[0]]\n",
    "                Sentences[i][j] = [data[0],chars,data[1],data[2]]\n",
    "        return Sentences\n",
    "    \n",
    "    def prepared_data(self,files):\n",
    "        list_sentences = list()\n",
    "        for each_file in files:\n",
    "            sentences = self.sentence_from_file(each_file)\n",
    "            #sentences = self.addCharInformatioin(sentences)\n",
    "            list_sentences+= sentences\n",
    "        return list_sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('book', 'NN', 'O'), ('The', 'DT', 'B-restaurant_name'), ('Middle', 'NNP', 'I-restaurant_name'), ('East', 'NNP', 'I-restaurant_name'), ('restaurant', 'NN', 'B-restaurant_type'), ('in', 'IN', 'O'), ('IN', 'NNP', 'B-state'), ('for', 'IN', 'O'), ('noon', 'NN', 'B-timeRange')], [('Book', 'VB', 'O'), ('a', 'DT', 'O'), ('table', 'NN', 'O'), ('at', 'IN', 'O'), ('T-Rex', 'NNP', 'B-restaurant_name'), ('distant', 'NN', 'B-spatial_relation'), ('from', 'IN', 'O'), ('Halsey', 'NNP', 'B-poi'), ('St', 'NNP', 'I-poi'), ('.', '.', 'O')], [(\"I'd\", 'NNP', 'O'), ('like', 'IN', 'O'), ('to', 'TO', 'O'), ('eat', 'VB', 'O'), ('at', 'IN', 'O'), ('a', 'DT', 'O'), ('taverna', 'NN', 'B-restaurant_type'), ('that', 'WDT', 'O'), ('serves', 'VBZ', 'O'), ('chili', 'JJ', 'B-served_dish'), ('con', 'NN', 'I-served_dish'), ('carne', 'NN', 'I-served_dish'), ('for', 'IN', 'O'), ('a', 'DT', 'O'), ('party', 'NN', 'O'), ('of', 'IN', 'O'), ('10', 'CD', 'B-party_size_number')], [('I', 'PRP', 'O'), ('have', 'VBP', 'O'), ('a', 'DT', 'O'), ('party', 'NN', 'O'), ('of', 'IN', 'O'), ('four', 'CD', 'B-party_size_number'), ('in', 'IN', 'O'), ('Japan', 'NNP', 'B-country'), ('and', 'CC', 'O'), ('need', 'VBP', 'O'), ('a', 'DT', 'O'), ('reservation', 'NN', 'O'), ('at', 'IN', 'O'), ('Rimsky-Korsakoffee', 'NNP', 'B-restaurant_name'), ('House', 'NNP', 'I-restaurant_name'), ('on', 'IN', 'O'), ('Aug.', 'NNP', 'B-timeRange'), ('the', 'DT', 'I-timeRange'), ('3rd', 'CD', 'I-timeRange'), ('.', '.', 'O')], [('Please', 'NNP', 'O'), ('make', 'VB', 'O'), ('a', 'DT', 'O'), ('restaurant', 'NN', 'B-restaurant_type'), ('reservation', 'NN', 'O'), ('for', 'IN', 'O'), ('somewhere', 'RB', 'O'), ('in', 'IN', 'O'), ('Mondovi', 'NNP', 'B-city'), (',', ',', 'O'), ('Connecticut', 'NNP', 'B-state'), ('.', '.', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "load_data_obj = LoadData()\n",
    "load_data_obj.get_data()\n",
    "trained_sen_list = load_data_obj.prepared_data(load_data_obj.train_files)\n",
    "validation_sen_list = load_data_obj.prepared_data(load_data_obj.validation_files)\n",
    "print(trained_sen_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    def __init__(self):\n",
    "        self.max_len = len(max(trained_sen_list))\n",
    "        \n",
    "    def make_data(self,data_list):\n",
    "        \n",
    "        \n",
    "        words = list()\n",
    "        for each_sent in data_list:\n",
    "            for each_item in each_sent:\n",
    "                words.append(each_item[0])\n",
    "        words = list(set(words))\n",
    "\n",
    "        \n",
    "        pos_tags = list()\n",
    "        for each_sent in data_list:\n",
    "            for each_item in each_sent:\n",
    "                pos_tags.append(each_item[1])\n",
    "        pos_tags = list(set(pos_tags))\n",
    "        \n",
    "        labels = list()\n",
    "        for each_sent in data_list:\n",
    "            for each_item in each_sent:\n",
    "                labels.append(each_item[2])\n",
    "        labels = list(set(labels))\n",
    "        \n",
    "        \n",
    "        self.word2idx = {w: i for i, w in enumerate(words)}\n",
    "        self.word2idx.update({\"PAD\": len(self.word2idx), \"UNK\": len(self.word2idx)+1})\n",
    "        self.num_words = len(self.word2idx)\n",
    "        \n",
    "        self.pos_tag2idx = {t: i for i, t in enumerate(pos_tags)}\n",
    "        self.pos_tag2idx.update({\"PAD\": len(self.pos_tag2idx), \"UNK\": len(self.pos_tag2idx)+1})\n",
    "        self.num_pos_tags = len(self.pos_tag2idx)\n",
    "        \n",
    "        self.label2idx = {t: i for i, t in enumerate(labels)}\n",
    "        self.num_lables = len(self.label2idx)\n",
    "        \n",
    "    def word2features(self,data, word_dict):\n",
    "        word = data[0]\n",
    "        postag = data[1]\n",
    "        binary_map = {True:0,False:1,None:2}\n",
    "        features = [word_dict[word],binary_map[word.islower()], \n",
    "                    binary_map[word.isupper()], binary_map[word.istitle()], \n",
    "                    binary_map[word.isdigit()], self.pos_tag2idx[postag] ]\n",
    "        return features\n",
    "\n",
    "\n",
    "    def sent2features(self,sent,word_dict):\n",
    "        sentence_features = list()\n",
    "        for index in range(len(sent)):\n",
    "            sentence_features.append(self.word2features(sent[index],word_dict))\n",
    "                       \n",
    "        return sentence_features\n",
    "\n",
    "    def sent2labels(self,sent):\n",
    "        return [label for token, postag, label in sent]\n",
    "\n",
    "    def sent2tokens(self,sent):\n",
    "        return [token for token, postag, label in sent]\n",
    "    \n",
    "    def create_data(self,data_list):\n",
    "        self.sentences = data_list\n",
    "        maxlen = max([len(item) for item in data_list])\n",
    "        self.max_len = maxlen\n",
    "        wd = [[self.word2idx[w[0]] for w in s] for s in self.sentences]\n",
    "        \n",
    "        wd = pad_sequences(maxlen=maxlen, sequences=wd, padding=\"post\",value=self.word2idx[\"PAD\"])\n",
    "        \n",
    "        pos = [[self.pos_tag2idx[w[1]] for w in s] for s in self.sentences]\n",
    "        pos = pad_sequences(maxlen=maxlen, sequences=pos, padding=\"post\",value=self.pos_tag2idx[\"PAD\"])\n",
    "\n",
    "        y = [[self.label2idx[w[2]] for w in s] for s in self.sentences]\n",
    "        y = pad_sequences(maxlen=maxlen, sequences=y, padding=\"post\", value=self.label2idx[\"O\"])\n",
    "        return (wd,pos),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_obj = Preprocessing()\n",
    "preprocess_obj.make_data(trained_sen_list+validation_sen_list)\n",
    "x_train,y_train = preprocess_obj.create_data(trained_sen_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, monitor='acc', baseline=0.95):\n",
    "        self.monitor = monitor\n",
    "        self.baseline = baseline\n",
    "        self.training_stop = False\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.history={'loss': [],'acc': [],'val_loss': [],'val_acc': []}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs and logs.get(self.monitor) >= self.baseline:\n",
    "            print(\"\\nReached %2.2f%% accuracy, so stopping training!!\" %(self.baseline*100))\n",
    "            self.training_stop = True\n",
    "        \n",
    "        if self.training_stop:    \n",
    "            self.model.stop_training = True\n",
    "\n",
    "\n",
    "class CreateModel():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.max_len = preprocess_obj.max_len\n",
    "        self.num_words = preprocess_obj.num_words\n",
    "        self.num_labels = preprocess_obj.num_lables\n",
    "        self.posEmbeddings = np.identity(len(preprocess_obj.pos_tag2idx), dtype='float32') \n",
    "        \n",
    "    def train(self):\n",
    "        word_input = Input(shape=(self.max_len,))\n",
    "        word_model = Embedding(input_dim=self.num_words, output_dim=50, input_length=self.max_len)(word_input)\n",
    "        \n",
    "        pos_input = Input(shape=(None,), dtype='int32')\n",
    "        pos_model = Embedding(output_dim = self.posEmbeddings.shape[1], input_dim = self.posEmbeddings.shape[0], weights = [self.posEmbeddings], trainable=False)(pos_input)\n",
    "\n",
    "        output = concatenate([word_model, pos_model])\n",
    "        \n",
    "        output = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(output)\n",
    "        output = TimeDistributed(Dense(self.num_labels, activation=\"softmax\"))(output)\n",
    "        \n",
    "        self.model = Model(inputs=[word_input, pos_input], outputs=[output])\n",
    "        self.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='nadam',metrics=[\"acc\"])\n",
    "        \n",
    "    def run(self,batch_size=32,epoch=5):\n",
    "        logdir = \"logs_tensorboard/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        logdir = \"logs_tensorboard\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "        \n",
    "        val_acc = 0.99\n",
    "        monitor_param = 'val_acc'\n",
    "        \n",
    "        checkpoint = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min')\n",
    "        \n",
    "        #checkpoint = MyCallback(monitor=monitor_param,baseline=val_acc) \n",
    "        self.history = self.model.fit(self.x_train, self.y_train,\n",
    "                                     batch_size=batch_size, epochs=epoch,\n",
    "                                     validation_split=0.1,callbacks=[checkpoint,tensorboard_callback],\n",
    "                                     verbose=1)\n",
    "    def save_model(self,model_file):\n",
    "        self.model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12405 samples, validate on 1379 samples\n",
      "Epoch 1/100\n",
      "12405/12405 [==============================] - 36s 3ms/sample - loss: 0.6159 - acc: 0.8815 - val_loss: 0.5973 - val_acc: 0.8457\n",
      "Epoch 2/100\n",
      "12405/12405 [==============================] - 35s 3ms/sample - loss: 0.1891 - acc: 0.9511 - val_loss: 0.2524 - val_acc: 0.9344\n",
      "Epoch 3/100\n",
      "12405/12405 [==============================] - 33s 3ms/sample - loss: 0.0867 - acc: 0.9776 - val_loss: 0.1808 - val_acc: 0.9514\n",
      "Epoch 4/100\n",
      "12405/12405 [==============================] - 41s 3ms/sample - loss: 0.0501 - acc: 0.9874 - val_loss: 0.1435 - val_acc: 0.9603\n",
      "Epoch 5/100\n",
      "12405/12405 [==============================] - 50s 4ms/sample - loss: 0.0328 - acc: 0.9919 - val_loss: 0.1313 - val_acc: 0.9627\n",
      "Epoch 6/100\n",
      "12405/12405 [==============================] - 42s 3ms/sample - loss: 0.0232 - acc: 0.9945 - val_loss: 0.1280 - val_acc: 0.9642\n",
      "Epoch 7/100\n",
      "12405/12405 [==============================] - 40s 3ms/sample - loss: 0.0167 - acc: 0.9962 - val_loss: 0.1158 - val_acc: 0.9665\n",
      "Epoch 8/100\n",
      "12405/12405 [==============================] - 39s 3ms/sample - loss: 0.0124 - acc: 0.9973 - val_loss: 0.1169 - val_acc: 0.9676\n"
     ]
    }
   ],
   "source": [
    "model_obj = CreateModel()\n",
    "model_obj.train()\n",
    "model_obj.run(batch_size=32,epoch=100)\n",
    "model_obj.save_model(\"models/simple_ner_model_v2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction():\n",
    "    def __init__(self):\n",
    "        self.word2idx = preprocess_obj.word2idx\n",
    "        self.pos_tag2idx = preprocess_obj.pos_tag2idx\n",
    "        self.idx2label = {v: k for k,v in preprocess_obj.label2idx.items()}\n",
    "        self.model = model_obj.model\n",
    "        self.max_len = preprocess_obj.max_len\n",
    "    def predict(self,texts):\n",
    "        label_lists = list()\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            tagged = nltk.pos_tag(words) \n",
    "            \n",
    "            wd = [[self.word2idx.get(word, self.word2idx[\"UNK\"]) for word in words]]\n",
    "            wd = pad_sequences(maxlen=self.max_len, sequences=wd,\n",
    "                          padding=\"post\", value=self.word2idx[\"PAD\"])\n",
    "            \n",
    "            pos = [[self.pos_tag2idx.get(item, self.pos_tag2idx[\"UNK\"]) for item in tagged]]\n",
    "            pos = pad_sequences(maxlen=self.max_len, sequences=pos,\n",
    "                          padding=\"post\", value=self.pos_tag2idx[\"PAD\"])\n",
    "            \n",
    "            y_pred = self.model.predict([wd,pos])\n",
    "            pred_index = np.argmax(y_pred, axis=-1)\n",
    "            preds = pred_index.flatten().tolist()\n",
    "            labels = [self.idx2label[ind] for ind in preds]\n",
    "            label_lists.append(labels)\n",
    "            \n",
    "            print([(words[idx],labels[idx]) for idx in range(len(words))])\n",
    "        return label_lists\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Play', 'O'), ('the', 'O'), ('last', 'B-sort'), ('track', 'B-music_item'), ('from', 'O'), ('Beyonce', 'O'), ('off', 'O'), ('Spotify', 'B-service')]\n"
     ]
    }
   ],
   "source": [
    "#print(preprocess_obj.word2idx)\n",
    "pred_obj = Prediction()\n",
    "text = \"Play the last track from Beyonce off Spotify\"\n",
    "y_pred = pred_obj.predict([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
