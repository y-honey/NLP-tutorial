{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Language Model Using Tensorflow & keras<h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Importing Libraries<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Preprocessing Data<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    \n",
    "    def __init__(self,input_file):\n",
    "        self.input_data_file = input_file\n",
    "        self.data = None\n",
    "        self.vocab_size = None\n",
    "        self.encoded_data = None\n",
    "        self.max_length = None\n",
    "        self.sequences = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.tokenizer = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        fp = open(self.input_data_file,'r')\n",
    "        self.data = fp.read().splitlines()        \n",
    "        fp.close()\n",
    "        \n",
    "    def encode_data(self):\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.tokenizer.fit_on_texts(self.data)\n",
    "        self.encoded_data = self.tokenizer.texts_to_sequences(self.data)\n",
    "        print(self.encoded_data)\n",
    "        self.vocab_size = len(self.tokenizer.word_counts)+1\n",
    "        \n",
    "    def generate_sequence(self):\n",
    "        seq_list = list()\n",
    "        for item in self.encoded_data:\n",
    "            l = len(item)\n",
    "            for id in range(1,l):\n",
    "                seq_list.append(item[:id+1])\n",
    "        self.max_length = max([len(seq) for seq in seq_list])\n",
    "        self.sequences = pad_sequences(seq_list, maxlen=self.max_length, padding='pre')\n",
    "        print(self.sequences)\n",
    "        self.sequences = array(self.sequences)\n",
    "            \n",
    "    def get_data(self):\n",
    "        self.x = self.sequences[:,:-1]\n",
    "        self.y = self.sequences[:,-1]\n",
    "        print(\"y before:\",self.y)\n",
    "        self.y = to_categorical(self.y,num_classes=self.vocab_size)\n",
    "        print(\"y After:\",self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 1, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13], [2, 14, 15, 1, 16, 17, 18], [1, 3, 19, 20, 21]]\n",
      "[[ 0  0  0  0  0  2  1]\n",
      " [ 0  0  0  0  2  1  3]\n",
      " [ 0  0  0  2  1  3  4]\n",
      " [ 0  0  2  1  3  4  5]\n",
      " [ 0  2  1  3  4  5  6]\n",
      " [ 2  1  3  4  5  6  7]\n",
      " [ 0  0  0  0  0  8  9]\n",
      " [ 0  0  0  0  8  9 10]\n",
      " [ 0  0  0  8  9 10 11]\n",
      " [ 0  0  8  9 10 11 12]\n",
      " [ 0  8  9 10 11 12 13]\n",
      " [ 0  0  0  0  0  2 14]\n",
      " [ 0  0  0  0  2 14 15]\n",
      " [ 0  0  0  2 14 15  1]\n",
      " [ 0  0  2 14 15  1 16]\n",
      " [ 0  2 14 15  1 16 17]\n",
      " [ 2 14 15  1 16 17 18]\n",
      " [ 0  0  0  0  0  1  3]\n",
      " [ 0  0  0  0  1  3 19]\n",
      " [ 0  0  0  1  3 19 20]\n",
      " [ 0  0  1  3 19 20 21]]\n",
      "y before: [ 1  3  4  5  6  7  9 10 11 12 13 14 15  1 16 17 18  3 19 20 21]\n",
      "y After: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "pr = Preprocessing('data.txt')\n",
    "pr.load_data()\n",
    "pr.encode_data()\n",
    "pr.generate_sequence()\n",
    "pr.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self,params):\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.vocab_size = params['vocab_size']\n",
    "        self.max_len = params['max_len']\n",
    "        self.activation = params['activation']\n",
    "        self.optimizer = params['optimizer']\n",
    "        self.epochs = params['epochs']\n",
    "        self.metrics = params['metrics']\n",
    "        \n",
    "        \n",
    "    def create_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(self.vocab_size,10,input_length=self.max_len-1))\n",
    "        self.model.add(LSTM(50))\n",
    "        self.model.add(Dropout(0.1))\n",
    "        self.model.add(Dense(self.vocab_size,activation=self.activation))\n",
    "        self.model.compile(loss='categorical_crossentropy',optimizer=self.optimizer,metrics=self.metrics)\n",
    "        print(self.model.summary())\n",
    "    def run(self):\n",
    "        self.history = self.model.fit(self.x,self.y,epochs=self.epochs)\n",
    "        \n",
    "    def save(self):\n",
    "        self.model.save(\"lang_model.h5\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"activation\":\"softmax\",\"epochs\":500,\"verbose\":2,\"loss\":\"categorical_crossentropy\",\n",
    "          \"optimizer\":\"adam\",\"metrics\":['accuracy'],\"vocab_size\":pr.vocab_size,\"max_len\":pr.max_length}\n",
    "model_obj = Model(params)\n",
    "model_obj.x = pr.x\n",
    "model_obj.y = pr.y\n",
    "model_obj.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj.run()\n",
    "model_obj.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction():\n",
    "    def __init__(self,tokenizer,max_len):\n",
    "        self.model = None\n",
    "        self.tokenizer = tokenizer\n",
    "        self.idx2word = {v:k for k,v in self.tokenizer.word_index.items()}\n",
    "        self.max_length = max_len\n",
    "    \n",
    "    def load_model(self):\n",
    "        self.model = load_model(\"lang_model.h5\")\n",
    "        \n",
    "    def predict_sequnce(self,text,num_words):\n",
    "        for id in range(num_words):\n",
    "            encoded_data = self.tokenizer.texts_to_sequences([text])[0]\n",
    "            padded_data = pad_sequences([encoded_data],maxlen = self.max_length-1,padding='pre')\n",
    "            y_pred = self.model.predict(padded_data)\n",
    "            y_pred = np.argmax(y_pred)\n",
    "            predict_word = self.idx2word[y_pred]\n",
    "            text += ' ' + predict_word\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Prediction(pr.tokenizer,pr.max_length)    \n",
    "pred.load_model()\n",
    "print(pred.predict_sequnce(\"Jack and\",5))\n",
    "print(pred.predict_sequnce('And Jill', 4))\n",
    "print(pred.predict_sequnce('fell down', 5))\n",
    "print(pred.predict_sequnce('pail of', 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
