{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def fit_on_texts(self,list_data):\n",
    "        word_list = \" \".join(list_data).split()\n",
    "        self.word_counts = list(set(word_list))\n",
    "        self.word_dict = {w: i for i, w in enumerate(self.word_counts)}\n",
    "        self.number_dict = {i: w for i, w in enumerate(self.word_counts)}\n",
    "        \n",
    "    def texts_to_sequences(self,data):\n",
    "        encoded_sequence = list()\n",
    "        for item in data:\n",
    "            encoded_sequence.append([self.word_dict[word] for word in item.split()])\n",
    "        return encoded_sequence\n",
    "    \n",
    "def pad_sequences(data,padding='pre',padding_value=0):\n",
    "    sequence = None\n",
    "    if isinstance(data,list):\n",
    "        maxlen = max(len(item) for item in data)\n",
    "        \n",
    "    if padding == 'pre':\n",
    "        for idx in range(len(data)):\n",
    "            data[idx] = [padding_value]*(maxlen-len(data[idx])) + data[idx]\n",
    "    else:\n",
    "        for idx in range(len(data)):\n",
    "            data[idx] = data[idx]+ [padding_value]*(maxlen-len(data[idx]))\n",
    "                                                    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    \n",
    "    def __init__(self,input_file):\n",
    "        self.input_data_file = input_file\n",
    "        self.data = None\n",
    "        self.vocab_size = None\n",
    "        self.encoded_data = None\n",
    "        self.max_length = None\n",
    "        self.sequences = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.tokenizer = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        fp = open(self.input_data_file,'r')\n",
    "        self.data = fp.read().splitlines()        \n",
    "        fp.close()\n",
    "        \n",
    "    def encode_data(self):\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.tokenizer.fit_on_texts(self.data)\n",
    "        self.encoded_data = self.tokenizer.texts_to_sequences(self.data)\n",
    "        print(self.encoded_data)\n",
    "        self.vocab_size = len(self.tokenizer.word_counts)+1\n",
    "        \n",
    "    def generate_sequence(self):\n",
    "        seq_list = list()\n",
    "        for item in self.encoded_data:\n",
    "            l = len(item)\n",
    "            for id in range(1,l):\n",
    "                seq_list.append(item[:id+1])\n",
    "        #print(seq_list[0])\n",
    "        print(seq_list)\n",
    "        self.sequences = pad_sequences(seq_list,padding='pre', padding_value=0)\n",
    "        print(self.sequences)\n",
    "        self.sequences = array(self.sequences)\n",
    "            \n",
    "    def get_data(self):\n",
    "        self.x = self.sequences[:,:-1]\n",
    "        self.y = self.sequences[:,-1]\n",
    "        print(self.y)\n",
    "        #self.y = to_categorical(self.y,num_classes=self.vocab_size)\n",
    "        self.y = pd.get_dummies(self.y)\n",
    "        print(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 12, 16, 4, 3, 14, 9], [17, 0, 18, 6, 19, 21], [2, 10, 8, 12, 11, 7, 15], [1, 16, 5, 20, 13]]\n",
      "[[2, 12], [2, 12, 16], [2, 12, 16, 4], [2, 12, 16, 4, 3], [2, 12, 16, 4, 3, 14], [2, 12, 16, 4, 3, 14, 9], [17, 0], [17, 0, 18], [17, 0, 18, 6], [17, 0, 18, 6, 19], [17, 0, 18, 6, 19, 21], [2, 10], [2, 10, 8], [2, 10, 8, 12], [2, 10, 8, 12, 11], [2, 10, 8, 12, 11, 7], [2, 10, 8, 12, 11, 7, 15], [1, 16], [1, 16, 5], [1, 16, 5, 20], [1, 16, 5, 20, 13]]\n",
      "[[0, 0, 0, 0, 0, 2, 12], [0, 0, 0, 0, 2, 12, 16], [0, 0, 0, 2, 12, 16, 4], [0, 0, 2, 12, 16, 4, 3], [0, 2, 12, 16, 4, 3, 14], [2, 12, 16, 4, 3, 14, 9], [0, 0, 0, 0, 0, 17, 0], [0, 0, 0, 0, 17, 0, 18], [0, 0, 0, 17, 0, 18, 6], [0, 0, 17, 0, 18, 6, 19], [0, 17, 0, 18, 6, 19, 21], [0, 0, 0, 0, 0, 2, 10], [0, 0, 0, 0, 2, 10, 8], [0, 0, 0, 2, 10, 8, 12], [0, 0, 2, 10, 8, 12, 11], [0, 2, 10, 8, 12, 11, 7], [2, 10, 8, 12, 11, 7, 15], [0, 0, 0, 0, 0, 1, 16], [0, 0, 0, 0, 1, 16, 5], [0, 0, 0, 1, 16, 5, 20], [0, 0, 1, 16, 5, 20, 13]]\n",
      "[12 16  4  3 14  9  0 18  6 19 21 10  8 12 11  7 15 16  5 20 13]\n",
      "    0   3   4   5   6   7   8   9   10  11  12  13  14  15  16  18  19  20  21\n",
      "0    0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0\n",
      "1    0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n",
      "2    0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "3    0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "4    0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "5    0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "6    1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "7    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0\n",
      "8    0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "9    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0\n",
      "10   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "11   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "12   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "13   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0\n",
      "14   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n",
      "15   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "16   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0\n",
      "17   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n",
      "18   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "19   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n",
      "20   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "pr = Preprocessing('data.txt')\n",
    "pr.load_data()\n",
    "pr.encode_data()\n",
    "pr.generate_sequence()\n",
    "pr.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
