{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class language_model():\n",
    "    def __init__(self,data,params):\n",
    "        self.data = data\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.max_length = None\n",
    "        self.params = params\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def preprocessing(self):\n",
    "        \n",
    "        # integer encode sequences of words\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.tokenizer.fit_on_texts([data])\n",
    "        encoded = self.tokenizer.texts_to_sequences([data])[0]\n",
    "        \n",
    "        # retrieve vocabulary size\n",
    "        vocab_size = len(self.tokenizer.word_index) + 1\n",
    "        print('Vocabulary Size: %d' % vocab_size)\n",
    "        \n",
    "        # encode 2 words -> 1 word\n",
    "        sequences = list()\n",
    "        for i in range(2, len(encoded)):\n",
    "            sequence = encoded[i-2:i+1]\n",
    "            sequences.append(sequence)\n",
    "        print('Total Sequences: %d' % len(sequences))\n",
    "        \n",
    "        # pad sequences\n",
    "        self.max_length = max([len(seq) for seq in sequences])\n",
    "        sequences = pad_sequences(sequences, maxlen=self.max_length, padding='pre')\n",
    "        print('Max Sequence Length: %d' % self.max_length)\n",
    "        \n",
    "        # split into input and output elements\n",
    "        sequences = array(sequences)\n",
    "        self.x, self.y = sequences[:,:-1],sequences[:,-1]\n",
    "        self.y = to_categorical(self.y, num_classes=vocab_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source text\n",
    "data = \"\"\" Jack and Jill went up the hill\\n\n",
    "        To fetch a pail of water\\n\n",
    "        Jack fell down and broke his crown\\n\n",
    "        And Jill came tumbling after\\n \"\"\"\n",
    "\n",
    "params = {\"activation\":\"softmax\",\"epochs\":500,\"verbose\":2,\"loss\":\"categorical_crossentropy\",\n",
    "          \"optimizer\":\"adam\",\"metrics\":['accuracy']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_model = language_model(data,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
