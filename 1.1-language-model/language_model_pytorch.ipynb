{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def fit_on_texts(self,list_data):\n",
    "        word_list = \" \".join(list_data).split()\n",
    "        self.word_counts = list(set(word_list))\n",
    "        self.word_dict = {w: i for i, w in enumerate(self.word_counts)}\n",
    "        self.number_dict = {i: w for i, w in enumerate(self.word_counts)}\n",
    "        \n",
    "    def texts_to_sequences(self,data):\n",
    "        encoded_sequence = list()\n",
    "        for item in data:\n",
    "            encoded_sequence.append([self.word_dict[word] for word in item.split()])\n",
    "        return encoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    \n",
    "    def __init__(self,input_file):\n",
    "        self.input_data_file = input_file\n",
    "        self.data = None\n",
    "        self.vocab_size = None\n",
    "        self.encoded_data = None\n",
    "        self.max_length = None\n",
    "        self.sequences = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.tokenizer = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        fp = open(self.input_data_file,'r')\n",
    "        self.data = fp.read().splitlines()        \n",
    "        fp.close()\n",
    "        \n",
    "    def encode_data(self):\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.tokenizer.fit_on_texts(self.data)\n",
    "        self.encoded_data = self.tokenizer.texts_to_sequences(self.data)\n",
    "        print(self.encoded_data)\n",
    "        self.vocab_size = len(self.tokenizer.word_counts)+1\n",
    "        \n",
    "    def generate_sequence(self):\n",
    "        seq_list = list()\n",
    "        for item in self.encoded_data:\n",
    "            l = len(item)\n",
    "            for id in range(1,l):\n",
    "                seq_list.append(item[:id+1])\n",
    "        #print(seq_list[0])\n",
    "        print(seq_list)\n",
    "        seq_tensor = [torch.Tensor(item) for item in seq_list]\n",
    "        self.sequences = pad_sequence(seq_tensor,batch_first=False, padding_value=0)\n",
    "        print(self.sequences)\n",
    "            \n",
    "    def get_data(self):\n",
    "        self.x = self.sequences[:,:-1]\n",
    "        self.y = self.sequences[:,-1]\n",
    "        self.y = to_categorical(self.y,num_classes=self.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 12, 16, 4, 3, 14, 9], [17, 0, 18, 6, 19, 21], [2, 10, 8, 12, 11, 7, 15], [1, 16, 5, 20, 13]]\n",
      "[[2, 12], [2, 12, 16], [2, 12, 16, 4], [2, 12, 16, 4, 3], [2, 12, 16, 4, 3, 14], [2, 12, 16, 4, 3, 14, 9], [17, 0], [17, 0, 18], [17, 0, 18, 6], [17, 0, 18, 6, 19], [17, 0, 18, 6, 19, 21], [2, 10], [2, 10, 8], [2, 10, 8, 12], [2, 10, 8, 12, 11], [2, 10, 8, 12, 11, 7], [2, 10, 8, 12, 11, 7, 15], [1, 16], [1, 16, 5], [1, 16, 5, 20], [1, 16, 5, 20, 13]]\n",
      "tensor([[ 2.,  2.,  2.,  2.,  2.,  2., 17., 17., 17., 17., 17.,  2.,  2.,  2.,\n",
      "          2.,  2.,  2.,  1.,  1.,  1.,  1.],\n",
      "        [12., 12., 12., 12., 12., 12.,  0.,  0.,  0.,  0.,  0., 10., 10., 10.,\n",
      "         10., 10., 10., 16., 16., 16., 16.],\n",
      "        [ 0., 16., 16., 16., 16., 16.,  0., 18., 18., 18., 18.,  0.,  8.,  8.,\n",
      "          8.,  8.,  8.,  0.,  5.,  5.,  5.],\n",
      "        [ 0.,  0.,  4.,  4.,  4.,  4.,  0.,  0.,  6.,  6.,  6.,  0.,  0., 12.,\n",
      "         12., 12., 12.,  0.,  0., 20., 20.],\n",
      "        [ 0.,  0.,  0.,  3.,  3.,  3.,  0.,  0.,  0., 19., 19.,  0.,  0.,  0.,\n",
      "         11., 11., 11.,  0.,  0.,  0., 13.],\n",
      "        [ 0.,  0.,  0.,  0., 14., 14.,  0.,  0.,  0.,  0., 21.,  0.,  0.,  0.,\n",
      "          0.,  7.,  7.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  9.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0., 15.,  0.,  0.,  0.,  0.]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'to_categorical' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-c74a141728f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-86-fe3a7f0e551f>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'to_categorical' is not defined"
     ]
    }
   ],
   "source": [
    "pr = Preprocessing('data.txt')\n",
    "pr.load_data()\n",
    "pr.encode_data()\n",
    "pr.generate_sequence()\n",
    "pr.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'language_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d957de0b6274>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlang_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'language_model' is not defined"
     ]
    }
   ],
   "source": [
    "lang_model = language_model(data,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
