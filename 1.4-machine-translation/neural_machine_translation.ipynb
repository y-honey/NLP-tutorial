{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from nltk.translate.bleu_score import corpus_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData():\n",
    "    def __init__(self,file):\n",
    "        self.file = file\n",
    "        self.text = None\n",
    "        \n",
    "    def load_data(self):\n",
    "        fp = open(self.file, mode='rt', encoding='utf-8')\n",
    "        self.text = fp.read()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing():\n",
    "    def __init__(self):\n",
    "        self.pairs = None\n",
    "        self.cleaned_pairs = list()\n",
    "        \n",
    "    def get_pairs(self,data):\n",
    "        lines = data.strip().split('\\n')\n",
    "        self.pairs = [line.split('\\t') for line in  lines]\n",
    "        \n",
    "    def clean_pairs(self):\n",
    "        re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        for pair in self.pairs:\n",
    "            clean_pair = list()\n",
    "            for line in pair:\n",
    "                line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "                line = line.decode('UTF-8')\n",
    "                line = line.split()\n",
    "                line = [word.lower() for word in line]\n",
    "                line = [word.translate(table) for word in line]\n",
    "                line = [re_print.sub('', w) for w in line]\n",
    "                line = [word for word in line if word.isalpha()]\n",
    "                clean_pair.append(' '.join(line))\n",
    "            self.cleaned_pairs.append(clean_pair)\n",
    "        self.cleaned_pairs = array(self.cleaned_pairs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveData():\n",
    "    \n",
    "    def load_clean_sentences(self,filename):\n",
    "        return load(open(filename, 'rb'))\n",
    "\n",
    "    def save_clean_data(self,sentences, filename):\n",
    "        dump(sentences, open(filename, 'wb'))\n",
    "        print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset():\n",
    "    def __init__(self):\n",
    "        self.dataset = None\n",
    "        self.training_data = None\n",
    "        self.testing_data = None\n",
    "        \n",
    "    def create_tokenizer(self,lines):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(lines)\n",
    "        return tokenizer\n",
    "        \n",
    "    def max_length(self,lines):\n",
    "        return max(len(line.split()) for line in lines)\n",
    "\n",
    "\n",
    "    def encode_sequences(self,tokenizer, length, lines):\n",
    "        X = tokenizer.texts_to_sequences(lines)\n",
    "        X = pad_sequences(X, maxlen=length, padding='post')\n",
    "        return X\n",
    " \n",
    "    def encode_output(self,sequences, vocab_size):\n",
    "        ylist = list()\n",
    "        for sequence in sequences:\n",
    "            encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "            ylist.append(encoded)\n",
    "        y = array(ylist)\n",
    "        y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, trainX, trainY, testX, testY, epochs, batch_size):\n",
    "        self.model = None\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.trainX = trainX\n",
    "        self.trainY = trainY \n",
    "        self.testX = testX\n",
    "        self.testY = testY\n",
    "    \n",
    "    def create_model(self,src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "        self.model.add(LSTM(n_units))\n",
    "        self.model.add(RepeatVector(tar_timesteps))\n",
    "        self.model.add(LSTM(n_units, return_sequences=True))\n",
    "        self.model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "        \n",
    "    def compile_model(self):\n",
    "        self.model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\")\n",
    "    \n",
    "    def plot_model(self):\n",
    "        print(self.model.summary())\n",
    "        plot_model(self.model, to_file='model.png', show_shapes=True)\n",
    "    \n",
    "    def fit_model(self):\n",
    "        filename = 'model.h5'\n",
    "        checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        self.model.fit(self.trainX, self.trainY, epochs=self.epochs, batch_size=self.batch_size,\\\n",
    "                       validation_data=(self.testX, self.testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/models/eng-german.pkl\n",
      "[['hi' 'hallo' 'ccby france attribution tatoebaorg cm cburgmer']\n",
      " ['hi' 'gru gott' 'ccby france attribution tatoebaorg cm esperantostern']\n",
      " ['run' 'lauf' 'ccby france attribution tatoebaorg papabear fingerhut']\n",
      " ...\n",
      " ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker'\n",
      "  'wenn jemand fremdes dir sagt dass du dich wie ein muttersprachler anhorst bedeutet das wahrscheinlich er hat etwas an deinem sprechen bemerkt dass dich als nichtmuttersprachler verraten hat mit anderen worten du horst dich nicht wirklich wie ein muttersprachler an'\n",
      "  'ccby france attribution tatoebaorg ck tickler']\n",
      " ['it may be impossible to get a completely errorfree corpus due to the nature of this kind of collaborative effort however if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning we might be able to minimize errors'\n",
      "  'es ist wohl unmoglich einen vollkommen fehlerfreien korpus zu erreichen das liegt in der natur eines solchen gemeinschaftsprojekts doch wenn wir unsere mitglieder dazu bringen konnen nicht mit sprachen herumzuexperimentieren die sie gerade lernen sondern satze in ihrer eigenen muttersprache beizutragen dann gelingt es uns vielleicht die zahl der fehler klein zu halten'\n",
      "  'ccby france attribution tatoebaorg ck pfirsichbaeumchen']\n",
      " ['doubtless there exists in this world precisely the right woman for any given man to marry and vice versa but when you consider that a human being has the opportunity of being acquainted with only a few hundred people and out of the few hundred that there are but a dozen or less whom he knows intimately and out of the dozen one or two friends at most it will easily be seen when we remember the number of millions who inhabit this world that probably since the earth was created the right man has never yet met the right woman'\n",
      "  'ohne zweifel findet sich auf dieser welt zu jedem mann genau die richtige ehefrau und umgekehrt wenn man jedoch in betracht zieht dass ein mensch nur gelegenheit hat mit ein paar hundert anderen bekannt zu sein von denen ihm nur ein dutzend oder weniger nahesteht darunter hochstens ein oder zwei freunde dann erahnt man eingedenk der millionen einwohner dieser weltleicht dass seit erschaffung ebenderselben wohl noch nie der richtige mann der richtigen frau begegnet ist'\n",
      "  'ccby france attribution tatoebaorg rm pfirsichbaeumchen']]\n",
      "Saved: data/models/english-german-both.pkl\n",
      "Saved: data/models/english-german-train.pkl\n",
      "Saved: data/models/english-german-test.pkl\n",
      "English Vocabulary Size: 2200\n",
      "English Max Length: 5\n",
      "German Vocabulary Size: 3529\n",
      "German Max Length: 9\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      " - 23s - loss: 4.1677 - val_loss: 3.4305\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.43051, saving model to model.h5\n",
      "Epoch 2/30\n",
      " - 9s - loss: 3.2668 - val_loss: 3.2735\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.43051 to 3.27349, saving model to model.h5\n",
      "Epoch 3/30\n",
      " - 9s - loss: 3.1271 - val_loss: 3.1951\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.27349 to 3.19509, saving model to model.h5\n",
      "Epoch 4/30\n",
      " - 9s - loss: 3.0150 - val_loss: 3.0907\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.19509 to 3.09074, saving model to model.h5\n",
      "Epoch 5/30\n",
      " - 10s - loss: 2.8396 - val_loss: 2.9298\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.09074 to 2.92979, saving model to model.h5\n",
      "Epoch 6/30\n",
      " - 11s - loss: 2.6563 - val_loss: 2.7854\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.92979 to 2.78544, saving model to model.h5\n",
      "Epoch 7/30\n",
      " - 10s - loss: 2.4708 - val_loss: 2.6577\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.78544 to 2.65770, saving model to model.h5\n",
      "Epoch 8/30\n",
      " - 11s - loss: 2.2839 - val_loss: 2.5375\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.65770 to 2.53746, saving model to model.h5\n",
      "Epoch 9/30\n",
      " - 12s - loss: 2.1149 - val_loss: 2.4288\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.53746 to 2.42884, saving model to model.h5\n",
      "Epoch 10/30\n",
      " - 11s - loss: 1.9660 - val_loss: 2.3446\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.42884 to 2.34461, saving model to model.h5\n",
      "Epoch 11/30\n",
      " - 11s - loss: 1.8315 - val_loss: 2.2853\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.34461 to 2.28530, saving model to model.h5\n",
      "Epoch 12/30\n",
      " - 13s - loss: 1.7100 - val_loss: 2.2252\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.28530 to 2.22519, saving model to model.h5\n",
      "Epoch 13/30\n",
      " - 11s - loss: 1.5961 - val_loss: 2.1613\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.22519 to 2.16128, saving model to model.h5\n",
      "Epoch 14/30\n",
      " - 11s - loss: 1.4865 - val_loss: 2.1188\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.16128 to 2.11882, saving model to model.h5\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-4e1230b60c47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m#md.plot_model()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-349c0f95b2db>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         self.model.fit(self.trainX, self.trainY, epochs=self.epochs, batch_size=self.batch_size,\\\n\u001b[0;32m---> 30\u001b[0;31m                        validation_data=(self.testX, self.testY), callbacks=[checkpoint], verbose=2)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Loading Data\n",
    "    ld = LoadData(\"data/deu-eng/deu.txt\")\n",
    "    ld.load_data()\n",
    "    \n",
    "    # Preprocessing Data\n",
    "    pd = PreProcessing()\n",
    "    pd.get_pairs(ld.text)\n",
    "    pd.clean_pairs()\n",
    "    \n",
    "    #Save data\n",
    "    sv = SaveData()\n",
    "    sv.save_clean_data(pd.cleaned_pairs,\"data/models/eng-german.pkl\")\n",
    "    raw_dataset = sv.load_clean_sentences(\"data/models/eng-german.pkl\")\n",
    "    print(raw_dataset)\n",
    "    # reduce dataset size\n",
    "    n_sentences = 10000\n",
    "    dataset = raw_dataset[:n_sentences, :]\n",
    "    # random shuffle\n",
    "    shuffle(dataset)\n",
    "    # split into train/test\n",
    "    train, test = dataset[:9000], dataset[9000:]\n",
    "    \n",
    "    # save train and test data\n",
    "    sv.save_clean_data(dataset, 'data/models/english-german-both.pkl')\n",
    "    sv.save_clean_data(train, 'data/models/english-german-train.pkl')\n",
    "    sv.save_clean_data(test, 'data/models/english-german-test.pkl')\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = sv.load_clean_sentences('data/models/english-german-both.pkl')\n",
    "    train = sv.load_clean_sentences('data/models/english-german-train.pkl')\n",
    "    test = sv.load_clean_sentences('data/models/english-german-test.pkl')\n",
    "    \n",
    "    #Create dataset\n",
    "    cd = CreateDataset()\n",
    "    # prepare english tokenizer\n",
    "    eng_tokenizer = cd.create_tokenizer(dataset[:, 0])\n",
    "    eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "    eng_length = cd.max_length(dataset[:, 0])\n",
    "    print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "    print('English Max Length: %d' % (eng_length))\n",
    "    # prepare german tokenizer\n",
    "    ger_tokenizer = cd.create_tokenizer(dataset[:, 1])\n",
    "    ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "    ger_length = cd.max_length(dataset[:, 1])\n",
    "    print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "    print('German Max Length: %d' % (ger_length))\n",
    "    \n",
    "    # prepare training data\n",
    "    trainX = cd.encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "    trainY = cd.encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "    trainY = cd.encode_output(trainY, eng_vocab_size)\n",
    "    # prepare validation data\n",
    "    testX = cd.encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "    testY = cd.encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "    testY = cd.encode_output(testY, eng_vocab_size)\n",
    "    \n",
    "    #model\n",
    "    epochs = 30\n",
    "    batch_size = 64\n",
    "    md = Model(trainX, trainY,testX, testY,epochs,batch_size)\n",
    "    md.create_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "    md.compile_model()\n",
    "    #md.plot_model()\n",
    "    \n",
    "    md.fit_model()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction():\n",
    "    def word_for_id(self,integer, tokenizer):\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == integer:\n",
    "                return word\n",
    "        return None\n",
    "\n",
    "    # generate target given source sequence\n",
    "    def predict_sequence(self,model, tokenizer, source):\n",
    "        prediction = model.predict(source, verbose=0)[0]\n",
    "        integers = [argmax(vector) for vector in prediction]\n",
    "        target = list()\n",
    "        for i in integers:\n",
    "            word = self.word_for_id(i, tokenizer)\n",
    "            if word is None:\n",
    "                break\n",
    "            target.append(word)\n",
    "        return ' '.join(target)\n",
    "\n",
    "    # evaluate the skill of the model\n",
    "    def evaluate_model(self,model, tokenizer, sources, raw_dataset):\n",
    "        actual, predicted = list(), list()\n",
    "        for i, source in enumerate(sources):\n",
    "            # translate encoded source text\n",
    "            source = source.reshape((1, source.shape[0]))\n",
    "            translation = self.predict_sequence(model, eng_tokenizer, source)\n",
    "            #print(\"raw_dataset[i]: \",raw_dataset[i])\n",
    "            raw_target = raw_dataset[i][0]\n",
    "            raw_src = raw_dataset[i][1]\n",
    "            if i < 10:\n",
    "                print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "            actual.append([raw_target.split()])\n",
    "            predicted.append(translation.split())\n",
    "        # calculate BLEU score\n",
    "        print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "        print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "        print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "        print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src=[wir lieben das], target=[we love this], predicted=[we love this]\n",
      "src=[ich werde es ihnen sagen], target=[ill tell you], predicted=[ill tell you]\n",
      "src=[geh voran], target=[lead the way], predicted=[give to do]\n",
      "src=[tom ist nicht verloren], target=[tom isnt lost], predicted=[tom isnt lost]\n",
      "src=[sie konnen kommen], target=[you can come], predicted=[you may come]\n",
      "src=[wo wir sind], target=[where are we], predicted=[were are]\n",
      "src=[macht geht vor recht], target=[might is right], predicted=[get careful serious]\n",
      "src=[lass die waffe fallen], target=[drop the gun], predicted=[drop your gun]\n",
      "src=[warten sie mal], target=[wait up], predicted=[wait a]\n",
      "src=[ich mag austern], target=[i like oysters], predicted=[i like horses]\n",
      "BLEU-1: 0.547696\n",
      "BLEU-2: 0.417366\n",
      "BLEU-3: 0.327859\n",
      "BLEU-4: 0.137535\n"
     ]
    }
   ],
   "source": [
    "pd = Prediction()\n",
    "pd.evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
