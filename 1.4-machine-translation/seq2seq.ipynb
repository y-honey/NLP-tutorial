{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Su6-OWedGRx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b2f9c60d-6f37-433d-f173-48e474609bf6"
      },
      "source": [
        "import os\n",
        "from pickle import dump\n",
        "import pandas as pd\n",
        "from numpy import array\n",
        "from numpy.random import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.layers import Input ,LSTM, Embedding, Dense, TimeDistributed\n",
        "\n",
        "from tensorflow.keras.layers import RepeatVector, Bidirectional, Dropout\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#from generate_phoneme import get_phoneme_list\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "from numpy import argmax\n",
        "from keras.models import load_model\n",
        "import time\n",
        "from pickle import load"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCpzQ2k2d7JD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "78b3f9e0-91aa-4596-b24b-74bda8b9c853"
      },
      "source": [
        "# Path to the data txt file on disk.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7gplBvjiGKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LoadData():\n",
        "        \n",
        "    def load_data(self):\n",
        "        data_path = '/content/drive/My Drive/Projects/NLP-Notebooks/1.4-machine-translation/data/ben-eng/ben.txt'\n",
        "        file = open(data_path, encoding=\"utf8\")\n",
        "        lines = file.read().split('\\n')\n",
        "        input_target = list()\n",
        "        for line in lines:\n",
        "            try:\n",
        "                input_text, target_text,_ = line.split('\\t')\n",
        "                input_target.append([input_text, target_text])\n",
        "            except:\n",
        "                pass\n",
        "        return input_target\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoN1Furqkguk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PreProcessing():\n",
        "    def __init__(self,dataset=None):\n",
        "        self.pairs = None\n",
        "        self.dataset = dataset\n",
        "        self.training_data = None\n",
        "        self.testing_data = None\n",
        "        \n",
        "            \n",
        "    def get_tokenizer(self,lines):\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(lines)\n",
        "        return tokenizer\n",
        "        \n",
        "    def max_length(self,lines):\n",
        "        return max(len(line.split()) for line in lines)\n",
        "\n",
        "    def get_length(self):\n",
        "        correct_data = self.dataset[:, 0]\n",
        "        faulty_data = self.dataset[:, 1]\n",
        "        tar_data_length = max(len(line.split()) for line in correct_data)\n",
        "        src_data_length = max(len(line.split()) for line in faulty_data)\n",
        "        return tar_data_length,src_data_length\n",
        "    \n",
        "    def encode_sequences(self,tokenizer, length, lines):\n",
        "        X = tokenizer.texts_to_sequences(lines)\n",
        "        X = pad_sequences(X, maxlen=length, padding='post')\n",
        "        return X\n",
        " \n",
        "    def encode_output(self,sequences, vocab_size):\n",
        "        ylist = list()\n",
        "        for sequence in sequences:\n",
        "            encoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "            ylist.append(encoded)\n",
        "        y = np.array(ylist)\n",
        "        y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "        return y\n",
        "\n",
        "    def get_train_test_data(self,data):\n",
        "        train, test = train_test_split(data, test_size=0.2, random_state = 143)\n",
        "        return train, test \n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poERbb0vfiv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DesignModel():\n",
        "    \n",
        "    def __init__(self,X_train,Y_train,X_test,Y_test,epochs,batch_size):\n",
        "        self.model = None\n",
        "        self.X_train = X_train\n",
        "        self.Y_train = Y_train\n",
        "        self.X_test = X_test\n",
        "        self.Y_test = Y_test\n",
        "        self.encoder_model = None\n",
        "        self.decoder_model = None\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "    \n",
        "    def generate_batch(self, X, y, max_length_src, max_length_tar,num_decoder_tokens, batch_size ):\n",
        "        ''' Generate a batch of data '''\n",
        "        while True:\n",
        "            for j in range(0, len(X), batch_size):\n",
        "                encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
        "                decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
        "                decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
        "                for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                    for t, item in enumerate(input_text):\n",
        "                        try:\n",
        "                            encoder_input_data[i, t] = item\n",
        "                        except Exception as e:\n",
        "                            print(\"Wrong word:\",word)\n",
        "                            print(\"Exception:\",e)\n",
        "                    for t, item in enumerate(target_text):\n",
        "                        if t<len(target_text)-1:\n",
        "                            decoder_input_data[i, t] = item\n",
        "                        if t>0:\n",
        "                            decoder_target_data[i, t - 1, item] = 1.\n",
        "                yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
        "                \n",
        "    def enc_dec_model(self, input_vocab_size, target_vocab_size, src_data_length, tar_data_length, n_units):\n",
        "        \n",
        "        current_directory = os.getcwd()\n",
        "        #data_folder = os.path.join(current_directory,\"..\",\"data\")\n",
        "        model_folder = os.path.join(current_directory,\"..\",\"models\")\n",
        "        \n",
        "        #encoder\n",
        "        encoder_input = Input(shape = (None,))\n",
        "        encoder_emb =  Embedding(input_vocab_size, n_units, mask_zero = False)(encoder_input)\n",
        "        encoder_lstm = LSTM(n_units,return_state = True)\n",
        "        encoder_outputs,encode_h,encoder_c = encoder_lstm(encoder_emb)\n",
        "        encoder_states = [encode_h,encoder_c]\n",
        "        \n",
        "        #decoder\n",
        "        decoder_input = Input(shape = (None,))\n",
        "        decoder_emb_layer = Embedding(target_vocab_size+1, n_units, mask_zero = False)\n",
        "        decoder_emb = decoder_emb_layer(decoder_input)\n",
        "        decoder_lstm = LSTM(n_units,return_sequences=True,return_state = True)\n",
        "        decoder_out,decode_h,decoder_c = decoder_lstm(decoder_emb,initial_state = encoder_states)\n",
        "        decoder_dense = Dense(target_vocab_size,activation=\"softmax\")\n",
        "        decoder_out = decoder_dense(decoder_out)\n",
        "        self.model = Model([encoder_input,decoder_input],decoder_out)\n",
        "        #compile\n",
        "        self.model.compile(optimizer=\"rmsprop\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
        "\n",
        "        #fit\n",
        "        tar_data_length,src_data_length = tar_data_length,src_data_length\n",
        "        train_gen = self.generate_batch(self.X_train,self.Y_train,src_data_length,tar_data_length,target_vocab_size,self.batch_size)\n",
        "        test_gen = self.generate_batch(self.X_test,self.Y_test,src_data_length,tar_data_length,target_vocab_size,self.batch_size)\n",
        "        train_samples_steps = len(self.X_train) / self.batch_size\n",
        "        val_samples_steps = len(self.X_test) / self.batch_size\n",
        "        \n",
        "        \n",
        "        \n",
        "        filename = os.path.join('Enc_Dec_base_model.h5')\n",
        "        checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "        callbacks=[checkpoint]\n",
        "        \n",
        "        self.model.fit(train_gen,\n",
        "                    steps_per_epoch = train_samples_steps,\n",
        "                    epochs=self.epochs,\n",
        "                    validation_data = test_gen,\n",
        "                    validation_steps = val_samples_steps,callbacks = callbacks)\n",
        "        \n",
        "        #eocoder setup\n",
        "        \n",
        "        self.encoder_model = Model(encoder_input, encoder_states)\n",
        "        \n",
        "        # Decoder setup\n",
        "        decoder_state_input_h = Input(shape=(n_units,))\n",
        "        decoder_state_input_c = Input(shape=(n_units,))\n",
        "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "        \n",
        "        dec_emb2 = decoder_emb_layer(decoder_input)\n",
        "        \n",
        "        decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "        decoder_states2 = [state_h2, state_c2]\n",
        "        decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "        \n",
        "        # Final decoder model\n",
        "        self.decoder_model = Model([decoder_input] + decoder_states_inputs,[decoder_outputs2] + decoder_states2)\n",
        "    \n",
        "    def save_model(self,model,model_file):\n",
        "        with open(model_file+'.json', 'w', encoding='utf8') as f:\n",
        "            f.write(model.to_json())\n",
        "        model.save_weights(model_file+'.h5')\n",
        "        \n",
        "    def save(self):\n",
        "        encoder_model_name = 'Encoder_Model'\n",
        "        self.save_model(self.encoder_model,encoder_model_name)\n",
        "        \n",
        "        decoder_model_name = 'Decoder_Model'\n",
        "        self.save_model(self.decoder_model,decoder_model_name)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRj78Ihxg-5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Prediction():\n",
        "    \n",
        "    def __init__(self,model_structure,src_data_length,tar_data_length,data_vocab_size):\n",
        "        self.src_data_length = src_data_length\n",
        "        self.tar_data_length = tar_data_length\n",
        "        self.data_vocab_size = data_vocab_size\n",
        "        cat = self.model_structure[\"cat\"]\n",
        "        version = self.model_structure[\"version\"]\n",
        "        current_directory = os.getcwd()\n",
        "        #data_folder = os.path.join(current_directory,\"..\",\"data\")\n",
        "        model_folder = os.path.join(current_directory,\"..\",\"models\")\n",
        "        encoder_model_filename = os.path.join(model_folder,cat+'_Encoder_base_model-v'+version+'.h5')\n",
        "        self.encoder_model = load(open(encoder_model_filename, 'rb'))\n",
        "        decoder_model_filename = os.path.join(model_folder,cat+'_Decoder_base_model-v'+version+'.h5')\n",
        "        self.decoder_model = load(open(decoder_model_filename, 'rb'))\n",
        "    \n",
        "    def get_tokens(self):\n",
        "        token_words = get_phoneme_list()\n",
        "        token_index = dict([(word, i+1) for i, word in enumerate(token_words)])\n",
        "        \n",
        "        reverse_token_index = dict((i, word) for word, i in token_index.items())\n",
        "\n",
        "        return token_index,reverse_token_index\n",
        "    \n",
        "    def generate_batch(self, x, y, max_length_src, max_length_tar, num_decoder_tokens, batch_size ):\n",
        "        token_index,_ = self.get_tokens()\n",
        "        ''' Generate a batch of data '''\n",
        "        while True:\n",
        "            for j in range(0, len(x), batch_size):\n",
        "                encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
        "                decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
        "                decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
        "                for i, (input_text, target_text) in enumerate(zip(x[j:j+batch_size], y[j:j+batch_size])):\n",
        "                    for t, word in enumerate(input_text.split()):\n",
        "                        try:\n",
        "                            encoder_input_data[i, t] = token_index[word]\n",
        "                        except Exception as e:\n",
        "                            print(\"Wrong word:\",word)\n",
        "                            print(\"Exception:\",e)\n",
        "                    for t, word in enumerate(target_text.split()):\n",
        "                        if t<len(target_text.split())-1:\n",
        "                            decoder_input_data[i, t] = token_index[word]\n",
        "                        if t>0:\n",
        "                            decoder_target_data[i, t - 1, token_index[word]] = 1.\n",
        "                yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
        "                \n",
        "    \n",
        "    \n",
        "    def decode_sequence(self,input_seq):\n",
        "        target_token_index,reverse_target_char_index = self.get_tokens()\n",
        "        #print(\"target_token_index:\",target_token_index)\n",
        "        states_value = self.encoder_model.predict(input_seq)\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = target_token_index[\"start\"]\n",
        "        stop_condition = False\n",
        "        decoded_phoneme = []\n",
        "        while not stop_condition:\n",
        "            output_tokens, h, c = self.decoder_model.predict([target_seq] + states_value)\n",
        "    \n",
        "            # Sample a token\n",
        "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "            sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "            decoded_phoneme.append(sampled_char)\n",
        "    \n",
        "\n",
        "            if len(decoded_phoneme)> 26 or sampled_char==\"end\":\n",
        "                del decoded_phoneme[-1]\n",
        "                stop_condition = True\n",
        "    \n",
        "            target_seq = np.zeros((1,1))\n",
        "            target_seq[0, 0] = sampled_token_index\n",
        "    \n",
        "            states_value = [h, c]\n",
        "    \n",
        "        return decoded_phoneme\n",
        "    \n",
        "    def get_preiction(self,item,actual_result):\n",
        "        \n",
        "        phoneme_item = get_phoneme(item)\n",
        "        actual_phoneme = get_phoneme(actual_result)\n",
        "        \n",
        "        for id in range(len(phoneme_item)):\n",
        "            if phoneme_item[id] == \" \":\n",
        "                phoneme_item[id] = \"SP\"\n",
        "                \n",
        "        for id in range(len(actual_phoneme)):\n",
        "            if actual_phoneme[id] == \" \":\n",
        "                actual_phoneme[id] = \"SP\"\n",
        "                \n",
        "        phoneme_str = \" \".join(phoneme_item)\n",
        "        actual_str = \" \".join(actual_phoneme)\n",
        "        \n",
        "        dataset = [[phoneme_str,actual_str]]\n",
        "        dataset = array(dataset)\n",
        "        x = dataset[:1,1]\n",
        "        y = dataset[:1,0]\n",
        "        test_gen = self.generate_batch(x, y, self.src_data_length, self.tar_data_length, self.data_vocab_size, 1)\n",
        "        (input_seq, actual_output), _ = next(test_gen)\n",
        "        enoded_phoneme_data = self.decode_sequence(input_seq)\n",
        "        \n",
        "        return enoded_phoneme_data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJh2Gc5OdKF-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "9582657e-a048-49a5-b6d7-721bdc675e1c"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    \n",
        "    # model_structure = {\"cat\" :\"area\",\"version\" : \"1\",\"model_type\" : \"enc_dec\"}\n",
        "    # epochs = 100\n",
        "    # batch_size = 35#ld.train_data_length\n",
        "    \n",
        "    \n",
        "    ld = LoadData()\n",
        "    input_target = ld.load_data()\n",
        "    print(input_target[:5])\n",
        "    input_texts = np.array(input_target)[:, 0]\n",
        "    target_texts = np.array(input_target)[:, 1]\n",
        "    prp = PreProcessing()\n",
        "    input_tokenizer = prp.get_tokenizer(input_texts)\n",
        "    innput_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "    input_length = prp.max_length(input_texts)\n",
        "    print(f'Input Vocabulary Size:{innput_vocab_size}')\n",
        "    print(f'Input Max Length Size:{input_length}')\n",
        "                \n",
        "    target_tokenizer = prp.get_tokenizer(target_texts)\n",
        "    target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "    target_length = prp.max_length(target_texts)\n",
        "    print(f'Target Vocabulary Size: {target_vocab_size}')\n",
        "    print(f'Target Max Length Size:{target_length}')\n",
        "\n",
        "    train , test = prp.get_train_test_data(input_target)\n",
        "    train = np.array(train) \n",
        "    test = np.array(test)\n",
        "    trainX = prp.encode_sequences(input_tokenizer, input_length, train[:, 1])\n",
        "    trainY = prp.encode_sequences(target_tokenizer, target_length, train[:, 0])\n",
        "    \n",
        "    testX = prp.encode_sequences(input_tokenizer, input_length, train[:, 1])\n",
        "    testY = prp.encode_sequences(target_tokenizer, target_length, train[:, 0])\n",
        "    \n",
        "    # #model\n",
        "    model_obj = DesignModel(trainX,trainY,testX,testY,5,32)\n",
        "    model_obj.enc_dec_model(innput_vocab_size,target_vocab_size,input_length,target_length,512)\n",
        "    model_obj.save()\n",
        "    \n",
        "    \n",
        "    # #prediction\n",
        "    # text_dict = load(open(os.path.join(\"..\",\"models\",cat+\"_phoneme_to_text_dict.pkl\"),'rb'))\n",
        "    # pred = Prediction(model_structure,src_data_length,tar_data_length,data_vocab_size)\n",
        "    # test_file = os.path.join(data_folder,cat+\"_test_cases.txt\")\n",
        "    # fread = open(test_file,'r')\n",
        "    # test_cases_list = fread.read().splitlines()\n",
        "    # test_list = list()\n",
        "    # for item in test_cases_list:\n",
        "    #     test_list.append(item.split(','))\n",
        "    \n",
        "    # for each_list in test_list:\n",
        "    #     actual_result = each_list[0]\n",
        "    #     for item in each_list[1:]:\n",
        "    #         start_time = time.time()\n",
        "    #         print(\"Input:\",item)\n",
        "    #         print(\"target:\",actual_result)\n",
        "    #         result = pred.get_preiction(item,actual_result)\n",
        "    #         print(\"Predicted Phoneme:\",result)\n",
        "    #         result = \" \".join(result)\n",
        "    #         result_list = result.split(\" SP \")\n",
        "            \n",
        "    #         pred_text = \"\"\n",
        "    #         for item in result_list:\n",
        "    #             try:\n",
        "    #                 if pred_text:\n",
        "    #                     pred_text = pred_text +\" \"+ text_dict[item]\n",
        "    #                 else:\n",
        "    #                     pred_text = text_dict[item]\n",
        "    #             except:\n",
        "    #                 pass            \n",
        "    #         print(\"Time:\",(time.time()-start_time)*1000,\" ms\" )\n",
        "    #         print(\"Predicted Text: \",pred_text)\n",
        "    #         print(\"Actual Text: \",actual_result)\n",
        "    #         if pred_text.lower() == actual_result.lower():\n",
        "    #             print(\"Pass\")\n",
        "    #         else:\n",
        "    #             print(\"Fail\")\n",
        "    #         print(\"\\n\")\n",
        "    \n",
        "    "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Go.', 'যাও।'], ['Go.', 'যান।'], ['Go.', 'যা।'], ['Run!', 'পালাও!'], ['Run!', 'পালান!']]\n",
            "Input Vocabulary Size:1875\n",
            "Input Max Length Size:19\n",
            "Target Vocabulary Size: 3312\n",
            "Target Max Length Size:18\n",
            "Epoch 1/5\n",
            "109/108 [==============================] - ETA: 0s - loss: 0.1577 - accuracy: 0.9907\n",
            "Epoch 00001: val_loss improved from inf to 0.00098, saving model to Enc_Dec_base_model.h5\n",
            "109/108 [==============================] - 5s 48ms/step - loss: 0.1577 - accuracy: 0.9907 - val_loss: 9.8267e-04 - val_accuracy: 0.9999\n",
            "Epoch 2/5\n",
            "109/108 [==============================] - ETA: 0s - loss: 9.8685e-04 - accuracy: 0.9999\n",
            "Epoch 00002: val_loss improved from 0.00098 to 0.00072, saving model to Enc_Dec_base_model.h5\n",
            "109/108 [==============================] - 5s 43ms/step - loss: 9.8685e-04 - accuracy: 0.9999 - val_loss: 7.2202e-04 - val_accuracy: 0.9999\n",
            "Epoch 3/5\n",
            "109/108 [==============================] - ETA: 0s - loss: 8.0972e-04 - accuracy: 0.9999\n",
            "Epoch 00003: val_loss improved from 0.00072 to 0.00061, saving model to Enc_Dec_base_model.h5\n",
            "109/108 [==============================] - 5s 43ms/step - loss: 8.0972e-04 - accuracy: 0.9999 - val_loss: 6.1090e-04 - val_accuracy: 0.9999\n",
            "Epoch 4/5\n",
            "109/108 [==============================] - ETA: 0s - loss: 7.1429e-04 - accuracy: 0.9999\n",
            "Epoch 00004: val_loss improved from 0.00061 to 0.00058, saving model to Enc_Dec_base_model.h5\n",
            "109/108 [==============================] - 5s 43ms/step - loss: 7.1429e-04 - accuracy: 0.9999 - val_loss: 5.8417e-04 - val_accuracy: 0.9999\n",
            "Epoch 5/5\n",
            "109/108 [==============================] - ETA: 0s - loss: 6.7990e-04 - accuracy: 0.9999\n",
            "Epoch 00005: val_loss improved from 0.00058 to 0.00057, saving model to Enc_Dec_base_model.h5\n",
            "109/108 [==============================] - 5s 43ms/step - loss: 6.7990e-04 - accuracy: 0.9999 - val_loss: 5.7202e-04 - val_accuracy: 0.9999\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}