{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Su6-OWedGRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from pickle import dump\n",
        "import pandas as pd\n",
        "from numpy import array\n",
        "from numpy.random import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.layers import Input ,LSTM, Embedding, Dense, TimeDistributed\n",
        "\n",
        "from tensorflow.keras.layers import RepeatVector, Bidirectional, Dropout\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#from generate_phoneme import get_phoneme_list\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "from numpy import argmax\n",
        "from tensorflow.keras.models import load_model,model_from_json\n",
        "import time\n",
        "from pickle import load"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCpzQ2k2d7JD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "d5063da8-2fd9-4a8d-ca7d-f8984d5a4e38"
      },
      "source": [
        "# Path to the data txt file on disk.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7gplBvjiGKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LoadData():\n",
        "        \n",
        "    def load_data(self):\n",
        "        data_path = '/content/drive/My Drive/Projects/NLP-Notebooks/1.4-machine-translation/data/ben-eng/ben.txt'\n",
        "        file = open(data_path, encoding=\"utf8\")\n",
        "        lines = file.read().split('\\n')\n",
        "        input_target = list()\n",
        "        for line in lines:\n",
        "            try:\n",
        "                input_text, target_text,_ = line.split('\\t')\n",
        "                target_text = \"START \"+target_text+\" END\"\n",
        "                input_target.append([input_text, target_text])\n",
        "            except:\n",
        "                pass\n",
        "        df = pd.DataFrame(input_target, columns = [\"Input\",\"Target\"])\n",
        "        return input_target,df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoN1Furqkguk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PreProcessing():\n",
        "    def __init__(self,dataset=None):\n",
        "        self.pairs = None\n",
        "        self.dataset = dataset\n",
        "        self.training_data = None\n",
        "        self.testing_data = None\n",
        "        \n",
        "    def tokenization(self,df):\n",
        "        # Input Vocabulary Size:1875\n",
        "        # Input Max Length Size:19\n",
        "        # Target Vocabulary Size: 3312\n",
        "        # Target Max Length Size:18\n",
        "        df['Target'] = df['Target'].apply(lambda x : 'START '+ x + ' END')\n",
        "        self.input_max_length=max(df['Input'].apply(lambda x:len(x.split())))\n",
        "        self.target_max_length=max(df['Target'].apply(lambda x:len(x.split())))\n",
        "\n",
        "        all_input_words = list(set([word for sent in df[\"Input\"] for word in sent.split()]))\n",
        "        all_target_words = list(set([word for sent in df[\"Target\"] for word in sent.split()]))\n",
        "\n",
        "        self.input_vocab_size = len(all_input_words)\n",
        "        self.target_vocab_size = len(all_target_words)\n",
        "\n",
        "        self.input_token_index = dict([(word, i+1) for i, word in enumerate(all_input_words)])\n",
        "        self.target_token_index = dict([(word, i+1) for i, word in enumerate(all_target_words)])\n",
        "        self.reverse_input_index = dict((i, word) for word, i in self.input_token_index.items())\n",
        "        self.reverse_target_index = dict((i, word) for word, i in self.target_token_index.items())\n",
        "        x_train, x_test, y_train, y_test = train_test_split(df[\"Input\"], df[\"Target\"], test_size = 0.2,random_state=123)\n",
        "        return x_train, x_test, y_train, y_test\n",
        "        \n",
        "\n",
        "    def get_tokenizer(self,lines):\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(lines)\n",
        "        return tokenizer\n",
        "        \n",
        "    def max_length(self,lines):\n",
        "        return max(len(line.split()) for line in lines)\n",
        "\n",
        "    def get_length(self):\n",
        "        correct_data = self.dataset[:, 0]\n",
        "        faulty_data = self.dataset[:, 1]\n",
        "        tar_data_length = max(len(line.split()) for line in correct_data)\n",
        "        src_data_length = max(len(line.split()) for line in faulty_data)\n",
        "        return tar_data_length,src_data_length\n",
        "    \n",
        "    def encode_sequences(self,tokenizer, length, lines):\n",
        "        X = tokenizer.texts_to_sequences(lines)\n",
        "        X = pad_sequences(X, maxlen=length, padding='post')\n",
        "        return X\n",
        " \n",
        "    def encode_output(self,sequences, vocab_size):\n",
        "        ylist = list()\n",
        "        for sequence in sequences:\n",
        "            encoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "            ylist.append(encoded)\n",
        "        y = np.array(ylist)\n",
        "        y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "        return y\n",
        "\n",
        "    def get_train_test_data(self,data):\n",
        "        train, test = train_test_split(data, test_size=0.2, random_state = 143)\n",
        "        return train, test \n",
        "\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poERbb0vfiv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DesignModel():\n",
        "    \n",
        "    def __init__(self,X_train,Y_train,X_test,Y_test,epochs,batch_size):\n",
        "        self.X_train = X_train\n",
        "        self.Y_train = Y_train\n",
        "        self.X_test = X_test\n",
        "        self.Y_test = Y_test\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "    \n",
        "    def generate_batch(self, X, y, max_length_src, max_length_tar,encoder_tokens, decoder_tokens,num_decoder_tokens, batch_size ):\n",
        "        ''' Generate a batch of data '''\n",
        "        while True:\n",
        "            for j in range(0, len(X), batch_size):\n",
        "                encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
        "                decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
        "                decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
        "                for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                    for t, item in enumerate(input_text.split()):\n",
        "                        try:\n",
        "                            encoder_input_data[i, t] = encoder_tokens[item]\n",
        "                        except Exception as e:\n",
        "                            print(\"Wrong word:\",item)\n",
        "                            print(\"Exception:\",e)\n",
        "                    for t, item in enumerate(target_text.split()):\n",
        "                        if t<len(target_text)-1:\n",
        "                            decoder_input_data[i, t] = decoder_tokens[item]\n",
        "                        if t>0:\n",
        "                            decoder_target_data[i, t - 1, decoder_tokens[item]] = 1.\n",
        "                yield ([encoder_input_data, decoder_input_data], decoder_target_data)\n",
        "                \n",
        "    def enc_dec_model(self, input_vocab_size, target_vocab_size, encoder_tokens, decoder_tokens, src_data_length, tar_data_length, n_units):\n",
        "        \n",
        "        #encoder\n",
        "        encoder_input = Input(shape = (None,))\n",
        "        encoder_emb =  Embedding(input_vocab_size, n_units, mask_zero = False)(encoder_input)\n",
        "        encoder_lstm = LSTM(n_units,return_state = True)\n",
        "        encoder_outputs,encode_h,encoder_c = encoder_lstm(encoder_emb)\n",
        "        encoder_states = [encode_h,encoder_c]\n",
        "        \n",
        "        #decoder\n",
        "        decoder_input = Input(shape = (None,))\n",
        "        decoder_emb_layer = Embedding(target_vocab_size, n_units, mask_zero = False)\n",
        "        decoder_emb = decoder_emb_layer(decoder_input)\n",
        "        decoder_lstm = LSTM(n_units,return_sequences=True,return_state = True)\n",
        "        decoder_out,decode_h,decoder_c = decoder_lstm(decoder_emb,initial_state = encoder_states)\n",
        "        decoder_dense = Dense(target_vocab_size,activation=\"softmax\")\n",
        "        decoder_out = decoder_dense(decoder_out)\n",
        "        self.model = Model([encoder_input,decoder_input],decoder_out)\n",
        "        #compile\n",
        "        self.model.compile(optimizer=\"rmsprop\",loss=\"categorical_crossentropy\",metrics=['acc'])\n",
        "        print(self.model.summary())\n",
        "        #fit\n",
        "        train_gen = self.generate_batch(self.X_train,self.Y_train,src_data_length,tar_data_length,encoder_tokens, decoder_tokens, target_vocab_size,self.batch_size)\n",
        "        test_gen = self.generate_batch(self.X_test,self.Y_test,src_data_length,tar_data_length,encoder_tokens, decoder_tokens, target_vocab_size,self.batch_size)\n",
        "        train_samples_steps = len(self.X_train) / self.batch_size\n",
        "        val_samples_steps = len(self.X_test) / self.batch_size\n",
        "                \n",
        "\n",
        "        self.model.fit(train_gen, steps_per_epoch = train_samples_steps, epochs=self.epochs, validation_data = test_gen, validation_steps = val_samples_steps)\n",
        "        \n",
        "        #eocoder setup\n",
        "        \n",
        "        self.encoder_model = Model(encoder_input, encoder_states)\n",
        "        \n",
        "        # Decoder setup\n",
        "        decoder_state_input_h = Input(shape=(n_units,))\n",
        "        decoder_state_input_c = Input(shape=(n_units,))\n",
        "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "        \n",
        "        dec_emb2 = decoder_emb_layer(decoder_input)\n",
        "        \n",
        "        decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "        decoder_states2 = [state_h2, state_c2]\n",
        "        decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "        \n",
        "        # Final decoder model\n",
        "        self.decoder_model = Model([decoder_input] + decoder_states_inputs,[decoder_outputs2] + decoder_states2)\n",
        "    \n",
        "    def save_model(self,model,model_file):\n",
        "        with open(model_file+'.json', 'w', encoding='utf8') as f:\n",
        "            f.write(model.to_json())\n",
        "        model.save_weights(model_file+'.h5')\n",
        "        \n",
        "    def save(self):\n",
        "        encoder_model_name = 'Encoder_Model'\n",
        "        self.save_model(self.encoder_model,encoder_model_name)\n",
        "        \n",
        "        decoder_model_name = 'Decoder_Model'\n",
        "        self.save_model(self.decoder_model,decoder_model_name)\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRj78Ihxg-5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Prediction():\n",
        "    \n",
        "    def __init__(self,input_vocab_size,target_vocab_size,input_length,target_length):\n",
        "        self.src_data_length = input_length\n",
        "        self.tar_data_length = target_length\n",
        "        self.data_vocab_size = target_vocab_size\n",
        "    \n",
        "    def load_weights(self,model_filename, model_weights_filename):\n",
        "        with open(model_filename, 'r', encoding='utf8') as f:\n",
        "            model = model_from_json(f.read())\n",
        "        model.load_weights(model_weights_filename)\n",
        "        return model\n",
        "    \n",
        "    def load_model(self):\n",
        "        encoder_model_name = 'Encoder_Model'\n",
        "        decoder_model_name = 'Decoder_Model'\n",
        "        self.encoder_model = self.load_weights(encoder_model_name+'.json', encoder_model_name+'.h5')\n",
        "        self.decoder_model = self.load_weights(decoder_model_name+'.json', decoder_model_name+'.h5')\n",
        "    \n",
        "    def decode_sequence(self,input_seq,target_token_index,reverse_target_index):\n",
        "        # Encode the input as state vectors.\n",
        "        states_value = encoder_model.predict(input_seq)\n",
        "        # Generate empty target sequence of length 1.\n",
        "        target_seq = np.zeros((1,1))\n",
        "        # Populate the first character of target sequence with the start character.\n",
        "        target_seq[0, 0] = target_token_index['START']\n",
        "\n",
        "        # Sampling loop for a batch of sequences\n",
        "        # (to simplify, here we assume a batch of size 1).\n",
        "        stop_condition = False\n",
        "        decoded_sentence = ''\n",
        "        while not stop_condition:\n",
        "            output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "            # Sample a token\n",
        "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "            sampled_char = reverse_target_index[sampled_token_index]\n",
        "            decoded_sentence += ' '+sampled_char\n",
        "\n",
        "            # Exit condition: either hit max length\n",
        "            # or find stop character.\n",
        "            if (sampled_char == 'END' or len(decoded_sentence) > 50):\n",
        "                stop_condition = True\n",
        "\n",
        "            # Update the target sequence (of length 1).\n",
        "            target_seq = np.zeros((1,1))\n",
        "            target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "            # Update states\n",
        "            states_value = [h, c]\n",
        "\n",
        "        return decoded_sentence"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWFBRwC3QN46",
        "colab_type": "text"
      },
      "source": [
        "Loading & preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM_UthjoQJ-K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "df7f10a7-ef03-4738-8880-465940eaf09a"
      },
      "source": [
        "batch_size = 32\n",
        "ld = LoadData()\n",
        "input_target,df = ld.load_data()\n",
        "\n",
        "print(df.head()) \n",
        "prp = PreProcessing()\n",
        "x_train, x_test, y_train, y_test = prp.tokenization(df)\n",
        "print(x_train.shape, x_test.shape)\n",
        "input_vocab_size = prp.input_vocab_size + 1\n",
        "target_vocab_size = prp.target_vocab_size + 1\n",
        "input_token_index = prp.input_token_index\n",
        "target_token_index = prp.target_token_index\n",
        "input_max_length = prp.input_max_length\n",
        "target_max_length = prp.target_max_length\n",
        "reverse_input_index = prp.reverse_input_index\n",
        "reverse_target_index = prp.reverse_target_index\n",
        "\n",
        "# input_texts = np.array(input_target)[:, 0]\n",
        "# target_texts = np.array(input_target)[:, 1]\n",
        "# prp = PreProcessing()\n",
        "# input_tokenizer = prp.get_tokenizer(input_texts)\n",
        "# input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "# input_max_length = prp.max_length(input_texts)\n",
        "# input_token_index = None\n",
        "# reverse_input_index = None\n",
        "# print(f'Input Vocabulary Size:{input_vocab_size}')\n",
        "# print(f'Input Max Length Size:{input_length}')\n",
        "            \n",
        "# target_tokenizer = prp.get_tokenizer(target_texts)\n",
        "# target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "# target_max_length = prp.max_length(target_texts)\n",
        "# target_token_index\n",
        "# reverse_target_index\n",
        "\n",
        "# print(f'Target Vocabulary Size: {target_vocab_size}')\n",
        "# print(f'Target Max Length Size:{target_length}')\n",
        "\n",
        "# train , test = prp.get_train_test_data(input_target)\n",
        "# train = np.array(train) \n",
        "# test = np.array(test)\n",
        "# trainX = prp.encode_sequences(input_tokenizer, input_length, train[:, 1])\n",
        "# trainY = prp.encode_sequences(target_tokenizer, target_length, train[:, 0])\n",
        "\n",
        "# testX = prp.encode_sequences(input_tokenizer, input_length, train[:, 1])\n",
        "# testY = prp.encode_sequences(target_tokenizer, target_length, train[:, 0])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Input  Target\n",
            "0   Go.    যাও।\n",
            "1   Go.    যান।\n",
            "2   Go.     যা।\n",
            "3  Run!  পালাও!\n",
            "4  Run!  পালান!\n",
            "(3480,) (870,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX2W046FQwhk",
        "colab_type": "text"
      },
      "source": [
        "Training & Model Saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO4uEfhJQzhV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "e7a88686-7a76-4b4f-894f-5f69e526fc2e"
      },
      "source": [
        "model_obj = DesignModel(x_train,y_train,x_test,y_test,100,32)\n",
        "model_obj.enc_dec_model(input_vocab_size, target_vocab_size, input_token_index, target_token_index, input_max_length,target_max_length,300)\n",
        "model_obj.save()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_15\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_25 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_26 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_18 (Embedding)        (None, None, 300)    858300      input_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_19 (Embedding)        (None, None, 300)    1062900     input_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_18 (LSTM)                  [(None, 300), (None, 721200      embedding_18[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_19 (LSTM)                  [(None, None, 300),  721200      embedding_19[0][0]               \n",
            "                                                                 lstm_18[0][1]                    \n",
            "                                                                 lstm_18[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, None, 3543)   1066443     lstm_19[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 4,430,043\n",
            "Trainable params: 4,430,043\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "109/108 [==============================] - 7s 61ms/step - loss: 1.5769 - acc: 0.0598 - val_loss: 1.4334 - val_acc: 0.0616\n",
            "Epoch 2/100\n",
            "109/108 [==============================] - 5s 46ms/step - loss: 1.4039 - acc: 0.0639 - val_loss: 1.3991 - val_acc: 0.0627\n",
            "Epoch 3/100\n",
            " 90/108 [=======================>......] - ETA: 0s - loss: 1.3082 - acc: 0.0685"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-KhfOpUQ5PW",
        "colab_type": "text"
      },
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fx71zjsQ6kN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "90acfde5-e537-4ac6-8056-167b89163550"
      },
      "source": [
        "reverse_word_map = dict(map(reversed, target_tokenizer.word_index.items()))\n",
        "pred = Prediction(input_vocab_size,target_vocab_size,input_length,target_length)\n",
        "pred.load_model()\n",
        "for seq_index in range(10):\n",
        "    # Take one sequence (part of the training test)\n",
        "    # for trying out decoding.\n",
        "    input_seq = prp.encode_sequences(input_tokenizer, input_length, input_texts[seq_index])\n",
        "    decoded_sentence = pred.decode_sequence(input_seq,reverse_word_map,target_vocab_size)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-eb6be705a582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# for trying out decoding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreverse_word_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input sentence:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-89c64cffce9d>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(self, input_seq, reverse_map, num_decoder_tokens)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtarget_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_decoder_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Populate the first character of target sequence with the start character.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtarget_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_token_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Sampling loop for a batch of sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'target_token_index' is not defined"
          ]
        }
      ]
    }
  ]
}