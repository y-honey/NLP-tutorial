{"cells":[{"metadata":{},"cell_type":"markdown","source":"Data\nAmazon fine food reviews from Kaggle"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","execution_count":39,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding Attention Layer as its not a part of the keras\nhttps://www.kaggle.com/kweku20/attention"},{"metadata":{"trusted":true},"cell_type":"code","source":"from shutil import copyfile\ncopyfile(src = \"/kaggle/input/attention/attention.py\", dst = \"/kaggle/working/attention.py\")\nfrom attention import AttentionLayer","execution_count":40,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class LoadData():\n    def __init__(self):\n        data = pd.read_csv(\"/kaggle/input/amazon-fine-food-reviews/Reviews.csv\")\n        print(data.head())\n        self.data = data.drop([\"Id\",\"ProductId\",\"UserId\",\"ProfileName\",\"HelpfulnessNumerator\",\"HelpfulnessDenominator\",\"Score\",\"Time\"],axis=1)\n        ","execution_count":41,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calling load data object"},{"metadata":{"trusted":true},"cell_type":"code","source":"load_data = LoadData()\ndata = load_data.data","execution_count":42,"outputs":[{"output_type":"stream","text":"   Id   ProductId          UserId                      ProfileName  \\\n0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n\n   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n0                     1                       1      5  1303862400   \n1                     0                       0      1  1346976000   \n2                     1                       1      4  1219017600   \n3                     3                       3      2  1307923200   \n4                     0                       0      5  1350777600   \n\n                 Summary                                               Text  \n0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n2  \"Delight\" says it all  This is a confection that has been around a fe...  \n3         Cough Medicine  If you are looking for the secret ingredient i...  \n4            Great taffy  Great taffy at a great price.  There was a wid...  \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PreprocessingData():\n    def __init__(self):\n        self.max_in_len = 100\n        self.max_tar_len = 10\n        \n    def preprocess_data(self,data):\n        data.dropna(axis=0,inplace=True)\n        data['Summary'] = data['Summary'].apply(lambda x : 'start '+ x + ' end')\n        return data\n    \n    def get_data(self,data):\n        x_train,x_val,y_train,y_val = train_test_split(np.array(data['Text']),np.array(data['Summary']),test_size=0.1,random_state=0,shuffle=True)\n        return x_train,x_val,y_train,y_val\n    \n    def encode_data(self,data,x_train,x_val,y_train,y_val):\n        \n        # Input Encoding\n        in_tokenizer = Tokenizer() \n        in_tokenizer.fit_on_texts(data[\"Text\"].tolist())\n\n        x_train_seq = in_tokenizer.texts_to_sequences(x_train) \n        x_val_seq = in_tokenizer.texts_to_sequences(x_val)\n\n        x_train = pad_sequences(x_train_seq, maxlen = self.max_in_len, padding='post')\n        x_val = pad_sequences(x_val_seq, maxlen = self.max_in_len, padding='post')\n\n        self.in_voc = len(in_tokenizer.word_counts) + 1\n        \n        # Target Encoding\n        tar_tokenizer = Tokenizer() \n        tar_tokenizer.fit_on_texts(data[\"Summary\"].tolist())\n\n        y_train_seq = tar_tokenizer.texts_to_sequences(y_train) \n        y_val_seq = tar_tokenizer.texts_to_sequences(y_val)\n\n        y_train = pad_sequences(y_train_seq,  maxlen = self.max_tar_len, padding='post')\n        y_val = pad_sequences(y_val_seq, maxlen = self.max_tar_len, padding='post')\n\n        self.tar_voc = len(tar_tokenizer.word_counts) + 1\n        return x_train,x_val,y_train,y_val","execution_count":43,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calling preprocessing module on loading data"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessing_data = PreprocessingData()\ndata = preprocessing_data.preprocess_data(data)\nx_train,x_val,y_train,y_val = preprocessing_data.get_data(data)\nx_train,x_val,y_train,y_val = preprocessing_data.encode_data(data,x_train,x_val,y_train,y_val)","execution_count":44,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"unsupported operand type(s) for +: 'collections.OrderedDict' and 'int'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-29d3347b8de3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-43-dbc313ffb414>\u001b[0m in \u001b[0;36mencode_data\u001b[0;34m(self, data, x_train, x_val, y_train, y_val)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_in_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_voc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Target Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'collections.OrderedDict' and 'int'"]}]},{"metadata":{},"cell_type":"markdown","source":"Model Creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model():\n    def __init__(self):\n        self.model = None\n        \n    def define_model(self):\n        raise NotImplementedError","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}