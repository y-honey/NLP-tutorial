{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from tensorflow.keras.layers import add, dot, concatenate\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessData():\n",
    "    def tokenize(self, sent):\n",
    "        return [ x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "    \n",
    "    def parse_data(self, lines, only_supporting=False):\n",
    "        \n",
    "        '''Parse stories provided in the bAbi tasks format\n",
    "        If only_supporting is true, only the sentences\n",
    "        that support the answer are kept.\n",
    "        '''\n",
    "        data = []\n",
    "        story = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            nid, line = line.split(' ', 1)\n",
    "            nid = int(nid)\n",
    "            if nid == 1:\n",
    "                story = []\n",
    "            if '\\t' in line:\n",
    "                q, a, supporting = line.split('\\t')\n",
    "                q = self.tokenize(q)\n",
    "                substory = None\n",
    "                if only_supporting:\n",
    "                    # Only select the related substory\n",
    "                    supporting = map(int, supporting.split())\n",
    "                    substory = [story[i - 1] for i in supporting]\n",
    "                else:\n",
    "                    # Provide all the substories\n",
    "                    substory = [x for x in story if x]\n",
    "                data.append((substory, q, a))\n",
    "                story.append('')\n",
    "            else:\n",
    "                sent = self.tokenize(line)\n",
    "                story.append(sent)\n",
    "        return data\n",
    "    \n",
    "    def make_data(self, lines, only_supporting=False, max_length=None):\n",
    "        data = self.parse_data(lines, only_supporting=only_supporting)\n",
    "        flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "        data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def vectorize_stories(self, data, word_idx, story_maxlen, query_maxlen):\n",
    "        X = []\n",
    "        Xq = []\n",
    "        Y = []\n",
    "        for story, query, answer in data:\n",
    "            x = [word_idx[w] for w in story]\n",
    "            xq = [word_idx[w] for w in query]\n",
    "            # let's not forget that index 0 is reserved\n",
    "            y = np.zeros(len(word_idx) + 1)\n",
    "            y[word_idx[answer]] = 1\n",
    "            X.append(x)\n",
    "            Xq.append(xq)\n",
    "            Y.append(y)\n",
    "        return (pad_sequences(X, maxlen=story_maxlen),\n",
    "                pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))\n",
    "    \n",
    "    def get_data(self,path,type = \"qa1_single-supporting-fact\"):\n",
    "        train_path = os.path.join(path,type+'_train.txt')\n",
    "        test_path = os.path.join(path,type+'_test.txt')\n",
    "        with open(train_path,'r')as fp:\n",
    "            data = fp.read().splitlines()\n",
    "            train_data = self.make_data(data)\n",
    "        with open(test_path,'r') as fp:\n",
    "            data = fp.read().splitlines()\n",
    "            test_data = self.make_data(data)\n",
    "        return train_data,test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prp_obj = PreprocessData()\n",
    "train_data,test_data = prp_obj.get_data(\"data/en-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
