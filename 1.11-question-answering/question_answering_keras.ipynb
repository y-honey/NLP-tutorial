{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from tensorflow.keras.layers import add, dot, concatenate\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessData():\n",
    "    def tokenize(self, sent):\n",
    "        return [ x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "    \n",
    "    def parse_data(self, lines, only_supporting=False):\n",
    "        \n",
    "        '''Parse stories provided in the bAbi tasks format\n",
    "        If only_supporting is true, only the sentences\n",
    "        that support the answer are kept.\n",
    "        '''\n",
    "        data = []\n",
    "        story = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            nid, line = line.split(' ', 1)\n",
    "            nid = int(nid)\n",
    "            if nid == 1:\n",
    "                story = []\n",
    "            if '\\t' in line:\n",
    "                q, a, supporting = line.split('\\t')\n",
    "                q = self.tokenize(q)\n",
    "                substory = None\n",
    "                if only_supporting:\n",
    "                    # Only select the related substory\n",
    "                    supporting = map(int, supporting.split())\n",
    "                    substory = [story[i - 1] for i in supporting]\n",
    "                else:\n",
    "                    # Provide all the substories\n",
    "                    substory = [x for x in story if x]\n",
    "                data.append((substory, q, a))\n",
    "                story.append('')\n",
    "            else:\n",
    "                sent = self.tokenize(line)\n",
    "                story.append(sent)\n",
    "        return data\n",
    "    \n",
    "    def make_data(self, lines, only_supporting=False, max_length=None):\n",
    "        data = self.parse_data(lines, only_supporting=only_supporting)\n",
    "        flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "        data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def vectorize_stories(self, data, word_idx, story_maxlen, query_maxlen):\n",
    "        X = []\n",
    "        Xq = []\n",
    "        Y = []\n",
    "        for story, query, answer in data:\n",
    "            x = [word_idx[w] for w in story]\n",
    "            xq = [word_idx[w] for w in query]\n",
    "            # let's not forget that index 0 is reserved\n",
    "            y = np.zeros(len(word_idx) + 1)\n",
    "            y[word_idx[answer]] = 1\n",
    "            X.append(x)\n",
    "            Xq.append(xq)\n",
    "            Y.append(y)\n",
    "        return (pad_sequences(X, maxlen=story_maxlen),\n",
    "                pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))\n",
    "    \n",
    "    def get_data(self,path,type = \"qa1_single-supporting-fact\"):\n",
    "        train_path = os.path.join(path,type+'_train.txt')\n",
    "        test_path = os.path.join(path,type+'_test.txt')\n",
    "        with open(train_path,'r')as fp:\n",
    "            data = fp.read().splitlines()\n",
    "            train_data = self.make_data(data)\n",
    "        with open(test_path,'r') as fp:\n",
    "            data = fp.read().splitlines()\n",
    "            test_data = self.make_data(data)\n",
    "            \n",
    "        vocab = set()\n",
    "        for story, q, answer in train_data + test_data:\n",
    "            vocab |= set(story + q + [answer])\n",
    "        self.vocab = sorted(vocab)\n",
    "\n",
    "        # Reserve 0 for masking via pad_sequences\n",
    "        self.story_maxlen = max(map(len, (x for x, _, _ in train_data + test_data)))\n",
    "        self.query_maxlen = max(map(len, (x for _, x, _ in train_data + test_data)))\n",
    "        return train_data,test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "prp_obj = PreprocessData()\n",
    "train_data,test_data = prp_obj.get_data(\"data/en-10k\")\n",
    "vocab = prp_obj.vocab\n",
    "vocab_size = len(vocab)+1\n",
    "word2idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "idx2word = dict((i+1, c) for i,c in enumerate(vocab))\n",
    "story_maxlen = prp_obj.story_maxlen\n",
    "query_maxlen = prp_obj.query_maxlen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, queries_train, answers_train = prp_obj.vectorize_stories(train_data,\n",
    "                                                               word2idx,\n",
    "                                                               story_maxlen,\n",
    "                                                               query_maxlen)\n",
    "inputs_test, queries_test, answers_test = prp_obj.vectorize_stories(test_data,\n",
    "                                                            word2idx,\n",
    "                                                            story_maxlen,\n",
    "                                                            query_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epochs = 100\n",
    "batch_size = 32\n",
    "lstm_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateModel():\n",
    "    \n",
    "    def build_model(self):\n",
    "        # placeholders\n",
    "        input_sequence = Input((story_maxlen,))\n",
    "        question = Input((query_maxlen,))\n",
    "\n",
    "        print('Input sequence:', input_sequence)\n",
    "        print('Question:', question)\n",
    "\n",
    "        # encoders\n",
    "        # embed the input sequence into a sequence of vectors\n",
    "        input_encoder_m = Sequential()\n",
    "        input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                                      output_dim=64))\n",
    "        input_encoder_m.add(Dropout(0.3))\n",
    "        # output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "        # embed the input into a sequence of vectors of size query_maxlen\n",
    "        input_encoder_c = Sequential()\n",
    "        input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                                      output_dim=query_maxlen))\n",
    "        input_encoder_c.add(Dropout(0.3))\n",
    "        # output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "        # embed the question into a sequence of vectors\n",
    "        question_encoder = Sequential()\n",
    "        question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                                       output_dim=64,\n",
    "                                       input_length=query_maxlen))\n",
    "        question_encoder.add(Dropout(0.3))\n",
    "        # output: (samples, query_maxlen, embedding_dim)\n",
    "\n",
    "        # encode input sequence and questions (which are indices)\n",
    "        # to sequences of dense vectors\n",
    "        input_encoded_m = input_encoder_m(input_sequence)\n",
    "        print('Input encoded m', input_encoded_m)\n",
    "        input_encoded_c = input_encoder_c(input_sequence)\n",
    "        print('Input encoded c', input_encoded_c)\n",
    "        question_encoded = question_encoder(question)\n",
    "        print('Question encoded', question_encoded)\n",
    "\n",
    "\n",
    "        # compute a 'match' between the first input vector sequence\n",
    "        # and the question vector sequence\n",
    "        # shape: `(samples, story_maxlen, query_maxlen)\n",
    "        match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "        print(match.shape)\n",
    "        match = Activation('softmax')(match)\n",
    "        print('Match shape', match)\n",
    "\n",
    "        # add the match matrix with the second input vector sequence\n",
    "        response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "        response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
    "        print('Response shape', response)\n",
    "\n",
    "        # concatenate the response vector with the question vector sequence\n",
    "        answer = concatenate([response, question_encoded])\n",
    "        print('Answer shape', answer)\n",
    "\n",
    "        #answer = LSTM(lstm_size, return_sequences=True)(answer)  # Generate tensors of shape 32\n",
    "        #answer = Dropout(0.3)(answer)\n",
    "        answer = LSTM(lstm_size)(answer)  # Generate tensors of shape 32\n",
    "        answer = Dropout(0.3)(answer)\n",
    "        answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "        # we output a probability distribution over the vocabulary\n",
    "        answer = Activation('softmax')(answer)\n",
    "        # build the final model\n",
    "        self.model = Model([input_sequence, question], answer)\n",
    "        self.model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "                      metrics=['acc'])\n",
    "        print(self.model.summary())\n",
    "        \n",
    "    def train(self):\n",
    "        callbacks = None#[TrainingVisualizer()]\n",
    "\n",
    "        self.model.fit([inputs_train, queries_train], answers_train, batch_size, train_epochs,callbacks=callbacks,\n",
    "          validation_data=([inputs_test, queries_test], answers_test))\n",
    "\n",
    "        self.model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: Tensor(\"input_9:0\", shape=(None, 68), dtype=float32)\n",
      "Question: Tensor(\"input_10:0\", shape=(None, 4), dtype=float32)\n",
      "Input encoded m Tensor(\"sequential_12/Identity:0\", shape=(None, 68, 64), dtype=float32)\n",
      "Input encoded c Tensor(\"sequential_13/Identity:0\", shape=(None, 68, 4), dtype=float32)\n",
      "Question encoded Tensor(\"sequential_14/Identity:0\", shape=(None, 4, 64), dtype=float32)\n",
      "(None, 68, 4)\n",
      "Match shape Tensor(\"activation_7/Identity:0\", shape=(None, 68, 4), dtype=float32)\n",
      "Response shape Tensor(\"permute_4/Identity:0\", shape=(None, 4, 68), dtype=float32)\n",
      "Answer shape Tensor(\"concatenate_4/Identity:0\", shape=(None, 4, 132), dtype=float32)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 68)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      multiple             1408        input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_14 (Sequential)      (None, 4, 64)        1408        input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 68, 4)        0           sequential_12[1][0]              \n",
      "                                                                 sequential_14[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 68, 4)        0           dot_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_13 (Sequential)      multiple             88          input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 68, 4)        0           activation_7[0][0]               \n",
      "                                                                 sequential_13[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "permute_4 (Permute)             (None, 4, 68)        0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 4, 132)       0           permute_4[0][0]                  \n",
      "                                                                 sequential_14[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 64)           50432       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 64)           0           lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 22)           1430        dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 22)           0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 54,766\n",
      "Trainable params: 54,766\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 1.8750 - acc: 0.1722 - val_loss: 1.7999 - val_acc: 0.1820\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 1.6768 - acc: 0.2907 - val_loss: 1.6490 - val_acc: 0.3220\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 2s 159us/sample - loss: 1.5382 - acc: 0.3743 - val_loss: 1.5301 - val_acc: 0.3440\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 1.4842 - acc: 0.4092 - val_loss: 1.4569 - val_acc: 0.4430\n",
      "Epoch 5/100\n",
      " 9664/10000 [===========================>..] - ETA: 0s - loss: 1.4111 - acc: 0.4485"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-644c595551aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCreateModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-838a3a92b8b0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         self.model.fit([inputs_train, queries_train], answers_train, batch_size, train_epochs,callbacks=callbacks,\n\u001b[0;32m---> 78\u001b[0;31m           validation_data=([inputs_test, queries_test], answers_test))\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_obj = CreateModel()\n",
    "model_obj.build_model()\n",
    "model_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
