{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pickle import load\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation():\n",
    "    def __init__(self):\n",
    "        file = 'data/dataset.txt'\n",
    "        with open(file,'r') as fp:\n",
    "            self.content_list = fp.read().splitlines()[1:]\n",
    "        #data_frame = pd.read_csv(file)\n",
    "        #self.content_list = data_frame['content'].tolist()\n",
    "        print(self.content_list[:10])\n",
    "    \n",
    "    def remove_punctuations(self,content):\n",
    "        punctuators = '#$%&*+-/<=>@[\\\\]^_`{|}~\\t\\n'\n",
    "        for idx in range(len(content)):\n",
    "            for punc in punctuators:\n",
    "                content[idx] = content[idx].replace(punc, '') \n",
    "        return content\n",
    "    \n",
    "    def split_sentence(self,content):\n",
    "        new_data_list = list()\n",
    "        for item in content:\n",
    "            item = item.replace('\\n','')\n",
    "            item = item.replace('\\t','')\n",
    "        for item in content:\n",
    "            new_data_list = new_data_list+item.split('.')\n",
    "        content = new_data_list\n",
    "        return content\n",
    "    \n",
    "    def cleaning_data(self):\n",
    "        content = self.split_sentence(self.content_list)\n",
    "        print(\"Sentences Splited\")\n",
    "        content = self.remove_punctuations(content)\n",
    "        print('Punctuators Removed')\n",
    "        content = list(set(content))\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dear Sir/Madam', 'Dear', 'Hello', 'Hi', 'Hi Team', 'Good morning Team', 'Good afternoon', 'I recently read about that', 'I recently heard about that', 'Thank you for taking the time to write to us']\n"
     ]
    }
   ],
   "source": [
    "dp_obj = DataPreparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences Splited\n",
      "Punctuators Removed\n"
     ]
    }
   ],
   "source": [
    "content = dp_obj.cleaning_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    def __init__(self):\n",
    "        self.content_list = content\n",
    "        \n",
    "    def make_data(self):\n",
    "        \n",
    "        data = list()\n",
    "        for sentence in self.content_list:\n",
    "            for idx in range(1, len(sentence)):\n",
    "                x = '<start> '+ sentence[:idx+1] + ' <end>'\n",
    "                y = '<start> '+ sentence[idx+1:] + ' <end>'\n",
    "                data.append([x,y])\n",
    "        random.shuffle(data)\n",
    "        print(\"data: \",data[10:20])\n",
    "        return data\n",
    "    \n",
    "    def create_vacab(self,pairs):\n",
    "        vocab = set()\n",
    "        word2idx = dict() \n",
    "        for phrase in pairs:\n",
    "            vocab.update(phrase.split(' '))\n",
    "        vocab = sorted(vocab)\n",
    "        word2idx[\"<pad>\"] = 0\n",
    "        for i,word in enumerate(vocab):\n",
    "            word2idx[word] = i + 1\n",
    "        return word2idx\n",
    "            \n",
    "    def get_data(self):\n",
    "        data = self.make_data()\n",
    "        \n",
    "        self.inputs = list()\n",
    "        self.outputs = list()\n",
    "        for item in data:\n",
    "            self.inputs.append(item[0])\n",
    "            self.outputs.append(item[1])\n",
    "        \n",
    "        self.in_word2idx = self.create_vacab(self.inputs)\n",
    "        self.out_word2idx = self.create_vacab(self.outputs)\n",
    "        \n",
    "        self.in_vocab_size = len(self.in_word2idx)\n",
    "        self.out_vocab_size = len(self.out_word2idx)\n",
    "        \n",
    "        self.in_maxlen = max(len(item.split(' ')) for item in self.inputs)\n",
    "        self.out_maxlen = max(len(item.split(' ')) for item in self.outputs)\n",
    "        \n",
    "        \n",
    "    def create_data(self,inputs,outputs,in_maxlen,out_maxlen,in_vocab_size,out_vocab_size):\n",
    "        input_data = [[in_word2idx[word] for word in sentence.split(' ')] for sentence in inputs]\n",
    "        output_data = [[out_word2idx[word] for word in sentence.split(' ')] for sentence in outputs]\n",
    "        input_data = tf.keras.preprocessing.sequence.pad_sequences(input_data, maxlen=in_maxlen, padding=\"post\")\n",
    "        output_data = tf.keras.preprocessing.sequence.pad_sequences(output_data, maxlen=out_maxlen, padding=\"post\")\n",
    "        \n",
    "        target_data = [[output_data[n][i+1] for i in range(len(output_data[n])-1)] for n in range(len(output_data))]\n",
    "        target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, maxlen=out_maxlen, padding=\"post\")\n",
    "        target_data = target_data.reshape((target_data.shape[0], target_data.shape[1], 1))\n",
    "        print(input_data.shape)\n",
    "        print(output_data.shape)\n",
    "        print(target_data.shape)\n",
    "        \n",
    "        return (input_data, output_data,target_data)\n",
    "    \n",
    "    def data_generator(self,x,y,in_maxlen,out_maxlen,in_vocab_size,out_vocab_size,batch_size):\n",
    "        ''' Generate a batch of data '''\n",
    "        while True:\n",
    "            for j in range(0, len(x), batch_size):\n",
    "                encoder_input_data = np.zeros((batch_size, in_maxlen),dtype='float32')\n",
    "                decoder_input_data = np.zeros((batch_size, out_maxlen),dtype='float32')\n",
    "                decoder_target_data = np.zeros((batch_size, out_maxlen),dtype='float32')\n",
    "                for i, (input_text, target_text) in enumerate(zip(x[j:j+batch_size], y[j:j+batch_size])):\n",
    "                    for t, word in enumerate(input_text.split(' ')):\n",
    "                        encoder_input_data[i, t] = in_word2idx[word]\n",
    "                        \n",
    "                    for t, word in enumerate(target_text.split(' ')):\n",
    "                        if t<len(target_text.split())-1:\n",
    "                            decoder_input_data[i, t] = out_word2idx[word]\n",
    "                        if t>0:\n",
    "                            decoder_target_data[i, t - 1] = out_word2idx[word]\n",
    "                encoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(encoder_input_data, maxlen=in_maxlen, padding=\"post\")\n",
    "                decoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(decoder_input_data, maxlen=out_maxlen, padding=\"post\")\n",
    "                decoder_target_data = tf.keras.preprocessing.sequence.pad_sequences(decoder_target_data, maxlen=out_maxlen, padding=\"post\")\n",
    "                yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  [['<start> Are you asking for a favor or you are meeting <end>', '<start>  soon? <end>'], ['<start> If you have any que <end>', '<start> stions please email or call me <end>'], [\"<start> I'm glad we had a chance to chat at the conv <end>\", '<start> ention <end>'], [\"<start> I'd be very grateful if you  <end>\", '<start> could <end>'], ['<start> It was nice to hear from y <end>', '<start> ou yesterday <end>'], ['<start> Thanks f <end>', '<start> or getting back to me so quickly <end>'], ['<start> In the meantime <end>', '<start> , if you need any more information <end>'], [\"<start> Hope you're feeling great <end>\", '<start> ! <end>'], [\"<start> I'm writing to tell you tha <end>\", '<start> t <end>'], [\"<start> It's out of my hands <end>\", '<start>  <end>']]\n",
      "12537\n",
      "3135\n",
      "in_maxlen:  21\n",
      "out_maxlen:  21\n"
     ]
    }
   ],
   "source": [
    "preprocess_obj = Preprocessing()\n",
    "\n",
    "preprocess_obj.get_data()\n",
    "# Training Params\n",
    "in_vocab_size = preprocess_obj.in_vocab_size\n",
    "out_vocab_size = preprocess_obj.out_vocab_size\n",
    "in_maxlen = preprocess_obj.in_maxlen\n",
    "out_maxlen = preprocess_obj.out_maxlen\n",
    "in_word2idx = preprocess_obj.in_word2idx\n",
    "out_word2idx = preprocess_obj.out_word2idx\n",
    "x = preprocess_obj.inputs\n",
    "y = preprocess_obj.outputs\n",
    "train_size = int(len(x)*0.8)\n",
    "train_x = x[:train_size]\n",
    "train_y = y[:train_size]\n",
    "val_x = x[train_size:]\n",
    "val_y = y[train_size:]\n",
    "print(len(train_x))\n",
    "print(len(val_x))\n",
    "print(\"in_maxlen: \",in_maxlen)\n",
    "print(\"out_maxlen: \",out_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateModel():\n",
    "    \n",
    "    def build(self,epochs,batch_size):\n",
    "        embedding_dim = 300\n",
    "        n_units = 128\n",
    "        '''Enoder'''\n",
    "        encoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
    "        encoder_emb = tf.keras.layers.Embedding(input_dim=in_vocab_size, output_dim=embedding_dim)\n",
    "        encoder_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=n_units, return_sequences=True, return_state=True))\n",
    "        \n",
    "        encoder_lstm_out, fstate_h, fstate_c, bstate_h, bstate_c = encoder_lstm(encoder_emb(encoder_inputs))\n",
    "        state_h = tf.keras.layers.Concatenate()([fstate_h,bstate_h])\n",
    "        state_c = tf.keras.layers.Concatenate()([bstate_h,bstate_c])\n",
    "        encoder_states = [state_h, state_c]\n",
    "        \n",
    "        '''Decoder'''\n",
    "        decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
    "        decoder_emb = tf.keras.layers.Embedding(input_dim=out_vocab_size, output_dim=embedding_dim)\n",
    "        decoder_lstm = tf.keras.layers.LSTM(units=n_units*2, return_sequences=True, return_state=True)\n",
    "        \n",
    "        decoder_lstm_out, _, _ = decoder_lstm(decoder_emb(decoder_inputs), initial_state=encoder_states)\n",
    "        \n",
    "        \n",
    "        '''Dense layer'''\n",
    "        decoder_dense = tf.keras.models.Sequential()\n",
    "        decoder_dense.add(tf.keras.layers.Dense(n_units, activation=\"relu\"))\n",
    "        decoder_dense.add(tf.keras.layers.Dropout(rate=.2))\n",
    "        decoder_dense.add(tf.keras.layers.Dense(out_vocab_size, activation=\"softmax\"))\n",
    "        decoder_out = decoder_dense(decoder_lstm_out)\n",
    "\n",
    "        self.model = tf.keras.models.Model(inputs = [encoder_inputs, decoder_inputs], outputs= decoder_out)\n",
    "        \n",
    "        '''Compile and Run'''\n",
    "        self.model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['acc'])\n",
    "        self.model.summary()\n",
    "        self.run(epochs,batch_size)\n",
    "        #self.run_gen(epochs,batch_size)\n",
    "        '''Here's our inference setup'''\n",
    "        self.encoder_model = tf.keras.models.Model(encoder_inputs, [encoder_lstm_out, state_h, state_c])\n",
    "\n",
    "        #inf_decoder_inputs = Input(shape=(None,))\n",
    "        decoder_state_input_h = tf.keras.layers.Input(shape=(n_units*2,))\n",
    "        decoder_state_input_c = tf.keras.layers.Input(shape=(n_units*2,))\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "        decoder_outputs, decoder_h, decoder_c = decoder_lstm(decoder_emb(decoder_inputs),\n",
    "                                                         initial_state=decoder_states_inputs)\n",
    "        \n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        \n",
    "        inputs = [decoder_inputs, decoder_state_input_h, decoder_state_input_c]\n",
    "        outputs = [decoder_outputs, decoder_h, decoder_c]\n",
    "        \n",
    "        self.decoder_model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def run_gen(self,epochs,batch_size):\n",
    "        epochs = 2\n",
    "        train_steps = len(train_x) / batch_size\n",
    "        val_steps = len(val_x) / batch_size\n",
    "        train_data = preprocess_obj.data_generator(train_x,train_y,in_maxlen,out_maxlen,in_vocab_size,out_vocab_size,batch_size)\n",
    "        val_data = preprocess_obj.data_generator(val_x,val_y,in_maxlen,out_maxlen,in_vocab_size,out_vocab_size,batch_size)\n",
    "        self.history = self.model.fit(train_data, steps_per_epoch = train_steps,\n",
    "                                      validation_data = val_data, validation_steps = val_steps,\n",
    "                                      epochs=epochs)\n",
    "        \n",
    "    def run(self,epochs,batch_size):\n",
    "        train_data = preprocess_obj.create_data(train_x,train_y,\n",
    "                                                in_maxlen,out_maxlen,\n",
    "                                                in_vocab_size,out_vocab_size)\n",
    "        val_data = preprocess_obj.create_data(val_x,val_y,\n",
    "                                              in_maxlen,out_maxlen,\n",
    "                                              in_vocab_size,out_vocab_size)\n",
    "        \n",
    "        train_enc_in, train_dec_in,train_dec_out = train_data\n",
    "        val_enc_in,val_dec_in,val_dec_out = val_data\n",
    "        self.history = self.model.fit([train_enc_in, train_dec_in], train_dec_out, \n",
    "                                      validation_data=([val_enc_in,val_dec_in],val_dec_out),\n",
    "                                      batch_size= batch_size, epochs=epochs,verbose=1)\n",
    "        \n",
    "        \n",
    "    def save_model(self,model,model_file):\n",
    "        with open(model_file+'.json', 'w', encoding='utf8') as f:\n",
    "            f.write(model.to_json())\n",
    "        model.save_weights(model_file+'.h5')\n",
    "        \n",
    "    def save(self):\n",
    "        encoder_model_name = 'models/Encoder_Model1'\n",
    "        self.save_model(self.encoder_model,encoder_model_name)\n",
    "        \n",
    "        decoder_model_name = 'models/Decoder_Model1'\n",
    "        self.save_model(self.decoder_model,decoder_model_name)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 300)    654300      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   [(None, None, 256),  439296      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    612900      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           bidirectional[0][1]              \n",
      "                                                                 bidirectional[0][3]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           bidirectional[0][3]              \n",
      "                                                                 bidirectional[0][4]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  570368      embedding_1[0][0]                \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, None, 2043)   296443      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,573,307\n",
      "Trainable params: 2,573,307\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(12537, 21)\n",
      "(12537, 21)\n",
      "(12537, 21, 1)\n",
      "(3135, 21)\n",
      "(3135, 21)\n",
      "(3135, 21, 1)\n",
      "Train on 12537 samples, validate on 3135 samples\n",
      "Epoch 1/70\n",
      "12537/12537 [==============================] - 60s 5ms/sample - loss: 1.3758 - acc: 0.7965 - val_loss: 1.0399 - val_acc: 0.8183\n",
      "Epoch 2/70\n",
      "12537/12537 [==============================] - 55s 4ms/sample - loss: 0.9707 - acc: 0.8220 - val_loss: 0.8911 - val_acc: 0.8326\n",
      "Epoch 3/70\n",
      "12537/12537 [==============================] - 57s 5ms/sample - loss: 0.8235 - acc: 0.8416 - val_loss: 0.7485 - val_acc: 0.8575\n",
      "Epoch 4/70\n",
      "12537/12537 [==============================] - 57s 5ms/sample - loss: 0.6829 - acc: 0.8652 - val_loss: 0.6305 - val_acc: 0.8792\n",
      "Epoch 5/70\n",
      "12537/12537 [==============================] - 58s 5ms/sample - loss: 0.5709 - acc: 0.8834 - val_loss: 0.5369 - val_acc: 0.8962\n",
      "Epoch 6/70\n",
      "12537/12537 [==============================] - 58s 5ms/sample - loss: 0.4846 - acc: 0.8986 - val_loss: 0.4787 - val_acc: 0.9060\n",
      "Epoch 7/70\n",
      "12537/12537 [==============================] - 52s 4ms/sample - loss: 0.4167 - acc: 0.9098 - val_loss: 0.4271 - val_acc: 0.9127\n",
      "Epoch 8/70\n",
      "12537/12537 [==============================] - 52s 4ms/sample - loss: 0.3638 - acc: 0.9189 - val_loss: 0.3944 - val_acc: 0.9201\n",
      "Epoch 9/70\n",
      "12537/12537 [==============================] - 50s 4ms/sample - loss: 0.3210 - acc: 0.9263 - val_loss: 0.3637 - val_acc: 0.9261\n",
      "Epoch 10/70\n",
      "12537/12537 [==============================] - 51s 4ms/sample - loss: 0.2869 - acc: 0.9325 - val_loss: 0.3428 - val_acc: 0.9285\n",
      "Epoch 11/70\n",
      "12537/12537 [==============================] - 71s 6ms/sample - loss: 0.2563 - acc: 0.9376 - val_loss: 0.3254 - val_acc: 0.9326\n",
      "Epoch 12/70\n",
      "12537/12537 [==============================] - 58s 5ms/sample - loss: 0.2314 - acc: 0.9419 - val_loss: 0.3213 - val_acc: 0.9353\n",
      "Epoch 13/70\n",
      "12537/12537 [==============================] - 57s 5ms/sample - loss: 0.2082 - acc: 0.9467 - val_loss: 0.3065 - val_acc: 0.9385\n",
      "Epoch 14/70\n",
      "12537/12537 [==============================] - 60s 5ms/sample - loss: 0.1885 - acc: 0.9500 - val_loss: 0.2964 - val_acc: 0.9409\n",
      "Epoch 15/70\n",
      "12537/12537 [==============================] - 61s 5ms/sample - loss: 0.1706 - acc: 0.9531 - val_loss: 0.2924 - val_acc: 0.9419\n",
      "Epoch 16/70\n",
      "12537/12537 [==============================] - 57s 5ms/sample - loss: 0.1564 - acc: 0.9565 - val_loss: 0.2846 - val_acc: 0.9441\n",
      "Epoch 17/70\n",
      "12537/12537 [==============================] - 61s 5ms/sample - loss: 0.1422 - acc: 0.9595 - val_loss: 0.2793 - val_acc: 0.9455\n",
      "Epoch 18/70\n",
      "12537/12537 [==============================] - 54s 4ms/sample - loss: 0.1312 - acc: 0.9617 - val_loss: 0.2786 - val_acc: 0.9464\n",
      "Epoch 19/70\n",
      "12537/12537 [==============================] - 55s 4ms/sample - loss: 0.1218 - acc: 0.9635 - val_loss: 0.2764 - val_acc: 0.9480\n",
      "Epoch 20/70\n",
      "12537/12537 [==============================] - 49s 4ms/sample - loss: 0.1126 - acc: 0.9660 - val_loss: 0.2704 - val_acc: 0.9493\n",
      "Epoch 21/70\n",
      "12537/12537 [==============================] - 48s 4ms/sample - loss: 0.1054 - acc: 0.9676 - val_loss: 0.2677 - val_acc: 0.9505\n",
      "Epoch 22/70\n",
      "12537/12537 [==============================] - 48s 4ms/sample - loss: 0.0995 - acc: 0.9690 - val_loss: 0.2706 - val_acc: 0.9504\n",
      "Epoch 23/70\n",
      "12537/12537 [==============================] - 53s 4ms/sample - loss: 0.0936 - acc: 0.9704 - val_loss: 0.2671 - val_acc: 0.9506\n",
      "Epoch 24/70\n",
      "12537/12537 [==============================] - 52s 4ms/sample - loss: 0.0897 - acc: 0.9711 - val_loss: 0.2643 - val_acc: 0.9524\n",
      "Epoch 25/70\n",
      "12537/12537 [==============================] - 52s 4ms/sample - loss: 0.0845 - acc: 0.9724 - val_loss: 0.2672 - val_acc: 0.9527\n",
      "Epoch 26/70\n",
      "12537/12537 [==============================] - 57s 5ms/sample - loss: 0.0809 - acc: 0.9732 - val_loss: 0.2661 - val_acc: 0.9521\n",
      "Epoch 27/70\n",
      "12537/12537 [==============================] - 58s 5ms/sample - loss: 0.0767 - acc: 0.9741 - val_loss: 0.2673 - val_acc: 0.9525\n",
      "Epoch 28/70\n",
      "12537/12537 [==============================] - 63s 5ms/sample - loss: 0.0743 - acc: 0.9746 - val_loss: 0.2659 - val_acc: 0.9528\n",
      "Epoch 29/70\n",
      "12537/12537 [==============================] - 50s 4ms/sample - loss: 0.0715 - acc: 0.9757 - val_loss: 0.2696 - val_acc: 0.9526\n",
      "Epoch 30/70\n",
      "12537/12537 [==============================] - 49s 4ms/sample - loss: 0.0691 - acc: 0.9761 - val_loss: 0.2681 - val_acc: 0.9532\n",
      "Epoch 31/70\n",
      "12537/12537 [==============================] - 49s 4ms/sample - loss: 0.0672 - acc: 0.9766 - val_loss: 0.2682 - val_acc: 0.9536\n",
      "Epoch 32/70\n",
      "12537/12537 [==============================] - 49s 4ms/sample - loss: 0.0650 - acc: 0.9771 - val_loss: 0.2652 - val_acc: 0.9542\n",
      "Epoch 33/70\n",
      "12537/12537 [==============================] - 49s 4ms/sample - loss: 0.0640 - acc: 0.9771 - val_loss: 0.2692 - val_acc: 0.9542\n",
      "Epoch 34/70\n",
      "12537/12537 [==============================] - 49s 4ms/sample - loss: 0.0622 - acc: 0.9775 - val_loss: 0.2696 - val_acc: 0.9547\n",
      "Epoch 35/70\n",
      "12537/12537 [==============================] - 49s 4ms/sample - loss: 0.0602 - acc: 0.9781 - val_loss: 0.2728 - val_acc: 0.9535\n",
      "Epoch 36/70\n",
      "12537/12537 [==============================] - 52s 4ms/sample - loss: 0.0590 - acc: 0.9782 - val_loss: 0.2708 - val_acc: 0.9537\n",
      "Epoch 37/70\n",
      "12537/12537 [==============================] - 52s 4ms/sample - loss: 0.0577 - acc: 0.9790 - val_loss: 0.2739 - val_acc: 0.9544\n",
      "Epoch 38/70\n",
      "12537/12537 [==============================] - 49s 4ms/sample - loss: 0.0565 - acc: 0.9791 - val_loss: 0.2708 - val_acc: 0.9543\n",
      "Epoch 39/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12537/12537 [==============================] - 52s 4ms/sample - loss: 0.0561 - acc: 0.9792 - val_loss: 0.2752 - val_acc: 0.9542\n",
      "Epoch 40/70\n",
      "12537/12537 [==============================] - 50s 4ms/sample - loss: 0.0559 - acc: 0.9790 - val_loss: 0.2733 - val_acc: 0.9541\n",
      "Epoch 41/70\n",
      "12537/12537 [==============================] - 53s 4ms/sample - loss: 0.0540 - acc: 0.9798 - val_loss: 0.2751 - val_acc: 0.9545\n",
      "Epoch 42/70\n",
      "12537/12537 [==============================] - 52s 4ms/sample - loss: 0.0534 - acc: 0.9797 - val_loss: 0.2736 - val_acc: 0.9547\n",
      "Epoch 43/70\n",
      "12537/12537 [==============================] - 62s 5ms/sample - loss: 0.0533 - acc: 0.9799 - val_loss: 0.2754 - val_acc: 0.9551\n",
      "Epoch 44/70\n",
      "12537/12537 [==============================] - 59s 5ms/sample - loss: 0.0522 - acc: 0.9798 - val_loss: 0.2698 - val_acc: 0.9548\n",
      "Epoch 45/70\n",
      "12537/12537 [==============================] - 59s 5ms/sample - loss: 0.0516 - acc: 0.9801 - val_loss: 0.2712 - val_acc: 0.9555\n",
      "Epoch 46/70\n",
      "12537/12537 [==============================] - 58s 5ms/sample - loss: 0.0506 - acc: 0.9805 - val_loss: 0.2765 - val_acc: 0.9547\n",
      "Epoch 47/70\n",
      "12537/12537 [==============================] - 60s 5ms/sample - loss: 0.0500 - acc: 0.9805 - val_loss: 0.2760 - val_acc: 0.9551\n",
      "Epoch 48/70\n",
      "12537/12537 [==============================] - 62s 5ms/sample - loss: 0.0490 - acc: 0.9809 - val_loss: 0.2780 - val_acc: 0.9550\n",
      "Epoch 49/70\n",
      "12537/12537 [==============================] - 68s 5ms/sample - loss: 0.0488 - acc: 0.9807 - val_loss: 0.2802 - val_acc: 0.9549\n",
      "Epoch 50/70\n",
      "12537/12537 [==============================] - 62s 5ms/sample - loss: 0.0491 - acc: 0.9805 - val_loss: 0.2726 - val_acc: 0.9556\n",
      "Epoch 51/70\n",
      "12537/12537 [==============================] - 56s 4ms/sample - loss: 0.0484 - acc: 0.9811 - val_loss: 0.2798 - val_acc: 0.9553\n",
      "Epoch 52/70\n",
      "12537/12537 [==============================] - 57s 5ms/sample - loss: 0.0475 - acc: 0.9811 - val_loss: 0.2768 - val_acc: 0.9560\n",
      "Epoch 53/70\n",
      "12537/12537 [==============================] - 56s 4ms/sample - loss: 0.0470 - acc: 0.9815 - val_loss: 0.2799 - val_acc: 0.9557\n",
      "Epoch 54/70\n",
      "12537/12537 [==============================] - 64s 5ms/sample - loss: 0.0471 - acc: 0.9813 - val_loss: 0.2754 - val_acc: 0.9555\n",
      "Epoch 55/70\n",
      "12537/12537 [==============================] - 61s 5ms/sample - loss: 0.0471 - acc: 0.9812 - val_loss: 0.2813 - val_acc: 0.9554\n",
      "Epoch 56/70\n",
      "12537/12537 [==============================] - 61s 5ms/sample - loss: 0.0464 - acc: 0.9814 - val_loss: 0.2812 - val_acc: 0.9553\n",
      "Epoch 57/70\n",
      "12537/12537 [==============================] - 63s 5ms/sample - loss: 0.0459 - acc: 0.9816 - val_loss: 0.2822 - val_acc: 0.9553\n",
      "Epoch 58/70\n",
      "12537/12537 [==============================] - 60s 5ms/sample - loss: 0.0456 - acc: 0.9816 - val_loss: 0.2794 - val_acc: 0.9555\n",
      "Epoch 59/70\n",
      "12537/12537 [==============================] - 62s 5ms/sample - loss: 0.0462 - acc: 0.9815 - val_loss: 0.2787 - val_acc: 0.9559\n",
      "Epoch 60/70\n",
      "12537/12537 [==============================] - 58s 5ms/sample - loss: 0.0454 - acc: 0.9818 - val_loss: 0.2807 - val_acc: 0.9560\n",
      "Epoch 61/70\n",
      "12537/12537 [==============================] - 53s 4ms/sample - loss: 0.0449 - acc: 0.9821 - val_loss: 0.2840 - val_acc: 0.9559\n",
      "Epoch 62/70\n",
      "12537/12537 [==============================] - 52s 4ms/sample - loss: 0.0447 - acc: 0.9816 - val_loss: 0.2868 - val_acc: 0.9548\n",
      "Epoch 63/70\n",
      "12537/12537 [==============================] - 59s 5ms/sample - loss: 0.0442 - acc: 0.9820 - val_loss: 0.2864 - val_acc: 0.9558\n",
      "Epoch 64/70\n",
      "12537/12537 [==============================] - 60s 5ms/sample - loss: 0.0445 - acc: 0.9819 - val_loss: 0.2815 - val_acc: 0.9565\n",
      "Epoch 65/70\n",
      "12537/12537 [==============================] - 57s 5ms/sample - loss: 0.0440 - acc: 0.9818 - val_loss: 0.2873 - val_acc: 0.9558\n",
      "Epoch 66/70\n",
      "12537/12537 [==============================] - 58s 5ms/sample - loss: 0.0436 - acc: 0.9820 - val_loss: 0.2844 - val_acc: 0.9559\n",
      "Epoch 67/70\n",
      "12537/12537 [==============================] - 58s 5ms/sample - loss: 0.0437 - acc: 0.9819 - val_loss: 0.2927 - val_acc: 0.9554\n",
      "Epoch 68/70\n",
      "12537/12537 [==============================] - 58s 5ms/sample - loss: 0.0440 - acc: 0.9819 - val_loss: 0.2964 - val_acc: 0.9558\n",
      "Epoch 69/70\n",
      "12537/12537 [==============================] - 65s 5ms/sample - loss: 0.0430 - acc: 0.9821 - val_loss: 0.2881 - val_acc: 0.9557\n",
      "Epoch 70/70\n",
      "12537/12537 [==============================] - 59s 5ms/sample - loss: 0.0434 - acc: 0.9822 - val_loss: 0.2893 - val_acc: 0.9558\n"
     ]
    }
   ],
   "source": [
    "model_obj = CreateModel()\n",
    "model_obj.build(70,32)\n",
    "model_obj.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction():\n",
    "    def __init__(self):\n",
    "        self.input_max_len = 21\n",
    "        self.target_max_len = 21\n",
    "        self.load_model()\n",
    "        \n",
    "    def load_weights(self,model_filename, model_weights_filename):\n",
    "        with open(model_filename, 'r', encoding='utf8') as f:\n",
    "            model = tf.keras.models.model_from_json(f.read())\n",
    "        model.load_weights(model_weights_filename)\n",
    "        return model\n",
    "    \n",
    "    def load_model(self):\n",
    "        encoder_model_name = 'models/Encoder_Model1'\n",
    "        decoder_model_name = 'models/Decoder_Model1'\n",
    "        self.encoder_model = self.load_weights(encoder_model_name+'.json', encoder_model_name+'.h5')\n",
    "        self.decoder_model = self.load_weights(decoder_model_name+'.json', decoder_model_name+'.h5')\n",
    "        \n",
    "        in_word2idx_dict = 'dict/in_word2idx_dict.pkl'\n",
    "        self.in_word2idx = load(open(in_word2idx_dict, 'rb'))\n",
    "        self.in_idx2word = {v:k for k,v in self.in_word2idx.items()}\n",
    "        out_word2idx_dict = 'dict/out_word2idx_dict.pkl'\n",
    "        self.out_word2idx =load(open(out_word2idx_dict, 'rb'))\n",
    "        self.out_idx2word = {v:k for k,v in self.out_word2idx.items()}\n",
    "        \n",
    "    def sentence_to_vector(self, sentence):\n",
    "        pre = sentence\n",
    "        vec = np.zeros(self.input_max_len)\n",
    "        sentence_list = [self.in_word2idx[s] for s in pre.split(' ')]\n",
    "        for i,w in enumerate(sentence_list):\n",
    "            vec[i] = w\n",
    "        return vec\n",
    "\n",
    "    def predict_(self,input_sentence):\n",
    "        sv = self.sentence_to_vector(input_sentence)\n",
    "        sv = sv.reshape(1,len(sv))\n",
    "        [emb_out, sh, sc] = self.encoder_model.predict(x=sv)\n",
    "\n",
    "        i = 0\n",
    "        start_vec = self.out_word2idx[\"<start>\"]\n",
    "        stop_vec = self.out_word2idx[\"<end>\"]\n",
    "\n",
    "        cur_vec = np.zeros((1,1))\n",
    "        cur_vec[0,0] = start_vec\n",
    "        cur_word = \"<start>\"\n",
    "        ouput_list = list()\n",
    "        output_sentence = \"\"\n",
    "        while cur_word != \"<end>\" and i < (self.target_max_len-1):\n",
    "            i += 1\n",
    "            if cur_word != \"<start>\":\n",
    "                output_sentence = output_sentence + \" \" + cur_word\n",
    "            x_in = [cur_vec, sh, sc]\n",
    "            [nvec, sh, sc] = self.decoder_model.predict(x=x_in)\n",
    "            cur_vec[0,0] = np.argmax(nvec[0,0])\n",
    "            cur_word = self.out_idx2word[np.argmax(nvec[0,0])]\n",
    "        return output_sentence\n",
    "        \n",
    "    def predict(self,input_sentence):\n",
    "        \n",
    "        sv = self.sentence_to_vector(input_sentence)\n",
    "        sv = sv.reshape(1,len(sv))\n",
    "        [emb_out, sh, sc] = self.encoder_model.predict(x=sv)   \n",
    "        states_value = [sh,sc]\n",
    "        target_seq = np.zeros((1,1))\n",
    "        start_vec = self.out_word2idx[\"<start>\"]\n",
    "        target_seq[0, 0] = start_vec\n",
    "        stop_condition = False\n",
    "        decoded_list = []\n",
    "        while not stop_condition:\n",
    "            output_tokens, h, c = self.decoder_model.predict([target_seq]+states_value)\n",
    "    \n",
    "            # Sample a token\n",
    "            sampled_token_index = np.argmax(output_tokens[0, 0])\n",
    "            sampled_word = self.out_idx2word[sampled_token_index]\n",
    "            decoded_list.append(sampled_word)\n",
    "    \n",
    "\n",
    "            if len(decoded_list)> self.target_max_len or sampled_word==\"<end>\":\n",
    "                del decoded_list[-1]\n",
    "                stop_condition = True\n",
    "    \n",
    "            target_seq = np.zeros((1,1))\n",
    "            target_seq[0, 0] = sampled_token_index\n",
    "    \n",
    "            states_value = [h, c]\n",
    "    \n",
    "        return ' '.join(decoded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_obj = Prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:  Thank you for contacting to us\n",
      "Result:  Please find the attached file\n",
      "Result:  Hope you're haeck out of my hands\n",
      "Result:  I would be happy to inform you that\n",
      "Result:  Please let me know's out of my hands\n",
      "Result:  For further distion to implement\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    \"Thank you for contac\",\n",
    "    \"Please find the\",\n",
    "    \"Hope you're ha\",\n",
    "    \"I would be happy\",\n",
    "    \"Please let me know\",\n",
    "    \"For further dis\"\n",
    "]\n",
    "for item in test_list:\n",
    "    print(\"Result: \",item+predict_obj.predict(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
