{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pickle import load\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation():\n",
    "    def __init__(self):\n",
    "        file = 'data/dataset.txt'\n",
    "        with open(file,'r') as fp:\n",
    "            self.content_list = fp.read().splitlines()[1:]\n",
    "        #data_frame = pd.read_csv(file)\n",
    "        #self.content_list = data_frame['content'].tolist()\n",
    "        print(self.content_list[:10])\n",
    "    \n",
    "    def remove_punctuations(self,content):\n",
    "        punctuators = '#$%&*+-/<=>@[\\\\]^_`{|}~\\t\\n'\n",
    "        for idx in range(len(content)):\n",
    "            for punc in punctuators:\n",
    "                content[idx] = content[idx].replace(punc, '') \n",
    "        return content\n",
    "    \n",
    "    def removing_sentence(self,content):\n",
    "        drop_index_list = list()\n",
    "        for idx in range(len(content)):\n",
    "            words = content[idx].split()\n",
    "            if len(words)<=2 or len(words)>100:\n",
    "                drop_index_list.append(idx)\n",
    "        content = np.array(content)\n",
    "        content = np.delete(content,drop_index_list)\n",
    "        content = content.tolist()\n",
    "        return content\n",
    "    \n",
    "    def split_sentence(self,content):\n",
    "        new_data_list = list()\n",
    "        for item in content:\n",
    "            item = item.replace('\\n','')\n",
    "            item = item.replace('\\t','')\n",
    "        for item in content:\n",
    "            new_data_list = new_data_list+item.split('.')\n",
    "        content = new_data_list\n",
    "        return content\n",
    "    \n",
    "    def cleaning_data(self):\n",
    "        content = self.split_sentence(self.content_list)\n",
    "        print(\"Sentences Splited\")\n",
    "        content = self.remove_punctuations(content)\n",
    "        print('Punctuators Removed')\n",
    "        content = list(set(content))\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dear Sir/Madam', 'Dear', 'Hello', 'Hi', 'Hi Team', 'Good morning Team', 'Good afternoon', 'I recently read about that', 'I recently heard about that', 'Thank you for taking the time to write to us']\n"
     ]
    }
   ],
   "source": [
    "dp_obj = DataPreparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences Splited\n",
      "Punctuators Removed\n"
     ]
    }
   ],
   "source": [
    "content = dp_obj.cleaning_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    def __init__(self):\n",
    "        self.content_list = content\n",
    "        \n",
    "    def make_data(self):\n",
    "        \n",
    "        data = list()\n",
    "        for sentence in self.content_list:\n",
    "            for idx in range(1, len(sentence)):\n",
    "                x = '<start> '+ sentence[:idx+1] + ' <end>'\n",
    "                y = '<start> '+ sentence[idx+1:] + ' <end>'\n",
    "                data.append([x,y])\n",
    "        random.shuffle(data)\n",
    "        print(\"data: \",data[10:20])\n",
    "        return data\n",
    "    \n",
    "    def create_vacab(self,pairs):\n",
    "        vocab = set()\n",
    "        word2idx = dict() \n",
    "        for phrase in pairs:\n",
    "            vocab.update(phrase.split(' '))\n",
    "        vocab = sorted(vocab)\n",
    "        word2idx[\"<pad>\"] = 0\n",
    "        for i,word in enumerate(vocab):\n",
    "            word2idx[word] = i + 1\n",
    "        return word2idx\n",
    "            \n",
    "    def get_data(self):\n",
    "        data = self.make_data()\n",
    "        \n",
    "        self.inputs = list()\n",
    "        self.outputs = list()\n",
    "        for item in data:\n",
    "            self.inputs.append(item[0])\n",
    "            self.outputs.append(item[1])\n",
    "        \n",
    "        self.in_word2idx = self.create_vacab(self.inputs)\n",
    "        self.out_word2idx = self.create_vacab(self.outputs)\n",
    "        \n",
    "        self.in_vocab_size = len(in_word2idx)\n",
    "        self.out_vocab_size = len(out_word2idx)\n",
    "        \n",
    "        self.in_maxlen = max(len(item.split(' ')) for item in self.inputs)\n",
    "        self.out_maxlen = max(len(item.split(' ')) for item in self.outputs)\n",
    "        \n",
    "        \n",
    "    def create_data(self,inputs,outputs,in_maxlen,out_maxlen,in_vocab_size,out_vocab_size):\n",
    "        input_data = [[in_word2idx[word] for word in sentence.split(' ')] for sentence in inputs]\n",
    "        output_data = [[out_word2idx[word] for word in sentence.split(' ')] for sentence in outputs]\n",
    "        input_data = tf.keras.preprocessing.sequence.pad_sequences(input_data, maxlen=in_maxlen, padding=\"post\")\n",
    "        output_data = tf.keras.preprocessing.sequence.pad_sequences(output_data, maxlen=out_maxlen, padding=\"post\")\n",
    "        \n",
    "        target_data = [[output_data[n][i+1] for i in range(len(output_data[n])-1)] for n in range(len(output_data))]\n",
    "        target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, maxlen=out_maxlen, padding=\"post\")\n",
    "        target_data = target_data.reshape((target_data.shape[0], target_data.shape[1], 1))\n",
    "        return input_data, output_data,target_data\n",
    "    \n",
    "    def data_generator(self,x,y,in_maxlen,out_maxlen,in_vocab_size,out_vocab_size,batch_size):\n",
    "        ''' Generate a batch of data '''\n",
    "        while True:\n",
    "            for j in range(0, len(x), batch_size):\n",
    "                encoder_input_data = np.zeros((batch_size, in_maxlen),dtype='float32')\n",
    "                decoder_input_data = np.zeros((batch_size, out_maxlen),dtype='float32')\n",
    "                decoder_target_data = np.zeros((batch_size, out_maxlen, out_vocab_size),dtype='float32')\n",
    "                for i, (input_text, target_text) in enumerate(zip(x[j:j+batch_size], y[j:j+batch_size])):\n",
    "                    for t, word in enumerate(input_text.split(' ')):\n",
    "                        try:\n",
    "                            encoder_input_data[i, t] = in_word2idx[word]\n",
    "                        except Exception as e:\n",
    "                            print(\"Wrong word:\",word)\n",
    "                            print(\"Exception:\",e)\n",
    "                    for t, word in enumerate(target_text.split(' ')):\n",
    "                        if t<len(target_text.split())-1:\n",
    "                            decoder_input_data[i, t] = out_word2idx[word]\n",
    "                        if t>0:\n",
    "                            decoder_target_data[i, t - 1, out_word2idx[word]] = 1.\n",
    "                encoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(encoder_input_data, maxlen=in_maxlen, padding=\"post\")\n",
    "                decoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(decoder_input_data, maxlen=out_maxlen, padding=\"post\")\n",
    "                decoder_target_data = tf.keras.preprocessing.sequence.pad_sequences(decoder_target_data, maxlen=out_maxlen, padding=\"post\")\n",
    "                yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  [['<start> Despite my best  <end>', '<start> efforts <end>'], [\"<start> We're glad the issu <end>\", '<start> es got sorted out despite the delay <end>'], [\"<start> I'm wr <end>\", '<start> iting to remind you about <end>'], ['<start> If you could hav <end>', '<start> e it ready <end>'], [\"<start> I didn't quite  <end>\", '<start> get your point <end>'], ['<start> Could you be more sp <end>', '<start> ecific? <end>'], ['<start> If you need mo <end>', '<start> re information <end>'], ['<start> Would you min <end>', '<start> d if I took the day off <end>'], ['<start> If <end>', \"<start>  so I'll book accordingly <end>\"], ['<start> We brought together some of the best tutorials w <end>', '<start> hich <end>']]\n",
      "in_maxlen:  21\n",
      "out_maxlen:  21\n"
     ]
    }
   ],
   "source": [
    "preprocess_obj = Preprocessing()\n",
    "\n",
    "preprocess_obj.get_data()\n",
    "# Training Params\n",
    "in_vocab_size = preprocess_obj.in_vocab_size\n",
    "out_vocab_size = preprocess_obj.out_vocab_size\n",
    "in_maxlen = preprocess_obj.in_maxlen\n",
    "out_maxlen = preprocess_obj.out_maxlen\n",
    "in_word2idx = preprocess_obj.in_word2idx\n",
    "out_word2idx = preprocess_obj.out_word2idx\n",
    "x = preprocess_obj.inputs\n",
    "y = preprocess_obj.outputs\n",
    "print(\"in_maxlen: \",in_maxlen)\n",
    "print(\"out_maxlen: \",out_maxlen)\n",
    "encoder_data_input, decoder_data_input,decoder_data_output = preprocess_obj.create_data(x,y,in_maxlen,out_maxlen,in_vocab_size,out_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateModel():\n",
    "    \n",
    "    def build(self,epochs,batch_size):\n",
    "        embedding_dim = 300\n",
    "        n_units = 128\n",
    "        '''Enoder'''\n",
    "        encoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
    "        encoder_emb = tf.keras.layers.Embedding(input_dim=in_vocab_size, output_dim=embedding_dim)\n",
    "        encoder_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=n_units, return_sequences=True, return_state=True))\n",
    "        \n",
    "        encoder_lstm_out, fstate_h, fstate_c, bstate_h, bstate_c = encoder_lstm(encoder_emb(encoder_inputs))\n",
    "        state_h = tf.keras.layers.Concatenate()([fstate_h,bstate_h])\n",
    "        state_c = tf.keras.layers.Concatenate()([bstate_h,bstate_c])\n",
    "        encoder_states = [state_h, state_c]\n",
    "        \n",
    "        '''Decoder'''\n",
    "        decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
    "        decoder_emb = tf.keras.layers.Embedding(input_dim=out_vocab_size, output_dim=embedding_dim)\n",
    "        decoder_lstm = tf.keras.layers.LSTM(units=n_units*2, return_sequences=True, return_state=True)\n",
    "        \n",
    "        decoder_lstm_out, _, _ = decoder_lstm(decoder_emb(decoder_inputs), initial_state=encoder_states)\n",
    "        \n",
    "        \n",
    "        '''Dense layer'''\n",
    "        decoder_dense = tf.keras.models.Sequential()\n",
    "        decoder_dense.add(tf.keras.layers.Dense(n_units, activation=\"relu\"))\n",
    "        decoder_dense.add(tf.keras.layers.Dropout(rate=.2))\n",
    "        decoder_dense.add(tf.keras.layers.Dense(out_vocab_size, activation=\"softmax\"))\n",
    "        decoder_out = decoder_dense(decoder_lstm_out)\n",
    "\n",
    "        self.model = tf.keras.models.Model(inputs = [encoder_inputs, decoder_inputs], outputs= decoder_out)\n",
    "        \n",
    "        '''Compile and Run'''\n",
    "        self.model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['acc'])\n",
    "        self.model.summary()\n",
    "        self.run(epochs,batch_size)\n",
    "        \n",
    "        '''Here's our inference setup'''\n",
    "        self.encoder_model = tf.keras.models.Model(encoder_inputs, [encoder_lstm_out, state_h, state_c])\n",
    "\n",
    "        #inf_decoder_inputs = Input(shape=(None,))\n",
    "        decoder_state_input_h = tf.keras.layers.Input(shape=(n_units*2,))\n",
    "        decoder_state_input_c = tf.keras.layers.Input(shape=(n_units*2,))\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "        decoder_outputs, decoder_h, decoder_c = decoder_lstm(decoder_emb(decoder_inputs),\n",
    "                                                         initial_state=decoder_states_inputs)\n",
    "        \n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        \n",
    "        inputs = [decoder_inputs, decoder_state_input_h, decoder_state_input_c]\n",
    "        outputs = [decoder_outputs, decoder_h, decoder_c]\n",
    "        \n",
    "        self.decoder_model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def run(self,epochs,batch_size):\n",
    "        self.history = self.model.fit([encoder_data_input, decoder_data_input], decoder_data_output, \n",
    "                                      batch_size= batch_size, epochs=epochs,validation_split=0.2)\n",
    "        \n",
    "    def save_model(self,model,model_file):\n",
    "        with open(model_file+'.json', 'w', encoding='utf8') as f:\n",
    "            f.write(model.to_json())\n",
    "        model.save_weights(model_file+'.h5')\n",
    "        \n",
    "    def save(self):\n",
    "        encoder_model_name = 'models/Encoder_Model1'\n",
    "        self.save_model(self.encoder_model,encoder_model_name)\n",
    "        \n",
    "        decoder_model_name = 'models/Decoder_Model1'\n",
    "        self.save_model(self.decoder_model,decoder_model_name)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        (None, None, 300)    654300      input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) [(None, None, 256),  439296      embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_19 (Embedding)        (None, None, 300)    612900      input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 256)          0           bidirectional_9[0][1]            \n",
      "                                                                 bidirectional_9[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 256)          0           bidirectional_9[0][3]            \n",
      "                                                                 bidirectional_9[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                  [(None, None, 256),  570368      embedding_19[0][0]               \n",
      "                                                                 concatenate_18[0][0]             \n",
      "                                                                 concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, None, 2043)   296443      lstm_19[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,573,307\n",
      "Trainable params: 2,573,307\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 12537 samples, validate on 3135 samples\n",
      "12512/12537 [============================>.] - ETA: 0s - loss: 1.3660 - acc: 0.7985"
     ]
    }
   ],
   "source": [
    "model_obj = CreateModel()\n",
    "model_obj.build(1,32)\n",
    "model_obj.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction():\n",
    "    def __init__(self):\n",
    "        self.input_max_len = 21\n",
    "        self.target_max_len = 21\n",
    "        self.load_model()\n",
    "        \n",
    "    def load_weights(self,model_filename, model_weights_filename):\n",
    "        with open(model_filename, 'r', encoding='utf8') as f:\n",
    "            model = tf.keras.models.model_from_json(f.read())\n",
    "        model.load_weights(model_weights_filename)\n",
    "        return model\n",
    "    \n",
    "    def load_model(self):\n",
    "        encoder_model_name = 'models/Encoder_Model1'\n",
    "        decoder_model_name = 'models/Decoder_Model1'\n",
    "        self.encoder_model = self.load_weights(encoder_model_name+'.json', encoder_model_name+'.h5')\n",
    "        self.decoder_model = self.load_weights(decoder_model_name+'.json', decoder_model_name+'.h5')\n",
    "        \n",
    "        in_word2idx_dict = 'dict/in_word2idx_dict.pkl'\n",
    "        self.in_word2idx = load(open(in_word2idx_dict, 'rb'))\n",
    "        self.in_idx2word = {v:k for k,v in self.in_word2idx.items()}\n",
    "        out_word2idx_dict = 'dict/out_word2idx_dict.pkl'\n",
    "        self.out_word2idx =load(open(out_word2idx_dict, 'rb'))\n",
    "        self.out_idx2word = {v:k for k,v in self.out_word2idx.items()}\n",
    "        \n",
    "    def sentence_to_vector(self, sentence):\n",
    "        pre = sentence\n",
    "        vec = np.zeros(self.input_max_len)\n",
    "        sentence_list = [self.in_word2idx[s] for s in pre.split(' ')]\n",
    "        for i,w in enumerate(sentence_list):\n",
    "            vec[i] = w\n",
    "        return vec\n",
    "\n",
    "    def predict(self,input_sentence):\n",
    "        sv = self.sentence_to_vector(input_sentence)\n",
    "        sv = sv.reshape(1,len(sv))\n",
    "        [emb_out, sh, sc] = self.encoder_model.predict(x=sv)\n",
    "\n",
    "        i = 0\n",
    "        start_vec = self.out_word2idx[\"<start>\"]\n",
    "        stop_vec = self.out_word2idx[\"<end>\"]\n",
    "\n",
    "        cur_vec = np.zeros((1,1))\n",
    "        cur_vec[0,0] = start_vec\n",
    "        cur_word = \"<start>\"\n",
    "        output_sentence = \"\"\n",
    "        while cur_word != \"<end>\" and i < (self.target_max_len-1):\n",
    "            i += 1\n",
    "            if cur_word != \"<start>\":\n",
    "                output_sentence = output_sentence + \" \" + cur_word\n",
    "            x_in = [cur_vec, sh, sc]\n",
    "            [nvec, sh, sc] = self.decoder_model.predict(x=x_in)\n",
    "            cur_vec[0,0] = np.argmax(nvec[0,0])\n",
    "            cur_word = self.out_idx2word[np.argmax(nvec[0,0])]\n",
    "        return output_sentence\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_obj = Prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:  I would appreci \n",
      "Result:  Please find the \n",
      "Result:  Could you ple \n",
      "Result:  I would be happy \n",
      "Result:  Please let me know \n",
      "Result:  For further d \n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    \"I would appreci\",\n",
    "    \"Please find the\",\n",
    "    \"Could you ple\",\n",
    "    \"I would be happy\",\n",
    "    \"Please let me know\",\n",
    "    \"For further d\"\n",
    "]\n",
    "for item in test_list:\n",
    "    print(\"Result: \",item+predict_obj.predict(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
