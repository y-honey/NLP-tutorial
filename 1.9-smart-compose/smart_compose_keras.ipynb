{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pickle import load\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation():\n",
    "    def __init__(self):\n",
    "        file = 'data/dataset.txt'\n",
    "        with open(file,'r') as fp:\n",
    "            self.content_list = fp.read().splitlines()[1:]\n",
    "        #data_frame = pd.read_csv(file)\n",
    "        #self.content_list = data_frame['content'].tolist()\n",
    "        print(self.content_list[:10])\n",
    "    \n",
    "    def remove_punctuations(self,content):\n",
    "        punctuators = '#$%&*+-/<=>@[\\\\]^_`{|}~\\t\\n'\n",
    "        for idx in range(len(content)):\n",
    "            for punc in punctuators:\n",
    "                content[idx] = content[idx].replace(punc, '') \n",
    "        return content\n",
    "    \n",
    "    def split_sentence(self,content):\n",
    "        new_data_list = list()\n",
    "        for item in content:\n",
    "            item = item.replace('\\n','')\n",
    "            item = item.replace('\\t','')\n",
    "        for item in content:\n",
    "            new_data_list = new_data_list+item.split('.')\n",
    "        content = new_data_list\n",
    "        return content\n",
    "    \n",
    "    def cleaning_data(self):\n",
    "        content = self.split_sentence(self.content_list)\n",
    "        print(\"Sentences Splited\")\n",
    "        content = self.remove_punctuations(content)\n",
    "        print('Punctuators Removed')\n",
    "        content = list(set(content))\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dear Sir/Madam', 'Dear', 'Hello', 'Hi', 'Hi Team', 'Good morning Team', 'Good afternoon', 'I recently read about that', 'I recently heard about that', 'Thank you for taking the time to write to us']\n"
     ]
    }
   ],
   "source": [
    "dp_obj = DataPreparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences Splited\n",
      "Punctuators Removed\n"
     ]
    }
   ],
   "source": [
    "content = dp_obj.cleaning_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    def __init__(self):\n",
    "        self.content_list = content\n",
    "        \n",
    "    def make_data(self):\n",
    "        \n",
    "        data = list()\n",
    "        for sentence in self.content_list:\n",
    "            for idx in range(1, len(sentence)):\n",
    "                x = '<start> '+ sentence[:idx+1] + ' <end>'\n",
    "                y = '<start> '+ sentence[idx+1:] + ' <end>'\n",
    "                data.append([x,y])\n",
    "        random.shuffle(data)\n",
    "        print(\"data: \",data[10:20])\n",
    "        return data\n",
    "    \n",
    "    def create_vacab(self,pairs):\n",
    "        vocab = set()\n",
    "        word2idx = dict() \n",
    "        for phrase in pairs:\n",
    "            vocab.update(phrase.split(' '))\n",
    "        vocab = sorted(vocab)\n",
    "        word2idx[\"<pad>\"] = 0\n",
    "        for i,word in enumerate(vocab):\n",
    "            word2idx[word] = i + 1\n",
    "        return word2idx\n",
    "            \n",
    "    def get_data(self):\n",
    "        data = self.make_data()\n",
    "        \n",
    "        self.inputs = list()\n",
    "        self.outputs = list()\n",
    "        for item in data:\n",
    "            self.inputs.append(item[0])\n",
    "            self.outputs.append(item[1])\n",
    "        \n",
    "        self.in_word2idx = self.create_vacab(self.inputs)\n",
    "        self.out_word2idx = self.create_vacab(self.outputs)\n",
    "        \n",
    "        self.in_vocab_size = len(self.in_word2idx)\n",
    "        self.out_vocab_size = len(self.out_word2idx)\n",
    "        \n",
    "        self.in_maxlen = max(len(item.split(' ')) for item in self.inputs)\n",
    "        self.out_maxlen = max(len(item.split(' ')) for item in self.outputs)\n",
    "        \n",
    "        \n",
    "    def create_data(self,inputs,outputs,in_maxlen,out_maxlen,in_vocab_size,out_vocab_size):\n",
    "        input_data = [[in_word2idx[word] for word in sentence.split(' ')] for sentence in inputs]\n",
    "        output_data = [[out_word2idx[word] for word in sentence.split(' ')] for sentence in outputs]\n",
    "        input_data = tf.keras.preprocessing.sequence.pad_sequences(input_data, maxlen=in_maxlen, padding=\"post\")\n",
    "        output_data = tf.keras.preprocessing.sequence.pad_sequences(output_data, maxlen=out_maxlen, padding=\"post\")\n",
    "        \n",
    "        target_data = [[output_data[n][i+1] for i in range(len(output_data[n])-1)] for n in range(len(output_data))]\n",
    "        target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, maxlen=out_maxlen, padding=\"post\")\n",
    "        target_data = target_data.reshape((target_data.shape[0], target_data.shape[1], 1))\n",
    "        print(input_data.shape)\n",
    "        print(output_data.shape)\n",
    "        print(target_data.shape)\n",
    "        \n",
    "        return (input_data, output_data,target_data)\n",
    "    \n",
    "    def data_generator(self,x,y,in_maxlen,out_maxlen,in_vocab_size,out_vocab_size,batch_size):\n",
    "        ''' Generate a batch of data '''\n",
    "        while True:\n",
    "            for j in range(0, len(x), batch_size):\n",
    "                encoder_input_data = np.zeros((batch_size, in_maxlen),dtype='float32')\n",
    "                decoder_input_data = np.zeros((batch_size, out_maxlen),dtype='float32')\n",
    "                decoder_target_data = np.zeros((batch_size, out_maxlen),dtype='float32')\n",
    "                for i, (input_text, target_text) in enumerate(zip(x[j:j+batch_size], y[j:j+batch_size])):\n",
    "                    for t, word in enumerate(input_text.split(' ')):\n",
    "                        encoder_input_data[i, t] = in_word2idx[word]\n",
    "                        \n",
    "                    for t, word in enumerate(target_text.split(' ')):\n",
    "                        if t<len(target_text.split())-1:\n",
    "                            decoder_input_data[i, t] = out_word2idx[word]\n",
    "                        if t>0:\n",
    "                            decoder_target_data[i, t - 1] = out_word2idx[word]\n",
    "                encoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(encoder_input_data, maxlen=in_maxlen, padding=\"post\")\n",
    "                decoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(decoder_input_data, maxlen=out_maxlen, padding=\"post\")\n",
    "                decoder_target_data = tf.keras.preprocessing.sequence.pad_sequences(decoder_target_data, maxlen=out_maxlen, padding=\"post\")\n",
    "                yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  [['<start> Are you asking for a favor or you are meeting <end>', '<start>  soon? <end>'], ['<start> If you have any que <end>', '<start> stions please email or call me <end>'], [\"<start> I'm glad we had a chance to chat at the conv <end>\", '<start> ention <end>'], [\"<start> I'd be very grateful if you  <end>\", '<start> could <end>'], ['<start> It was nice to hear from y <end>', '<start> ou yesterday <end>'], ['<start> Thanks f <end>', '<start> or getting back to me so quickly <end>'], ['<start> In the meantime <end>', '<start> , if you need any more information <end>'], [\"<start> Hope you're feeling great <end>\", '<start> ! <end>'], [\"<start> I'm writing to tell you tha <end>\", '<start> t <end>'], [\"<start> It's out of my hands <end>\", '<start>  <end>']]\n",
      "12537\n",
      "3135\n",
      "in_maxlen:  21\n",
      "out_maxlen:  21\n"
     ]
    }
   ],
   "source": [
    "preprocess_obj = Preprocessing()\n",
    "\n",
    "preprocess_obj.get_data()\n",
    "# Training Params\n",
    "in_vocab_size = preprocess_obj.in_vocab_size\n",
    "out_vocab_size = preprocess_obj.out_vocab_size\n",
    "in_maxlen = preprocess_obj.in_maxlen\n",
    "out_maxlen = preprocess_obj.out_maxlen\n",
    "in_word2idx = preprocess_obj.in_word2idx\n",
    "out_word2idx = preprocess_obj.out_word2idx\n",
    "x = preprocess_obj.inputs\n",
    "y = preprocess_obj.outputs\n",
    "train_size = int(len(x)*0.8)\n",
    "train_x = x[:train_size]\n",
    "train_y = y[:train_size]\n",
    "val_x = x[train_size:]\n",
    "val_y = y[train_size:]\n",
    "print(len(train_x))\n",
    "print(len(val_x))\n",
    "print(\"in_maxlen: \",in_maxlen)\n",
    "print(\"out_maxlen: \",out_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateModel():\n",
    "    \n",
    "    def build(self,epochs,batch_size):\n",
    "        embedding_dim = 300\n",
    "        n_units = 128\n",
    "        '''Enoder'''\n",
    "        encoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
    "        encoder_emb = tf.keras.layers.Embedding(input_dim=in_vocab_size, output_dim=embedding_dim)\n",
    "        encoder_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=n_units, return_sequences=True, return_state=True))\n",
    "        \n",
    "        encoder_lstm_out, fstate_h, fstate_c, bstate_h, bstate_c = encoder_lstm(encoder_emb(encoder_inputs))\n",
    "        state_h = tf.keras.layers.Concatenate()([fstate_h,bstate_h])\n",
    "        state_c = tf.keras.layers.Concatenate()([bstate_h,bstate_c])\n",
    "        encoder_states = [state_h, state_c]\n",
    "        \n",
    "        '''Decoder'''\n",
    "        decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
    "        decoder_emb = tf.keras.layers.Embedding(input_dim=out_vocab_size, output_dim=embedding_dim)\n",
    "        decoder_lstm = tf.keras.layers.LSTM(units=n_units*2, return_sequences=True, return_state=True)\n",
    "        \n",
    "        decoder_lstm_out, _, _ = decoder_lstm(decoder_emb(decoder_inputs), initial_state=encoder_states)\n",
    "        \n",
    "        \n",
    "        '''Dense layer'''\n",
    "        decoder_dense = tf.keras.models.Sequential()\n",
    "        decoder_dense.add(tf.keras.layers.Dense(n_units, activation=\"relu\"))\n",
    "        decoder_dense.add(tf.keras.layers.Dropout(rate=.2))\n",
    "        decoder_dense.add(tf.keras.layers.Dense(out_vocab_size, activation=\"softmax\"))\n",
    "        decoder_out = decoder_dense(decoder_lstm_out)\n",
    "\n",
    "        self.model = tf.keras.models.Model(inputs = [encoder_inputs, decoder_inputs], outputs= decoder_out)\n",
    "        \n",
    "        '''Compile and Run'''\n",
    "        self.model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['acc'])\n",
    "        self.model.summary()\n",
    "        self.run(epochs,batch_size)\n",
    "        #self.run_gen(epochs,batch_size)\n",
    "        '''Here's our inference setup'''\n",
    "        self.encoder_model = tf.keras.models.Model(encoder_inputs, [encoder_lstm_out, state_h, state_c])\n",
    "\n",
    "        #inf_decoder_inputs = Input(shape=(None,))\n",
    "        decoder_state_input_h = tf.keras.layers.Input(shape=(n_units*2,))\n",
    "        decoder_state_input_c = tf.keras.layers.Input(shape=(n_units*2,))\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "        decoder_outputs, decoder_h, decoder_c = decoder_lstm(decoder_emb(decoder_inputs),\n",
    "                                                         initial_state=decoder_states_inputs)\n",
    "        \n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        \n",
    "        inputs = [decoder_inputs, decoder_state_input_h, decoder_state_input_c]\n",
    "        outputs = [decoder_outputs, decoder_h, decoder_c]\n",
    "        \n",
    "        self.decoder_model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def run_gen(self,epochs,batch_size):\n",
    "        epochs = 2\n",
    "        train_steps = len(train_x) / batch_size\n",
    "        val_steps = len(val_x) / batch_size\n",
    "        train_data = preprocess_obj.data_generator(train_x,train_y,in_maxlen,out_maxlen,in_vocab_size,out_vocab_size,batch_size)\n",
    "        val_data = preprocess_obj.data_generator(val_x,val_y,in_maxlen,out_maxlen,in_vocab_size,out_vocab_size,batch_size)\n",
    "        self.history = self.model.fit(train_data, steps_per_epoch = train_steps,\n",
    "                                      validation_data = val_data, validation_steps = val_steps,\n",
    "                                      epochs=epochs)\n",
    "        \n",
    "    def run(self,epochs,batch_size):\n",
    "        train_data = preprocess_obj.create_data(train_x,train_y,\n",
    "                                                in_maxlen,out_maxlen,\n",
    "                                                in_vocab_size,out_vocab_size)\n",
    "        val_data = preprocess_obj.create_data(val_x,val_y,\n",
    "                                              in_maxlen,out_maxlen,\n",
    "                                              in_vocab_size,out_vocab_size)\n",
    "        \n",
    "        train_enc_in, train_dec_in,train_dec_out = train_data\n",
    "        val_enc_in,val_dec_in,val_dec_out = val_data\n",
    "        self.history = self.model.fit([train_enc_in, train_dec_in], train_dec_out, \n",
    "                                      validation_data=([val_enc_in,val_dec_in],val_dec_out),\n",
    "                                      batch_size= batch_size, epochs=epochs,verbose=1)\n",
    "        \n",
    "        \n",
    "    def save_model(self,model,model_file):\n",
    "        with open(model_file+'.json', 'w', encoding='utf8') as f:\n",
    "            f.write(model.to_json())\n",
    "        model.save_weights(model_file+'.h5')\n",
    "        \n",
    "    def save(self):\n",
    "        encoder_model_name = 'models/Encoder_Model1'\n",
    "        self.save_model(self.encoder_model,encoder_model_name)\n",
    "        \n",
    "        decoder_model_name = 'models/Decoder_Model1'\n",
    "        self.save_model(self.decoder_model,decoder_model_name)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 300)    654300      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   [(None, None, 256),  439296      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    612900      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           bidirectional[0][1]              \n",
      "                                                                 bidirectional[0][3]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           bidirectional[0][3]              \n",
      "                                                                 bidirectional[0][4]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  570368      embedding_1[0][0]                \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, None, 2043)   296443      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,573,307\n",
      "Trainable params: 2,573,307\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(12537, 21)\n",
      "(12537, 21)\n",
      "(12537, 21, 1)\n",
      "(3135, 21)\n",
      "(3135, 21)\n",
      "(3135, 21, 1)\n",
      "Train on 12537 samples, validate on 3135 samples\n",
      "Epoch 1/70\n",
      "12512/12537 [============================>.] - ETA: 0s - loss: 1.3767 - acc: 0.7964"
     ]
    }
   ],
   "source": [
    "model_obj = CreateModel()\n",
    "model_obj.build(70,32)\n",
    "model_obj.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction():\n",
    "    def __init__(self):\n",
    "        self.input_max_len = 21\n",
    "        self.target_max_len = 21\n",
    "        self.load_model()\n",
    "        \n",
    "    def load_weights(self,model_filename, model_weights_filename):\n",
    "        with open(model_filename, 'r', encoding='utf8') as f:\n",
    "            model = tf.keras.models.model_from_json(f.read())\n",
    "        model.load_weights(model_weights_filename)\n",
    "        return model\n",
    "    \n",
    "    def load_model(self):\n",
    "        encoder_model_name = 'models/Encoder_Model1'\n",
    "        decoder_model_name = 'models/Decoder_Model1'\n",
    "        self.encoder_model = self.load_weights(encoder_model_name+'.json', encoder_model_name+'.h5')\n",
    "        self.decoder_model = self.load_weights(decoder_model_name+'.json', decoder_model_name+'.h5')\n",
    "        \n",
    "        in_word2idx_dict = 'dict/in_word2idx_dict.pkl'\n",
    "        self.in_word2idx = load(open(in_word2idx_dict, 'rb'))\n",
    "        self.in_idx2word = {v:k for k,v in self.in_word2idx.items()}\n",
    "        out_word2idx_dict = 'dict/out_word2idx_dict.pkl'\n",
    "        self.out_word2idx =load(open(out_word2idx_dict, 'rb'))\n",
    "        self.out_idx2word = {v:k for k,v in self.out_word2idx.items()}\n",
    "        \n",
    "    def sentence_to_vector(self, sentence):\n",
    "        pre = sentence\n",
    "        vec = np.zeros(self.input_max_len)\n",
    "        sentence_list = [self.in_word2idx[s] for s in pre.split(' ')]\n",
    "        for i,w in enumerate(sentence_list):\n",
    "            vec[i] = w\n",
    "        return vec\n",
    "\n",
    "    def predict(self,input_sentence):\n",
    "        sv = self.sentence_to_vector(input_sentence)\n",
    "        sv = sv.reshape(1,len(sv))\n",
    "        [emb_out, sh, sc] = self.encoder_model.predict(x=sv)\n",
    "\n",
    "        i = 0\n",
    "        start_vec = self.out_word2idx[\"<start>\"]\n",
    "        stop_vec = self.out_word2idx[\"<end>\"]\n",
    "\n",
    "        cur_vec = np.zeros((1,1))\n",
    "        cur_vec[0,0] = start_vec\n",
    "        cur_word = \"<start>\"\n",
    "        output_sentence = \"\"\n",
    "        while cur_word != \"<end>\" and i < (self.target_max_len-1):\n",
    "            i += 1\n",
    "            if cur_word != \"<start>\":\n",
    "                output_sentence = output_sentence + \" \" + cur_word\n",
    "            x_in = [cur_vec, sh, sc]\n",
    "            [nvec, sh, sc] = self.decoder_model.predict(x=x_in)\n",
    "            cur_vec[0,0] = np.argmax(nvec[0,0])\n",
    "            cur_word = self.out_idx2word[np.argmax(nvec[0,0])]\n",
    "        return output_sentence\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_obj = Prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [\n",
    "    \"I would appreci\",\n",
    "    \"Please find the\",\n",
    "    \"Could you ple\",\n",
    "    \"I would be happy\",\n",
    "    \"Please let me know\",\n",
    "    \"For further d\"\n",
    "]\n",
    "for item in test_list:\n",
    "    print(\"Result: \",item+predict_obj.predict(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
