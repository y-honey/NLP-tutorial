{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pickle import load\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation():\n",
    "    def __init__(self):\n",
    "        file = 'data/dataset.csv'\n",
    "        data_frame = pd.read_csv(file)\n",
    "        self.content_list = data_frame['content'].tolist()\n",
    "    \n",
    "    def remove_punctuations(self,content):\n",
    "        punctuators = '#$%&*+-/<=>@[\\\\]^_`{|}~\\t\\n'\n",
    "        for idx in range(len(content)):\n",
    "            for punc in punctuators:\n",
    "                content[idx] = content[idx].replace(punc, '') \n",
    "        return content\n",
    "    \n",
    "    def removing_sentence(self,content):\n",
    "        drop_index_list = list()\n",
    "        for idx in range(len(content)):\n",
    "            words = content[idx].split()\n",
    "            if len(words)<=2 or len(words)>100:\n",
    "                drop_index_list.append(idx)\n",
    "        content = np.array(content)\n",
    "        content = np.delete(content,drop_index_list)\n",
    "        content = content.tolist()\n",
    "        return content\n",
    "    \n",
    "    def split_sentence(self,content):\n",
    "        new_data_list = list()\n",
    "        for item in content:\n",
    "            item = item.replace('\\n','')\n",
    "            item = item.replace('\\t','')\n",
    "        for item in content:\n",
    "            new_data_list = new_data_list+item.split('.')\n",
    "        content = new_data_list\n",
    "        return content\n",
    "    \n",
    "    def cleaning_data(self):\n",
    "        content = self.split_sentence(self.content_list)\n",
    "        print(\"Sentences Splited\")\n",
    "        content = self.removing_sentence(content)\n",
    "        print('Sentences deleted')\n",
    "        content = self.remove_punctuations(content)\n",
    "        print('Punctuators Removed')\n",
    "        content = list(set(content))\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_obj = DataPreparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences Splited\n",
      "Sentences deleted\n",
      "Punctuators Removed\n"
     ]
    }
   ],
   "source": [
    "content = dp_obj.cleaning_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    def __init__(self):\n",
    "        self.content_list = content\n",
    "        \n",
    "    def make_data(self):\n",
    "        data = list()\n",
    "        for sentence in self.content_list:\n",
    "            for idx in range(1, len(sentence)):\n",
    "                x = '<start> '+ sentence[:idx+1] + ' <end>'\n",
    "                y = '<start> '+ sentence[idx+1:] + ' <end>'\n",
    "                data.append([x,y])\n",
    "        print(\"data: \",data[10:20])\n",
    "        return data\n",
    "    \n",
    "    def create_data(self,pairs):\n",
    "        vocab = set()\n",
    "        word2idx = dict() \n",
    "        for phrase in pairs:\n",
    "            vocab.update(phrase.split(' '))\n",
    "        vocab = sorted(vocab)\n",
    "        word2idx[\"<pad>\"] = 0\n",
    "        for i,word in enumerate(vocab):\n",
    "            word2idx[word] = i + 1\n",
    "        return word2idx,vocab\n",
    "            \n",
    "    def get_data(self):\n",
    "        data = self.make_data()\n",
    "        \n",
    "        inputs = list()\n",
    "        outputs = list()\n",
    "        for item in data:\n",
    "            inputs.append(item[0])\n",
    "            outputs.append(item[1])\n",
    "        \n",
    "        in_word2idx,in_vocab = self.create_data(inputs)\n",
    "        out_word2idx,out_vocab = self.create_data(outputs)\n",
    "        \n",
    "        input_data = [[in_word2idx[word] for word in sentence.split(' ')] for sentence in inputs]\n",
    "        output_data = [[out_word2idx[word] for word in sentence.split(' ')] for sentence in outputs]\n",
    "        \n",
    "        in_maxlen = max(len(item) for item in input_data)\n",
    "        out_maxlen = max(len(item) for item in output_data)\n",
    "        \n",
    "        input_data = tf.keras.preprocessing.sequence.pad_sequences(input_data, maxlen=in_maxlen, padding=\"post\")\n",
    "        output_data = tf.keras.preprocessing.sequence.pad_sequences(output_data, maxlen=out_maxlen, padding=\"post\")\n",
    "        \n",
    "        return input_data, output_data, in_word2idx,out_word2idx, in_maxlen, out_maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  [['<start> In other wor <end>', '<start> ds would you like us to <end>'], ['<start> In other word <end>', '<start> s would you like us to <end>'], ['<start> In other words <end>', '<start>  would you like us to <end>'], ['<start> In other words  <end>', '<start> would you like us to <end>'], ['<start> In other words w <end>', '<start> ould you like us to <end>'], ['<start> In other words wo <end>', '<start> uld you like us to <end>'], ['<start> In other words wou <end>', '<start> ld you like us to <end>'], ['<start> In other words woul <end>', '<start> d you like us to <end>'], ['<start> In other words would <end>', '<start>  you like us to <end>'], ['<start> In other words would  <end>', '<start> you like us to <end>']]\n",
      "in_maxlen:  35\n",
      "out_maxlen:  35\n"
     ]
    }
   ],
   "source": [
    "preprocess_obj = Preprocessing()\n",
    "input_data, output_data, in_word2idx,out_word2idx, in_maxlen, out_maxlen = preprocess_obj.get_data()\n",
    "\n",
    "print(\"in_maxlen: \",in_maxlen)\n",
    "print(\"out_maxlen: \",out_maxlen)\n",
    "target_data = [[output_data[n][i+1] for i in range(len(output_data[n])-1)] for n in range(len(output_data))]\n",
    "target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, maxlen=out_maxlen, padding=\"post\")\n",
    "target_data = target_data.reshape((target_data.shape[0], target_data.shape[1], 1))\n",
    "\n",
    "p = np.random.permutation(len(input_data))\n",
    "encoder_data_input = input_data[p]\n",
    "decoder_data_input = output_data[p]\n",
    "decoder_data_output = target_data[p]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vocab_size = len(in_word2idx)\n",
    "out_vocab_size = len(out_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateModel():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "                \n",
    "    def build(self,epochs,batch_size):\n",
    "        units = 64\n",
    "        embedding_dim = 300\n",
    "        #encoder\n",
    "        encoder_inputs = tf.keras.layers.Input(shape=(in_maxlen,))\n",
    "        encoder_emb = tf.keras.layers.Embedding(input_dim=in_vocab_size, output_dim=embedding_dim)\n",
    "        encoder_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=units, \n",
    "                                                                          return_sequences=True, \n",
    "                                                                          return_state=True))\n",
    "        encoder_out, fstate_h, fstate_c, bstate_h, bstate_c = encoder_lstm(encoder_emb(encoder_inputs))\n",
    "        state_h = tf.keras.layers.Concatenate()([fstate_h,bstate_h])\n",
    "        state_c = tf.keras.layers.Concatenate()([bstate_h,bstate_c])\n",
    "        encoder_states = [state_h, state_c]\n",
    "        \n",
    "        #decoder       \n",
    "        decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
    "        decoder_emb = tf.keras.layers.Embedding(input_dim=out_vocab_size, output_dim=embedding_dim)\n",
    "        decoder_lstm = tf.keras.layers.LSTM(units=units*2, return_sequences=True, return_state=True)\n",
    "        decoder_lstm_out, _, _ = decoder_lstm(decoder_emb(decoder_inputs), initial_state=encoder_states)\n",
    "        decoder_d1 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        decoder_d2 = tf.keras.layers.Dense(out_vocab_size, activation=\"softmax\")\n",
    "        decoder_out = decoder_d2(tf.keras.layers.Dropout(rate=.2)(decoder_d1(tf.keras.layers.Dropout(rate=.2)(decoder_lstm_out))))\n",
    "        \n",
    "        self.model = tf.keras.models.Model(inputs = [encoder_inputs, decoder_inputs], outputs= decoder_out)\n",
    "\n",
    "        self.model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "        self.model.summary()\n",
    "        \n",
    "        #run\n",
    "        self.run(epochs,batch_size)\n",
    "        \n",
    "        #eocoder setup\n",
    "        self.encoder_model = tf.keras.models.Model(encoder_inputs, [encoder_out, state_h, state_c])\n",
    "\n",
    "        # Decoder setup\n",
    "        inf_decoder_inputs = tf.keras.layers.Input(shape=(None,), name=\"inf_decoder_inputs\")\n",
    "        state_input_h = tf.keras.layers.Input(shape=(units*2,), name=\"state_input_h\")\n",
    "        state_input_c = tf.keras.layers.Input(shape=(units*2,), name=\"state_input_c\")\n",
    "        decoder_res, decoder_h, decoder_c = decoder_lstm(decoder_emb(inf_decoder_inputs), \n",
    "                                                         initial_state=[state_input_h, state_input_c])\n",
    "        inf_decoder_out = decoder_d2(decoder_d1(decoder_res))\n",
    "        self.decoder_model = tf.keras.models.Model(inputs=[inf_decoder_inputs, state_input_h, state_input_c], \n",
    "                                                   outputs=[inf_decoder_out, decoder_h, decoder_c])\n",
    "\n",
    "    def run(self,epochs,batch_size):\n",
    "        self.history = self.model.fit([encoder_data_input,decoder_data_input],decoder_data_output,\n",
    "                                      batch_size =batch_size, epochs=epochs,validation_split=0.1)\n",
    "        \n",
    "    def save_model(self,model,model_name):\n",
    "        with open(model_name+'.json', 'w', encoding='utf8') as f:\n",
    "            f.write(model.to_json())\n",
    "        model.save_weights(model_name+'.h5')\n",
    "            \n",
    "    def save(self):\n",
    "        \n",
    "        encoder_model_name = 'models/Encoder_base_model-v1.0'\n",
    "        self.save_model(self.encoder_model,encoder_model_name)\n",
    "        \n",
    "        decoder_model_name = 'models/Decoder_base_model-v1.0'\n",
    "        self.save_model(self.decoder_model,decoder_model_name)\n",
    "        \n",
    "        in_word2idx_dict = 'dict/in_word2idx_dict.pkl'\n",
    "        dump(in_word2idx, open(in_word2idx_dict, 'wb'))\n",
    "        out_word2idx_dict = 'dict/out_word2idx_dict.pkl'\n",
    "        dump(out_word2idx, open(out_word2idx_dict, 'wb'))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_30 (InputLayer)           [(None, 35)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_23 (Embedding)        (None, 35, 300)      980400      input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_31 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional [(None, 35, 128), (N 186880      embedding_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_24 (Embedding)        (None, None, 300)    951900      input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 128)          0           bidirectional_11[0][1]           \n",
      "                                                                 bidirectional_11[0][3]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 128)          0           bidirectional_11[0][3]           \n",
      "                                                                 bidirectional_11[0][4]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_25 (LSTM)                  [(None, None, 128),  219648      embedding_24[0][0]               \n",
      "                                                                 concatenate_20[0][0]             \n",
      "                                                                 concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, None, 128)    0           lstm_25[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, None, 64)     8256        dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, None, 64)     0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, None, 3173)   206245      dropout_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 2,553,329\n",
      "Trainable params: 2,553,329\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 19539 samples, validate on 2171 samples\n",
      "Epoch 1/30\n",
      "19539/19539 [==============================] - 107s 5ms/sample - loss: 1.8491 - accuracy: 0.8301 - val_loss: 0.8753 - val_accuracy: 0.8641\n",
      "Epoch 2/30\n",
      "19539/19539 [==============================] - 104s 5ms/sample - loss: 0.8645 - accuracy: 0.8614 - val_loss: 0.8100 - val_accuracy: 0.8663\n",
      "Epoch 3/30\n",
      "19539/19539 [==============================] - 102s 5ms/sample - loss: 0.8019 - accuracy: 0.8656 - val_loss: 0.7404 - val_accuracy: 0.8724\n",
      "Epoch 4/30\n",
      "19539/19539 [==============================] - 116s 6ms/sample - loss: 0.7379 - accuracy: 0.8706 - val_loss: 0.6837 - val_accuracy: 0.8764\n",
      "Epoch 5/30\n",
      " 8192/19539 [===========>..................] - ETA: 1:02 - loss: 0.6891 - accuracy: 0.8751"
     ]
    }
   ],
   "source": [
    "model_obj = CreateModel()\n",
    "model_obj.build(30,128)\n",
    "model_obj.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction():\n",
    "    def __init__(self):\n",
    "        self.input_max_len = 35\n",
    "        self.target_max_len = 35\n",
    "        self.load_model()\n",
    "        \n",
    "    def load_weights(self,model_filename, model_weights_filename):\n",
    "        with open(model_filename, 'r', encoding='utf8') as f:\n",
    "            model = tf.keras.models.model_from_json(f.read())\n",
    "        model.load_weights(model_weights_filename)\n",
    "        return model\n",
    "    \n",
    "    def load_model(self):\n",
    "        encoder_model_name = 'models/Encoder_base_model-v1.0'\n",
    "        decoder_model_name = 'models/Decoder_base_model-v1.0'\n",
    "        self.encoder_model = self.load_weights(encoder_model_name+'.json', encoder_model_name+'.h5')\n",
    "        self.decoder_model = self.load_weights(decoder_model_name+'.json', decoder_model_name+'.h5')\n",
    "        \n",
    "        in_word2idx_dict = 'dict/in_word2idx_dict.pkl'\n",
    "        self.in_word2idx = load(open(in_word2idx_dict, 'rb'))\n",
    "        self.in_idx2word = {v:k for k,v in self.in_word2idx.items()}\n",
    "        out_word2idx_dict = 'dict/out_word2idx_dict.pkl'\n",
    "        self.out_word2idx =load(open(out_word2idx_dict, 'rb'))\n",
    "        self.out_idx2word = {v:k for k,v in self.out_word2idx.items()}\n",
    "        \n",
    "    def sentence_to_vector(self, sentence):\n",
    "        pre = sentence\n",
    "        vec = np.zeros(self.input_max_len)\n",
    "        sentence_list = [self.in_word2idx[s] for s in pre.split(' ')]\n",
    "        for i,w in enumerate(sentence_list):\n",
    "            vec[i] = w\n",
    "        return vec\n",
    "\n",
    "    def predict(self,input_sentence):\n",
    "        sv = self.sentence_to_vector(input_sentence)\n",
    "        sv = sv.reshape(1,len(sv))\n",
    "        [emb_out, sh, sc] = self.encoder_model.predict(x=sv)\n",
    "\n",
    "        i = 0\n",
    "        start_vec = self.out_word2idx[\"<start>\"]\n",
    "        stop_vec = self.out_word2idx[\"<end>\"]\n",
    "\n",
    "        cur_vec = np.zeros((1,1))\n",
    "        cur_vec[0,0] = start_vec\n",
    "        cur_word = \"<start>\"\n",
    "        output_sentence = \"\"\n",
    "        while cur_word != \"<end>\" and i < (self.target_max_len-1):\n",
    "            i += 1\n",
    "            if cur_word != \"<start>\":\n",
    "                output_sentence = output_sentence + \" \" + cur_word\n",
    "            x_in = [cur_vec, sh, sc]\n",
    "            [nvec, sh, sc] = self.decoder_model.predict(x=x_in)\n",
    "            cur_vec[0,0] = np.argmax(nvec[0,0])\n",
    "            cur_word = self.out_idx2word[np.argmax(nvec[0,0])]\n",
    "        return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_obj = Prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " d a vacation in June\n"
     ]
    }
   ],
   "source": [
    "print(predict_obj.predict(\"Thank you for ta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
